{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Notebook\n",
    "-   Beatrice Re 104972\n",
    "-   Claudio Di Salvo 105677\n",
    "-   Raymond Li 105281\n",
    "\n",
    "Group Name : 10 TimeBenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycaret.regression import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df, time_column):\n",
    "    \"\"\"\n",
    "    Adds linear time features to a DataFrame for use in a LightGBM model.\n",
    "    It extracts various time components like hour, day of week, month, day of year, week of year, and year.\n",
    "\n",
    "    :param df: DataFrame to which the time features will be added.\n",
    "    :param time_column: The name of the column in the DataFrame that contains datetime information.\n",
    "    :return: The DataFrame with added time features.\n",
    "    \"\"\"\n",
    "    df[time_column] = pd.to_datetime(df[time_column])  \n",
    "    df['hour'] = df[time_column].dt.hour\n",
    "    df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "    df['month'] = df[time_column].dt.month\n",
    "    df['day_of_year'] = df[time_column].dt.dayofyear\n",
    "    df['week_of_year'] = df[time_column].dt.isocalendar().week \n",
    "    df['year'] = df[time_column].dt.year\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features nr 2\n",
    "def add_time_features_cat(df, time_column):\n",
    "    \"\"\"\n",
    "    Adds linear time features to a DataFrame for use in a Catboost model.\n",
    "    It extracts various time components like hour, day of week, month, day of year, week of year, and year.\n",
    "\n",
    "    :param df: DataFrame to which the time features will be added.\n",
    "    :param time_column: The name of the column in the DataFrame that contains datetime information.\n",
    "    :return: The DataFrame with added time features.\n",
    "    \"\"\"\n",
    "    df[time_column] = pd.to_datetime(df[time_column])  \n",
    "    \n",
    "    df['sin_hour'] = np.sin(np.pi * df[time_column].dt.hour/23.)\n",
    "    df['sin_month'] = np.sin(np.pi * df[time_column].dt.month/12.)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_est(observed, estimated, test):\n",
    "      \n",
    "      estimated['time_dummy'] = (estimated['date_forecast'] - estimated['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
    "      observed['time_dummy'] = 0 \n",
    "      test['time_dummy'] = (test['date_forecast'] - test['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
    "      \n",
    "      estimated['time_delta'] = (estimated['date_calc'] - estimated['date_forecast']).dt.total_seconds() / 3600\n",
    "      observed['time_delta'] = 0  # since observed data is not forecasting ahead\n",
    "      test['time_delta'] = (test['date_calc'] - test['date_forecast']).dt.total_seconds() / 3600\n",
    "      \n",
    "      estimated['is_estimated'] = 1\n",
    "      observed['is_estimated'] = 0\n",
    "      test['is_estimated'] = 1\n",
    "      df = pd.concat([observed, estimated], axis=0).sort_values(by='date_forecast')\n",
    "      \n",
    "      return df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(targets, observed, estimated, test):\n",
    "    \"\"\"\n",
    "    Preprocesses the data for LightGBM or CatBoost models.\n",
    "\n",
    "    :param targets: DataFrame containing target values.\n",
    "    :param observed: DataFrame with observed data.\n",
    "    :param estimated: DataFrame with estimated data.\n",
    "    :param test: DataFrame with test data.\n",
    "    :param mode: Specifies the model type ('lgbm' or 'catboost'). Default is 'lgbm'.\n",
    "    :return: Tuple containing preprocessed data for modeling and additional features.\n",
    "    \"\"\"\n",
    "\n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "\n",
    "    date_calc_resampled_ob = estimated.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    date_calc_resampled_te = test.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    \n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_ob, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_te, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    is_day_feature = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    \n",
    "    #Drop some noise\n",
    "    test_resampled = test_resampled.drop(columns =['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = observed_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    estimated_resampled = estimated_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    \n",
    "    weather_data, test_resampled = is_est(observed_resampled, estimated_resampled, test_resampled)\n",
    "    \n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "\n",
    "    # Add the time-based features\n",
    "    merged_data = add_time_features(merged_data, 'time')  \n",
    "    test_resampled = add_time_features(test_resampled, 'date_forecast') \n",
    "    \n",
    "    # Remove data where targes are zero as its no reason to train for this\n",
    "    merged_data = merged_data[merged_data['pv_measurement'] != 0]\n",
    "\n",
    "    \"\"\"\n",
    "    Removes constant stretches from the 'pv_measurement' column in a DataFrame. \n",
    "    This function is intended to address stationarity by identifying and removing blocks \n",
    "    where 'pv_measurement' remains constant for more than two consecutive data points.\n",
    "    \"\"\"\n",
    "    merged_data['diff'] = merged_data['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    merged_data['constant'] = (merged_data['diff'] == 0).astype(int)\n",
    "    merged_data['block'] = (merged_data['constant'].diff() != 0).astype(int).cumsum()\n",
    "    block_sizes = merged_data.groupby('block')['constant'].sum()\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "    filtered_data = merged_data[~merged_data['block'].isin(constant_blocks)]\n",
    "\n",
    "    # Drop time and temporary features\n",
    "    targets_ny = filtered_data[ ['time', 'pv_measurement']]\n",
    "    filtered_data = filtered_data.drop(columns=['diff', 'constant', 'block'])\n",
    "    \n",
    "    \n",
    "    # Drop time features\n",
    "    filtered_data = filtered_data.drop(columns=['time', 'pv_measurement','date_calc'])\n",
    "    test_resampled = test_resampled.drop(columns=[ 'date_forecast', 'date_calc'])\n",
    "    \n",
    "    return filtered_data, test_resampled, is_day_feature, targets_ny"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for catboost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_est_cat(observed, estimated, test):\n",
    "      # Add indicator variable for estimated data\n",
    "      estimated['is_estimated'] = 1\n",
    "      observed['is_estimated'] = 0\n",
    "      test['is_estimated'] = 1\n",
    "\n",
    "      # Merge or concatenate data\n",
    "      df = pd.concat([observed, estimated], axis=0).sort_values(by='date_forecast')\n",
    "      \n",
    "      return df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_cat(targets, observed, estimated, test):\n",
    "    \"\"\"\n",
    "    Preprocesses the data for LightGBM or CatBoost models.\n",
    "\n",
    "    :param targets: DataFrame containing target values.\n",
    "    :param observed: DataFrame with observed data.\n",
    "    :param estimated: DataFrame with estimated data.\n",
    "    :param test: DataFrame with test data.\n",
    "    :param mode: Specifies the model type ('lgbm' or 'catboost'). Default is 'lgbm'.\n",
    "    :return: Tuple containing preprocessed data for modeling and additional features.\n",
    "    \"\"\"\n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "\n",
    "    date_calc_resampled_ob = estimated.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    date_calc_resampled_te = test.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    \n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_ob, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_te, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    is_day_feature = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    \n",
    "    #Drop some noise\n",
    "    test_resampled = test_resampled.drop(columns =['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = observed_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    estimated_resampled = estimated_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "   \n",
    "    # Filter observed and estimated data for April to August\n",
    "    observed_resampled = observed_resampled[observed_resampled['date_forecast'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    estimated_resampled = estimated_resampled[estimated_resampled['date_forecast'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    targets = targets[targets['time'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    \n",
    "    weather_data, test_resampled = is_est_cat(observed_resampled, estimated_resampled, test_resampled)\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "    merged_data = add_time_features_cat(merged_data, 'time')  \n",
    "    test_resampled = add_time_features_cat(test_resampled, 'date_forecast')\n",
    "    \n",
    "    \"\"\"\n",
    "    Removes constant stretches from the 'pv_measurement' column in a DataFrame. \n",
    "    This function is intended to address stationarity by identifying and removing blocks \n",
    "    where 'pv_measurement' remains constant for more than two consecutive data points.\n",
    "    \"\"\"\n",
    "    merged_data = merged_data[merged_data['pv_measurement'] != 0]\n",
    "\n",
    "    merged_data['diff'] = merged_data['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    merged_data['constant'] = (merged_data['diff'] == 0).astype(int)\n",
    "\n",
    "    merged_data['block'] = (merged_data['constant'].diff() != 0).astype(int).cumsum()\n",
    "\n",
    "    block_sizes = merged_data.groupby('block')['constant'].sum()\n",
    "\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "    filtered_data = merged_data[~merged_data['block'].isin(constant_blocks)]\n",
    "    \n",
    "    targets_ny = filtered_data[ ['time', 'pv_measurement']]\n",
    "    filtered_data = filtered_data.drop(columns=['diff', 'constant', 'block'])\n",
    "    \n",
    "    # Drop some time features\n",
    "    filtered_data = filtered_data.drop(columns=['time', 'date_forecast', 'pv_measurement','date_calc'])\n",
    "    test_resampled = test_resampled.drop(columns=[ 'date_forecast','date_calc'])\n",
    "    \n",
    "    return filtered_data, test_resampled, is_day_feature, targets_ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['A','B','C']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(X, y, location_name,seeds):\n",
    "    \n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "\n",
    "    # Feature Combination 1: Solar Radiation and Cloud Cover Combination\n",
    "    # This feature represents a combination of direct and diffuse radiation adjusted by total cloud cover.\n",
    "    # It aims to provide a more nuanced representation of solar radiation considering cloud cover impact.\n",
    "    data['weighted_rad'] = ((data['direct_rad:W'] * (1 - data['total_cloud_cover:p']/100)) +\n",
    "                        (data['diffuse_rad:W'] * (data['total_cloud_cover:p']/100)))\n",
    "\n",
    "    # Feature Combination 2: Atmospheric Conditions Combination\n",
    "    # This feature adjusts the clear sky radiation based on absolute humidity and air density.\n",
    "    # It's an advanced representation considering the impact of atmospheric conditions on solar radiation.\n",
    "    data['adjusted_clear_sky_rad'] = (data['clear_sky_rad:W'] * np.exp(-0.0001 * data['absolute_humidity_2m:gm3']) *\n",
    "                                      (1 - 0.1 * (data['air_density_2m:kgm3'] - 1.225)))\n",
    "\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    html=True, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm)#, early_stopping=True, fold=15)\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM training and predictions\n",
    "all_predictions_lGBM_e = []\n",
    "for loc in locations:\n",
    "\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Calling preprocessing\n",
    "    X_train_1, X_test_1, is_day_feature_1, targets_1 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    \n",
    "    # Adding the extra features to the test set as well\n",
    "    X_train_1 = X_train_1.drop(columns=['date_forecast'])\n",
    "    X_test_1['weighted_rad'] = ((X_test_1['direct_rad:W'] * (1 - X_test_1['total_cloud_cover:p']/100)) +\n",
    "                        (X_test_1['diffuse_rad:W'] * (X_test_1['total_cloud_cover:p']/100)))\n",
    "\n",
    "    X_test_1['adjusted_clear_sky_rad'] = (X_test_1['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * X_test_1['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (X_test_1['air_density_2m:kgm3'] - 1.225)))\n",
    "    \n",
    "    # Training and prediction for diffrent seeds\n",
    "    total_predictions_light = None\n",
    "    seeds = [42]\n",
    "    for seed in seeds: \n",
    "        final_model_lGBM_e = train_lgbm(X_train_1, targets_1, loc, seed)\n",
    "        predictions_lGBM_e = predict_model(final_model_lGBM_e, data=X_test_1)\n",
    "        final_predictions_lGBM_e = predictions_lGBM_e['prediction_label']\n",
    "        if total_predictions_light is None:\n",
    "            total_predictions_light = np.zeros_like(final_predictions_lGBM_e)\n",
    "        total_predictions_light += final_predictions_lGBM_e\n",
    "\n",
    "    mean_pred_light = total_predictions_light/len(seeds)\n",
    "    \n",
    "    #post_process\n",
    "    adjusted_final_predictions_lGBM_e = mean_pred_light * is_day_feature_1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_lGBM_e = np.clip(adjusted_final_predictions_lGBM_e, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_lGBM_e.append([adjusted_final_predictions_lGBM_e])\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_2(X, y, location_name,seeds):\n",
    "    \n",
    "    # Dropping some features for this one model\n",
    "    features_to_drop = ['dew_or_rime:idx', #'snow_density:kgm3',\n",
    "                        'fresh_snow_3h:cm', 'fresh_snow_1h:cm', 'snow_drift:idx', \n",
    "                        'snow_depth:cm', 'wind_speed_w_1000hPa:ms', 'prob_rime:p', \n",
    "                        'fresh_snow_6h:cm', 'snow_melt_10min:mm', \n",
    "                        'fresh_snow_12h:cm', 'rain_water:kgm2', \n",
    "                        'super_cooled_liquid_water:kgm2']\n",
    "    \n",
    "    X = X.drop(columns=features_to_drop)\n",
    "    \n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "                    html=True,\n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a Catboost model\n",
    "    cat = create_model('catboost')\n",
    "\n",
    "    # Tune the model\n",
    "    tuned_cat = tune_model(cat)\n",
    "    \n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_cat = ensemble_model(tuned_cat, method='Bagging')\n",
    "\n",
    "    # Train on whole dataset\n",
    "    final_model = finalize_model(bagged_cat)\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat_2 = []\n",
    "for loc in locations:\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train, X_test, is_day_feature, targets = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Dropping date feature\n",
    "    X_train = X_train.drop(columns=['date_forecast'])\n",
    "\n",
    "    # Training and prediction for diffrent seeds\n",
    "    seeds = [42]\n",
    "    total_predictions_cat_2 = None\n",
    "    for seed in seeds: \n",
    "        final_model_cat_2 = catboost_2(X_train, targets, loc,seed)\n",
    "        predictions_cat_2 = predict_model(final_model_cat_2, X_test)\n",
    "        final_predictions_cat_2 = predictions_cat_2['prediction_label']\n",
    "        if total_predictions_cat_2 is None:\n",
    "            total_predictions_cat_2 = np.zeros_like(final_predictions_cat_2)\n",
    "            total_predictions_cat_2+=final_predictions_cat_2\n",
    "\n",
    "    mean_pred_cat_2 = total_predictions_cat_2/len(seeds)\n",
    "\n",
    "    adjusted_final_predictions_cat_2 = mean_pred_cat_2 * is_day_feature['is_day:idx']\n",
    "    adjusted_final_predictions_cat_2 = np.clip(adjusted_final_predictions_cat_2, 0, None)\n",
    "    all_predictions_cat_2.append([adjusted_final_predictions_cat_2])\n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat = []\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train, X_test, is_day_feature1, targets_cat = preprocessing_cat(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Making categorical features\n",
    "    cat_features = ['dew_or_rime:idx' ,'is_in_shadow:idx']\n",
    "    X_train['dew_or_rime:idx'] = X_train['dew_or_rime:idx'].astype(int)\n",
    "    X_train['is_in_shadow:idx'] = X_train['is_in_shadow:idx'].astype(int)\n",
    "    X_test['dew_or_rime:idx'] = X_test['dew_or_rime:idx'].astype(int)\n",
    "    X_test['is_in_shadow:idx'] = X_test['is_in_shadow:idx'].astype(int)\n",
    "\n",
    "    # Catboooooooozt fun\n",
    "    model_cat = CatBoostRegressor(\n",
    "        loss_function='MAE', \n",
    "        learning_rate=0.1, \n",
    "        verbose=200,\n",
    "        cat_features=cat_features,\n",
    "        random_state=42) \n",
    "    # Training\n",
    "    model_cat.fit(X_train,targets_cat['pv_measurement'])\n",
    "\n",
    "    # Prediction\n",
    "    predictions_cat = model_cat.predict(X_train)\n",
    "    feature_importances = model_cat.get_feature_importance()\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat = predictions_cat * is_day_feature1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat = np.clip(adjusted_final_predictions_cat, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat.append(adjusted_final_predictions_cat)\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_cat = np.array(all_predictions_cat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat_3 = []\n",
    "for loc in locations:\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train_3, X_test_3, is_day_feature_3, targets_3 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Dropping date feature\n",
    "    X_train_3 = X_train_3.drop(columns=['date_forecast'])\n",
    "    \n",
    "    model_cat_3 = CatBoostRegressor(\n",
    "                                    verbose=200, \n",
    "                                    learning_rate=0.03,\n",
    "                                    depth=10,\n",
    "                                    l2_leaf_reg=5,\n",
    "                                    random_state=42, \n",
    "                                    n_estimators=20000, \n",
    "                                    loss_function='MAE', \n",
    "                                    early_stopping_rounds=100,)\n",
    "    '''\n",
    "    Adds engineered features based on the sun azimuth angle to the DataFrame.\n",
    "\n",
    "    This function calculates the sine and cosine of the sun azimuth angle, \n",
    "    which can be useful for capturing cyclical patterns related to the sun's position.\n",
    "    '''\n",
    "    X_train_3['sin_sun_azimuth'] = np.sin(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_train_3['cos_sun_azimuth'] = np.cos(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_test_3['sin_sun_azimuth'] = np.sin(np.radians(X_test_3['sun_azimuth:d']))\n",
    "    X_test_3['cos_sun_azimuth'] = np.cos(np.radians(X_test_3['sun_azimuth:d']))\n",
    "\n",
    "    X_train_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    X_test_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "\n",
    "  \n",
    "    # Split the training data into training and validation sets\n",
    "    X_train_cat_3, X_test_cat_3, y_train_cat_3, y_test_cat_3 = train_test_split(X_train_3, targets_3, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model_cat_3.fit(X_train_cat_3, y_train_cat_3['pv_measurement'],eval_set=(X_test_cat_3, y_test_cat_3['pv_measurement']),)  \n",
    "    \n",
    "    pred_cat_3 = model_cat_3.predict(X_test_3)\n",
    "\n",
    "    adjusted_final_predictions_cat_3 = pred_cat_3 * is_day_feature_3['is_day:idx']\n",
    "\n",
    "    adjusted_final_predictions_cat_3 = np.clip(adjusted_final_predictions_cat_3, 0, None)\n",
    "    all_predictions_cat_3.append(adjusted_final_predictions_cat_3) \n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with some extra features\n",
    "def train_xgb(X, y, location_name,seeds):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Added some extra features to this one model, did it here so we could reuse the same preprocesssing function on diffrent models\n",
    "    # Feature Combination 1: Solar Radiation and Cloud Cover Combination\n",
    "    data['weighted_rad'] = ((data['direct_rad:W'] * (1 - data['total_cloud_cover:p']/100)) +\n",
    "                        (data['diffuse_rad:W'] * (data['total_cloud_cover:p']/100)))\n",
    "\n",
    "    # Feature Combination 2: Atmospheric Conditions Combination\n",
    "    data['adjusted_clear_sky_rad'] = (data['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * data['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (data['air_density_2m:kgm3'] - 1.225)))  # Adjusted based on humidity and air density\n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    xgb = create_model('xgboost')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_xgb = tune_model(xgb, optimize='MAE')\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_xgb = ensemble_model(tuned_xgb, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_xgb)\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = []  # Initialize a list to store results for each location.\n",
    "\n",
    "for loc in locations:  # Iterate through each location.\n",
    "    # Load data specific to the current location.\n",
    "    train, X_train_estimated, X_train_observed, X_test_estimated = load(loc)\n",
    "    \n",
    "    # Preprocess the data.\n",
    "    X_train, X_test, is_day_feature, targets = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    X_train = X_train.drop(columns=['date_forecast'])\n",
    "    X_test['weighted_rad'] = ((X_test['direct_rad:W'] * (1 - X_test['total_cloud_cover:p']/100)) +\n",
    "                        (X_test['diffuse_rad:W'] * (X_test['total_cloud_cover:p']/100)))\n",
    "\n",
    "    X_test['adjusted_clear_sky_rad'] = (X_test['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * X_test['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (X_test['air_density_2m:kgm3'] - 1.225)))\n",
    "    \n",
    "\n",
    "    # Train the XGBoost model using a custom function (train_xgb).\n",
    "    final_model_xgb = train_xgb(X_train, targets, loc, 42)\n",
    "\n",
    "    # Make predictions on the test dataset using the trained model.\n",
    "    predictions_xgb = predict_model(final_model_xgb, data=X_test)\n",
    "    final_predictions_xgb = predictions_xgb['prediction_label']\n",
    "        \n",
    "    # Adjust the predictions by multiplying with the 'is_day:idx' feature.\n",
    "    final_predictions_xgb_filter = final_predictions_xgb * is_day_feature['is_day:idx']\n",
    "\n",
    "    # Clip the predictions to ensure they are non-negative.\n",
    "    final_predictions_xgb_filter = np.clip(final_predictions_xgb_filter, 0, None)\n",
    "\n",
    "    # Append the adjusted predictions for each location to the 'all_predictions_xb' list.\n",
    "    xb.append([final_predictions_xgb_filter])\n",
    "\n",
    "# Convert the list of predictions for all locations into a numpy array.\n",
    "xb = np.array(xb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e).flatten()\n",
    "all_predictions_xb = np.array(xb).flatten()\n",
    "all_predictions_cat = np.array(all_predictions_cat).flatten()\n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2).flatten()\n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3).flatten()\n",
    "all_pred = 0.2*all_predictions_cat+0.2 * all_predictions_lGBM_e+ 0.2* all_predictions_cat_2+0.2*all_predictions_cat_3 + 0.2*all_predictions_xb\n",
    "all_pred[all_pred < 6] = 0\n",
    "print(all_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_predictions = all_pred\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df.to_csv('final_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-name",
   "language": "python",
   "name": "pycaret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
