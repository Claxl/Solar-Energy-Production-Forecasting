{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Nh0NKRBlKpy6",
      "metadata": {
        "id": "Nh0NKRBlKpy6"
      },
      "source": [
        "# Long Notebook\n",
        "-   Beatrice Re 104972\n",
        "-   Claudio Di Salvo 105677\n",
        "-   Raymond Li 105281\n",
        "\n",
        "Group Name : 10 TimeBenders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b9afd23",
      "metadata": {
        "id": "0b9afd23"
      },
      "source": [
        "# Analysis\n",
        "We have done the same analysis on the other location, we omitted them for readability of the notebook.\n",
        "\n",
        "\n",
        "Attention, this notebook is only partially runnable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qG383UWxCPjy",
      "metadata": {
        "id": "qG383UWxCPjy"
      },
      "source": [
        "## 1. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f34a861",
      "metadata": {
        "id": "2f34a861"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gXFSSLefC4QG",
      "metadata": {
        "id": "gXFSSLefC4QG"
      },
      "source": [
        "## 2. Data Reading and intuitive analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yCSqSKB0ett5",
      "metadata": {
        "id": "yCSqSKB0ett5"
      },
      "outputs": [],
      "source": [
        "def readData(location):\n",
        "  if(location == 1):\n",
        "    y_train_obs_A = pd.read_parquet('data/A/train_targets.parquet')\n",
        "    X_test_est_A = pd.read_parquet('data/A/X_test_estimated.parquet')\n",
        "    X_train_est_A = pd.read_parquet('data/A/X_train_estimated.parquet')\n",
        "    X_train_obs_A = pd.read_parquet('data/A/X_train_observed.parquet')\n",
        "    return y_train_obs_A, X_test_est_A, X_train_est_A, X_train_obs_A\n",
        "  elif(location == 2):\n",
        "    y_train_obs_B = pd.read_parquet('data/B/train_targets.parquet')\n",
        "    X_test_est_B = pd.read_parquet('data/B/X_test_estimated.parquet' )\n",
        "    X_train_est_B = pd.read_parquet('data/B/X_train_estimated.parquet')\n",
        "    X_train_obs_B = pd.read_parquet('data/B/X_train_observed.parquet')\n",
        "    return y_train_obs_B, X_test_est_B , X_train_est_B, X_train_obs_B\n",
        "  else:\n",
        "    y_train_obs_C = pd.read_parquet('data/C/train_targets.parquet')\n",
        "    X_test_est_C = pd.read_parquet('data/C/X_test_estimated.parquet')\n",
        "    X_train_est_C = pd.read_parquet('data/C/X_train_estimated.parquet')\n",
        "    X_train_obs_C = pd.read_parquet('data/C/X_train_observed.parquet')\n",
        "    return y_train_obs_C, X_test_est_C, X_train_est_C, X_train_obs_C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd6a608d",
      "metadata": {
        "id": "cd6a608d"
      },
      "outputs": [],
      "source": [
        "df_train_est_A, df_train_obs_A, df_test_A, target = readData(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1GZ3E_RzfPrZ",
      "metadata": {
        "id": "1GZ3E_RzfPrZ"
      },
      "source": [
        "The date_forecast must be the index since we are predicting the future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156d0950",
      "metadata": {
        "id": "156d0950"
      },
      "outputs": [],
      "source": [
        "df_train_est_A.set_index('date_forecast', inplace=True)\n",
        "df_train_obs_A.set_index('date_forecast', inplace=True)\n",
        "df_test_A.set_index('date_forecast', inplace=True)\n",
        "target.set_index('time',inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WCdCkzBPf9Ee",
      "metadata": {
        "id": "WCdCkzBPf9Ee"
      },
      "outputs": [],
      "source": [
        "df_train = pd.concat([df_train_obs_A,df_train_est_A])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9mPCkfN8AnAD",
      "metadata": {
        "id": "9mPCkfN8AnAD"
      },
      "source": [
        "### 2.1. Visual Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83vtQogrfkLb",
      "metadata": {
        "id": "83vtQogrfkLb"
      },
      "source": [
        "We decided to scale the data to have a proper way to visualize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QTzmklnXgXbB",
      "metadata": {
        "id": "QTzmklnXgXbB"
      },
      "outputs": [],
      "source": [
        "def scale_df(df):\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
        "    return df_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66e5uY9-doZ5",
      "metadata": {
        "id": "66e5uY9-doZ5"
      },
      "outputs": [],
      "source": [
        "df = df_train\n",
        "df_target = target\n",
        "\n",
        "df.index = pd.to_datetime(df.index)\n",
        "df_target.index = pd.to_datetime(df_target.index)\n",
        "\n",
        "df_scaled = scale_df(df)\n",
        "# Ensuring that df_target contains only numeric columns\n",
        "df_target_numeric = df_target.select_dtypes(include=[np.number])\n",
        "df_target_scaled = scale_df(df_target_numeric)\n",
        "\n",
        "# Number of features to plot\n",
        "num_features = len(df.columns)\n",
        "\n",
        "# Creating a figure\n",
        "plt.figure(figsize=(15, 5*num_features))\n",
        "\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    plt.subplot(num_features, 1, i)\n",
        "\n",
        "    plt.plot( df_scaled[col], label=f'Scaled Feature: {col}', linestyle='-', marker='o')\n",
        "    plt.plot( df_target_scaled['pv_measurement'], label='Scaled PV Measurement', linestyle='-', marker='o')\n",
        "\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Scaled Values')\n",
        "    plt.title(f'Scaled {col} and PV Measurement over Time')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JjHDu12dgNhb",
      "metadata": {
        "id": "JjHDu12dgNhb"
      },
      "source": [
        "The data visualization clearly indicated that pv_measurement values were higher in summer, which could be attributed to the more intense solar radiation resulting from cloudless skies and longer daylight hours."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LBtoGsyVHFYe",
      "metadata": {
        "id": "LBtoGsyVHFYe"
      },
      "source": [
        "## 3. EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrrTx2eTIRSN",
      "metadata": {
        "id": "wrrTx2eTIRSN"
      },
      "source": [
        "### 3.1 Impute the NaN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f606aea8",
      "metadata": {
        "id": "f606aea8"
      },
      "outputs": [],
      "source": [
        "df_train.isnull().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yNdHt995gzg1",
      "metadata": {
        "id": "yNdHt995gzg1"
      },
      "source": [
        "We encountered a few columns with a high percentage of missing values (NaNs). Consequently, we chose to drop two columns ('snow_density:kgm3', 'elevation:m') and impute the remaining ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "glPvAstOhMxn",
      "metadata": {
        "id": "glPvAstOhMxn"
      },
      "outputs": [],
      "source": [
        "df_train.drop[['snow_density:kgm3', 'elevation:m']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xr2WrOlZhRGz",
      "metadata": {
        "id": "Xr2WrOlZhRGz"
      },
      "source": [
        "To address the missing values, we experimented with various imputation methods. Our initial approach involved using a neighbor-based technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78dd345d",
      "metadata": {
        "id": "78dd345d"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=24)\n",
        "df_train_est_A_imputed = pd.DataFrame(imputer.fit_transform(df), columns = df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WHdFJqeqhdy6",
      "metadata": {
        "id": "WHdFJqeqhdy6"
      },
      "source": [
        "This was too much expensive in computational POV. So we stepped over."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uTiRiKNOh2-w",
      "metadata": {
        "id": "uTiRiKNOh2-w"
      },
      "source": [
        "To determine the most effective imputation method, we established an evaluation protocol based on LightGBM. However, none of the imputation solutions we tested in this evaluation were ultimately utilized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vob8a7gWiIHp",
      "metadata": {
        "id": "vob8a7gWiIHp"
      },
      "outputs": [],
      "source": [
        "df_nandrop = df\n",
        "df_nandrop = df_nandrop.drop(columns= ['snow_densitykgm3'],axis = 1)\n",
        "df_nandrop = df_nandrop.dropna()\n",
        "\n",
        "df_nan0 = df.fillna(0)\n",
        "df_nanmedian = df.fillna(df.median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I2DBJM0OhuYp",
      "metadata": {
        "id": "I2DBJM0OhuYp"
      },
      "outputs": [],
      "source": [
        "df_dict = {\n",
        "    'drop' : df_nandrop,\n",
        "    'zero' : df_nan0,\n",
        "    'median' : df_nanmedian,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afN1tmq9iSBV",
      "metadata": {
        "id": "afN1tmq9iSBV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "# Define the number of splits (e.g., 5 for 80-20 train-test splits)\n",
        "n_splits = 3\n",
        "# Initialize the TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8TXDLKm6iLTU",
      "metadata": {
        "id": "8TXDLKm6iLTU"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "param = {'verbose' :-1}\n",
        "\n",
        "model = lgb.LGBMRegressor(**param)\n",
        "def test(df_dict):\n",
        "    mean_mae_list = []\n",
        "    for key in df_dict:\n",
        "        print(key)\n",
        "        data = df_dict[key]\n",
        "        maelist = []\n",
        "        for train_index, test_index in tscv.split(data):\n",
        "            train_data = data.iloc[train_index]\n",
        "            test_data = data.iloc[test_index]\n",
        "\n",
        "            # Extract target variable for training and testing data\n",
        "            y_train = train_data['pv_measurement']\n",
        "            y_test = test_data['pv_measurement']\n",
        "\n",
        "            # Extract features for training and testing data\n",
        "            X_train = train_data.drop(columns = 'pv_measurement')\n",
        "            X_test = test_data.drop(columns = 'pv_measurement')\n",
        "\n",
        "            # Train the XGBoost model\n",
        "            model.fit(X_train, y_train,eval_set=[(X_test,y_test)])\n",
        "\n",
        "            # Make predictions on the test data\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Evaluate the model using Mean Absolute Error (MAE)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            print(\"Mean Absolute Error:\", mae)\n",
        "            maelist.append(mae)\n",
        "        # Return MAE\n",
        "        mean_mae = np.average(maelist,weights=[1,2,3])\n",
        "        print(\"Mean MAE: \",mean_mae )\n",
        "        mean_mae_list.append(mean_mae)\n",
        "    return mean_mae_list\n",
        "test(df_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VTPIN5hviX-g",
      "metadata": {
        "id": "VTPIN5hviX-g"
      },
      "source": [
        "After this evaluation metrics we found out the iterative imputation and we used this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df84W93pimAA",
      "metadata": {
        "id": "df84W93pimAA"
      },
      "outputs": [],
      "source": [
        "imputer = IterativeImputer(max_iter=5, random_state=42)\n",
        "for col in X_train_cat.columns:\n",
        "        X_train_cat[col] = imputer.fit_transform(np.array(X_train_cat[col]).reshape(-1,1))\n",
        "for col in X_test_cat.columns:\n",
        "        X_test_cat[col] = imputer.fit_transform(np.array(X_test_cat[col]).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VZ4yH3wbJAfF",
      "metadata": {
        "id": "VZ4yH3wbJAfF"
      },
      "source": [
        "### 3.2. Outliers Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zd1bHxccixlu",
      "metadata": {
        "id": "zd1bHxccixlu"
      },
      "source": [
        "The presence of outliers poses significant challenges for certain models. Although we investigated this issue, we ultimately did not address it because we are employing a gradient boosting model, which is less affected by outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oyWQp9K_PXU4",
      "metadata": {
        "id": "oyWQp9K_PXU4"
      },
      "source": [
        "#### 3.2.1 IQR for Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d78fde",
      "metadata": {
        "id": "47d78fde"
      },
      "outputs": [],
      "source": [
        "Q1 = X_train.quantile(0.25)\n",
        "Q3 = X_train.quantile(0.75)\n",
        "\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "limite_inferiore = Q1 - 3 * IQR\n",
        "limite_superiore = Q3 + 3 * IQR\n",
        "\n",
        "for colonna in X_train.columns:\n",
        "    X_train[colonna] = X_train[colonna].apply(lambda x: limite_inferiore[colonna] if x < limite_inferiore[colonna] else (limite_superiore[colonna] if x > limite_superiore[colonna] else x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0cc1f3",
      "metadata": {
        "id": "7d0cc1f3"
      },
      "source": [
        "#### 3.2.2 Z-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b629c0a",
      "metadata": {
        "id": "5b629c0a"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "def removal_outliers(df):\n",
        "    # Sample dataset with outliers\n",
        "    df_no_outliers = df\n",
        "    # Calculate Z-scores for each column\n",
        "    z_scores = np.abs(stats.zscore(df_no_outliers))\n",
        "\n",
        "    # Define a Z-score threshold (e.g., 2 standard deviations)\n",
        "    z_score_threshold = 3\n",
        "\n",
        "    # Create a boolean mask for identifying outliers\n",
        "    outlier_mask = z_scores > z_score_threshold\n",
        "    # Define the window size (12 values before and 12 values after)\n",
        "    window_size = 12\n",
        "    j =0\n",
        "    # Replace outliers with the mean of the surrounding values for each column\n",
        "    for column in df_no_outliers.columns:\n",
        "        for i in range(len(df_no_outliers)):\n",
        "            if outlier_mask[column][i]:\n",
        "                j = j+1\n",
        "                start = max(i - window_size, 0)\n",
        "                end = min(i + window_size + 1, len(df_no_outliers))\n",
        "                df_no_outliers[column][i] = df_no_outliers[column][start:end].mean()\n",
        "    print(j)\n",
        "    return df_no_outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720a0845",
      "metadata": {
        "id": "720a0845"
      },
      "outputs": [],
      "source": [
        "df_train_A_no_outliers = removal_outliers(df_train_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c29caa",
      "metadata": {
        "id": "c6c29caa"
      },
      "source": [
        "### 3.3 Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MQSTUorE6jqm",
      "metadata": {
        "id": "MQSTUorE6jqm"
      },
      "source": [
        "After we moved into scaling the data, but also this was not worth the time spent and the extra computational cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b182975c",
      "metadata": {
        "id": "b182975c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "norm = StandardScaler()\n",
        "X_norm =pd.DataFrame( norm.fit_transform(df[scaling_columns]), columns = df[scaling_columns].columns, index = df.index)\n",
        "X_norm.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9175c635",
      "metadata": {
        "id": "9175c635"
      },
      "source": [
        "### 3.4 Correlation Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tOwCOUGw7UJu",
      "metadata": {
        "id": "tOwCOUGw7UJu"
      },
      "source": [
        "Our strategy was to reduce the dataset's dimensionality by eliminating the most correlated features. However, this approach did not yield the desired results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U1kM9BteFyND",
      "metadata": {
        "id": "U1kM9BteFyND"
      },
      "source": [
        "#### 3.4.1 Correlation between target and features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bef2363d",
      "metadata": {
        "id": "bef2363d"
      },
      "outputs": [],
      "source": [
        "target = 'pv_measurement'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "942ed650",
      "metadata": {
        "id": "942ed650",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Create the dataframe\n",
        "df = pd.concat([df_train_obs_A, df_train_est_A], ignore_index=True)\n",
        "# Correlation matrix\n",
        "correlation_matrix = df.corrwith(df[target])\n",
        "plt.figure(figsize=(20, 20))\n",
        "ax = sns.barplot(x=correlation_matrix.values, y=correlation_matrix.index, orient='h')\n",
        "plt.title('Correlation with pv')\n",
        "plt.xlabel('Correlation')\n",
        "plt.ylabel('Feature')\n",
        "plt.xticks(rotation=90)\n",
        "for i, v in enumerate(correlation_matrix.values):\n",
        "    ax.text(v, i, f'{v:.2f}', ha='left', va='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe619cf",
      "metadata": {
        "id": "efe619cf"
      },
      "source": [
        "#### 3.4.2 Correlation between features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d1feae9",
      "metadata": {
        "id": "1d1feae9"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df_train_obs_A, df_train_est_A], ignore_index=True)\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Define a threshold\n",
        "threshold = 0.0\n",
        "\n",
        "# Find the columns to be kept\n",
        "columns_to_keep = correlation_matrix.columns[(correlation_matrix >= threshold).sum() >=0]\n",
        "\n",
        "# Select only those columns satisfying the criterium\n",
        "filtered_corr_matrix = correlation_matrix.loc[columns_to_keep, columns_to_keep]\n",
        "mask_upper = np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\",mask=mask_upper)\n",
        "plt.title(f'Correlation Heatmap (Threshold = {threshold})')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wKKc1o8j8FKN",
      "metadata": {
        "id": "wKKc1o8j8FKN"
      },
      "source": [
        "### 3.5 PairPlot\n",
        "Following this, we generated a pairplot to examine potential combinations of features that could reveal additional insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lBW-gMnn8TZa",
      "metadata": {
        "id": "lBW-gMnn8TZa"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zDmf9Uv88nQl",
      "metadata": {
        "id": "zDmf9Uv88nQl"
      },
      "source": [
        "## 4. Dimension Reduction\n",
        "Our objective was to apply analytical methods to decrease the dataset's dimensionality, particularly because the existing correlations did not align well with the problem at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73e22058",
      "metadata": {
        "id": "73e22058"
      },
      "source": [
        "#### 4.1. PCA & Pipeline\n",
        "We incorporated PCA into our pipeline to test its effectiveness in solving the problem. However, this approach also proved unsuccessful. Spoiler alert: none of the dimension reduction techniques we tried worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d07ab85e",
      "metadata": {
        "id": "d07ab85e"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(steps=[('Scaler', Normalizer()),\n",
        "                                 ('PCA', PCA(n_components=40)),\n",
        "                                 ('Forest', RandomForestRegressor(n_estimators=100, max_depth=5))])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JpQKWNi3Ngoe",
      "metadata": {
        "id": "JpQKWNi3Ngoe"
      },
      "source": [
        "#### 4.2. Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8EChpKaiNosv",
      "metadata": {
        "id": "8EChpKaiNosv"
      },
      "source": [
        "##### 4.2.1. Feature importance with Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572a0362",
      "metadata": {
        "id": "572a0362"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(scaled_X_train, y_train)\n",
        "# get importance\n",
        "importance = rf.feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886fd7d4",
      "metadata": {
        "id": "886fd7d4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dataRF = pd.DataFrame()\n",
        "dataRF['Feature_name'] = scaled_X_train.columns\n",
        "dataRF['importance'] = importance\n",
        "dataRF = dataRF.sort_values(by='importance',ascending = True)\n",
        "plt.figure(figsize=(20, 25))  # width:20, height:3\n",
        "\n",
        "plot = plt.barh(dataRF['Feature_name'],dataRF['importance'])\n",
        "plt.bar_label(plot)\n",
        "plt.rc('ytick', labelsize=8)    # fontsize of the tick labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca3760e",
      "metadata": {
        "id": "3ca3760e"
      },
      "outputs": [],
      "source": [
        "threshold = 0.021\n",
        "\n",
        "dataRF = dataRF[dataRF['importance'] > threshold]\n",
        "dataRF['Feature_name'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aC0VZH_ZNyA2",
      "metadata": {
        "id": "aC0VZH_ZNyA2"
      },
      "source": [
        "##### 4.2.2. Feature importance with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de26e8fc",
      "metadata": {
        "id": "de26e8fc"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "model = xgb.XGBRegressor()\n",
        "model.fit(scaled_X_train, scaled_Y_train)\n",
        "\n",
        "  # get importance\n",
        "importance = model.feature_importances_\n",
        "dataXGB = pd.DataFrame()\n",
        "dataXGB['Feature_name'] = scaled_X_train.columns\n",
        "dataXGB['importance'] = importance\n",
        "dataXGB = dataXGB.sort_values(by='importance',ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c75851ed",
      "metadata": {
        "id": "c75851ed",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(20, 25))  # width:20, height:3\n",
        "\n",
        "plot = plt.barh(dataXGB['Feature_name'],dataXGB['importance'])\n",
        "plt.bar_label(plot)\n",
        "plt.rc('ytick', labelsize=8)    # fontsize of the tick labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cd9ced3",
      "metadata": {
        "id": "1cd9ced3"
      },
      "outputs": [],
      "source": [
        "# Specifica il threshold\n",
        "threshold = 0.01\n",
        "\n",
        "# Seleziona le righe in cui il valore nella colonna 'Colonna1' supera il threshold\n",
        "dataXGB = dataXGB[dataXGB['importance'] > threshold]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8g4QmhKlN4bZ",
      "metadata": {
        "id": "8g4QmhKlN4bZ"
      },
      "source": [
        "##### 4.2.3. ExhaustiveFeatureSelector\n",
        "This took us 30hours of run and was the most useless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d767013",
      "metadata": {
        "id": "8d767013",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
        "\n",
        "# IMPORT MODEL TO EVALUATE WITH FEATURES\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "SEED = 42\n",
        "\n",
        "models = {\n",
        "    #'rf': RandomForestRegressor(random_state=SEED,),\n",
        "   # 'lgbm': LGBMRegressor(),\n",
        "    'xgb': XGBRegressor(random_state=SEED,),\n",
        "    #'gb': GradientBoostingRegressor(random_state=SEED,),\n",
        "    #'cb': CatBoostRegressor(random_state=SEED, silent=True),\n",
        "    #'ab': AdaBoostRegressor(random_state=SEED,),\n",
        "}\n",
        "\n",
        "for key in models:\n",
        "    print(key)\n",
        "    model = models[key]\n",
        "    efs = ExhaustiveFeatureSelector(model,\n",
        "                                   min_features=3,\n",
        "                                   max_features=5,\n",
        "                                   scoring='neg_mean_absolute_error',\n",
        "                                   cv=5,\n",
        "                                   print_progress=True\n",
        "                                   )\n",
        "    efs = efs.fit(X_train, np.ravel(Y_train))\n",
        "\n",
        "    selected_features = X_train.columns[list(efs.best_idx_)]\n",
        "    print(selected_features)\n",
        "    print(efs.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eTpVCXdt-XtM",
      "metadata": {
        "id": "eTpVCXdt-XtM"
      },
      "source": [
        "## 5 Data Transformation\n",
        "When scaling the data did not yield results, we shifted our focus to transforming the data to achieve a more Gaussian-like distribution. This decision was influenced by our observation (as noted in section 5.0.1) that most of the columns exhibited right skewness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BbliYoZb-ubi",
      "metadata": {
        "id": "BbliYoZb-ubi"
      },
      "source": [
        "### 5.0.1 Histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5260cf13",
      "metadata": {
        "id": "5260cf13",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "data = df\n",
        "num_features = len(data.columns)\n",
        "num_rows = math.ceil(num_features / 3)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for i, column in enumerate(data.columns):\n",
        "    plt.hist(data[column], bins=20, color='blue', alpha=0.7)\n",
        "    plt.title(column)\n",
        "    plt.xlabel(\"Valore\")\n",
        "    plt.ylabel(\"Frequenza\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6nL3O9lC_CEl",
      "metadata": {
        "id": "6nL3O9lC_CEl"
      },
      "source": [
        "### 5.1. Transformation\n",
        "We performed this for the train data and the target."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CnVpF-G8_W-D",
      "metadata": {
        "id": "CnVpF-G8_W-D"
      },
      "source": [
        "#### 5.1.1 Log Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7aec786",
      "metadata": {
        "id": "a7aec786"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "original_data = df_train_est_A_imputed['pv_measurement']\n",
        "\n",
        "\n",
        "# Adding a small constant to avoid division by zero\n",
        "epsilon = 1e-6\n",
        "data_with_epsilon = [x + epsilon if x == 0 else x for x in original_data]\n",
        "\n",
        "# Apply the Box-Cox transformation\n",
        "transformed_data = np.log(original_data + 1)\n",
        "\n",
        "# Create a DataFrame to see the original and transformed data side by side\n",
        "df = pd.DataFrame({'Original Data': original_data, 'Transformed Data': transformed_data})\n",
        "\n",
        "# Create a figure and two subplots for original and transformed data\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "fig.suptitle('Original vs. Transformed Data')\n",
        "\n",
        "# Plot the original data\n",
        "axes[0].hist(original_data, bins=5, edgecolor='black', alpha=0.7, color='blue')\n",
        "axes[0].set_title('Original Data')\n",
        "axes[0].set_xlabel('Value')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# Plot the transformed data\n",
        "axes[1].hist(transformed_data, bins=5, edgecolor='black', alpha=0.7, color='green')\n",
        "axes[1].set_title('Transformed Data')\n",
        "axes[1].set_xlabel('Box-Cox Transformed Value')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b285d75",
      "metadata": {
        "id": "2b285d75"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "original_data = df_dew_rime['pv_measurement']\n",
        "\n",
        "# Apply the Box-Cox transformation\n",
        "transformed_data = np.log(original_data+1)\n",
        "\n",
        "# Create a DataFrame to see the original and transformed data side by side\n",
        "df = pd.DataFrame({'Original Data': original_data, 'Transformed Data': transformed_data})\n",
        "\n",
        "# Create a figure and two subplots for original and transformed data\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "fig.suptitle('Original vs. Transformed Data')\n",
        "\n",
        "# Plot the original data\n",
        "axes[0].hist(original_data, bins=5, edgecolor='black', alpha=0.7, color='blue')\n",
        "axes[0].set_title('Original Data')\n",
        "axes[0].set_xlabel('Value')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# Plot the transformed data\n",
        "axes[1].hist(transformed_data, bins=5, edgecolor='black', alpha=0.7, color='green')\n",
        "axes[1].set_title('Transformed Data')\n",
        "axes[1].set_xlabel('Box-Cox Transformed Value')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfcn9YeEAGYB",
      "metadata": {
        "id": "bfcn9YeEAGYB"
      },
      "source": [
        "## 6. Maybe all location together?\n",
        "We attempted to unify all the locations, but encountered several issues, such as encoding the locations and significant differences in feature scales. These challenges led us to abandon this approach and instead continue with the use of three separate models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a77143",
      "metadata": {
        "id": "21a77143"
      },
      "source": [
        "### 6.1 One hot encoder\n",
        "This was our idea to fix the location encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb62044f",
      "metadata": {
        "id": "eb62044f"
      },
      "outputs": [],
      "source": [
        "locations = df_train['Location']\n",
        "df = pd.get_dummies(df_train, columns=['Location'], prefix=['Location_'])\n",
        "location_df.index = df_train.index\n",
        "location_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "376dc816",
      "metadata": {
        "id": "376dc816"
      },
      "outputs": [],
      "source": [
        "df_with_location = pd.concat([df_train.drop(columns='Location'), location_df], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KVuhlxP1A970",
      "metadata": {
        "id": "KVuhlxP1A970"
      },
      "source": [
        "## 7. Feature Engineering\n",
        "After all this attempt we moved on the feature Engineering and tried to combine the most useful feature, and tried to catch some seasonality with the time information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DXWzzgreBktG",
      "metadata": {
        "id": "DXWzzgreBktG"
      },
      "source": [
        "### 7.1 Time Feature\n",
        "We attempted to develop features that could capture the seasonality of the PV output. However, as we discovered through the feature importance analysis in section 4.2, each model required distinct time-related features. Consequently, we created two separate sets of these features to accommodate the varying needs of each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2dac95a",
      "metadata": {
        "id": "c2dac95a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def add_time_features(df, time_column):\n",
        "\n",
        "    df[time_column] = pd.to_datetime(df[time_column])\n",
        "    df['hour'] = df[time_column].dt.hour\n",
        "    df['day_of_week'] = df[time_column].dt.dayofweek\n",
        "    df['month'] = df[time_column].dt.month\n",
        "    df['day_of_year'] = df[time_column].dt.dayofyear\n",
        "    df['week_of_year'] = df[time_column].dt.isocalendar().week\n",
        "    df['year'] = df[time_column].dt.year\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6QN_Jt9ICHtq",
      "metadata": {
        "id": "6QN_Jt9ICHtq"
      },
      "outputs": [],
      "source": [
        "def add_time_features_cat(df, time_column):\n",
        "\n",
        "    df[time_column] = pd.to_datetime(df[time_column])\n",
        "\n",
        "    df['sin_hour'] = np.sin(np.pi * df[time_column].dt.hour/23.)\n",
        "    df['sin_month'] = np.sin(np.pi * df[time_column].dt.month/12.)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NRNiwDddCKFv",
      "metadata": {
        "id": "NRNiwDddCKFv"
      },
      "outputs": [],
      "source": [
        "      estimated['time_dummy'] = (estimated['date_forecast'] - estimated['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
        "      observed['time_dummy'] = 0\n",
        "      test['time_dummy'] = (test['date_forecast'] - test['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
        "\n",
        "      estimated['time_delta'] = (estimated['date_calc'] - estimated['date_forecast']).dt.total_seconds() / 3600\n",
        "      observed['time_delta'] = 0  # since observed data is not forecasting ahead\n",
        "      test['time_delta'] = (test['date_calc'] - test['date_forecast']).dt.total_seconds() / 3600"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B9sRl-MLCNku",
      "metadata": {
        "id": "B9sRl-MLCNku"
      },
      "source": [
        "### 7.2 Is_estimated feature\n",
        "These features proved to be highly beneficial, both for us and for the model, in more accurately discerning the differences between the estimated and the observed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l_91ZtbtChj6",
      "metadata": {
        "id": "l_91ZtbtChj6"
      },
      "outputs": [],
      "source": [
        "      estimated['is_estimated'] = 1\n",
        "      observed['is_estimated'] = 0\n",
        "      test['is_estimated'] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ED_cgGVOCjqw",
      "metadata": {
        "id": "ED_cgGVOCjqw"
      },
      "source": [
        "### 7.3 Combination between radiation and cloud\n",
        "These features were pivotal in our project as they provided the model with an insight into the actual solar radiation at any given time, thereby offering a better estimate of the potential PV output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K9SvrW89C60A",
      "metadata": {
        "id": "K9SvrW89C60A"
      },
      "outputs": [],
      "source": [
        " data['weighted_rad'] = ((data['direct_rad:W'] * (1 - data['total_cloud_cover:p']/100)) +\n",
        "                        (data['diffuse_rad:W'] * (data['total_cloud_cover:p']/100)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l1V-LGaHC9uB",
      "metadata": {
        "id": "l1V-LGaHC9uB"
      },
      "source": [
        "#### 7.4 Atmospheric combination\n",
        "Recognizing the significant impact of humidity and air density on solar radiation, we attempted to combine these factors to capture more detailed insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RGc2fWJ7DU7_",
      "metadata": {
        "id": "RGc2fWJ7DU7_"
      },
      "outputs": [],
      "source": [
        "data['adjusted_clear_sky_rad'] = (data['clear_sky_rad:W'] *\n",
        "                                  np.exp(-0.0001 * data['absolute_humidity_2m:gm3']) *\n",
        "                                  (1 - 0.1 * (data['air_density_2m:kgm3'] - 1.225)))  # Adjusted based on humidity and air density"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3hn2-ANlDeEd",
      "metadata": {
        "id": "3hn2-ANlDeEd"
      },
      "source": [
        "#### 7.5 Sun Azimuth feature\n",
        "The idea behind was to captures the cyclical nature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZPpQa8ZcDwR9",
      "metadata": {
        "id": "ZPpQa8ZcDwR9"
      },
      "outputs": [],
      "source": [
        "    df['sin_sun_azimuth'] = np.sin(np.radians(df['sun_azimuth:d']))\n",
        "\n",
        "    df['cos_sun_azimuth'] = np.cos(np.radians(df['sun_azimuth:d']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OkGppf_5D4Hj",
      "metadata": {
        "id": "OkGppf_5D4Hj"
      },
      "source": [
        "#### 7.6 Other idea that didn't work out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UqO3MgvTD9RY",
      "metadata": {
        "id": "UqO3MgvTD9RY"
      },
      "source": [
        "##### 7.6.1 Sun Zenith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f91afdf",
      "metadata": {
        "id": "2f91afdf"
      },
      "outputs": [],
      "source": [
        "sun_azimuth = np.array(df['sun_azimuth:d']) * (np.pi / 180)\n",
        "\n",
        "hour = np.array(df['hour'])\n",
        "day_of_year = np.array(df['day_of_year'])\n",
        "\n",
        "solar_time = hour + 0.17 * np.sin(4 * np.pi * (day_of_year - 80) / 373) - 0.129 * np.sin(2 * np.pi * (day_of_year - 8) / 355)\n",
        "hour_angle = (15 * (12 - solar_time)) * (np.pi / 180)\n",
        "\n",
        "declination =  0.4093 * np.sin(2 * np.pi * (day_of_year - 81) / 368)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55af12e0",
      "metadata": {
        "id": "55af12e0"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "sun_zenith = np.arccos(\n",
        "    np.sin(sun_azimuth) * np.sin(declination) + np.cos(sun_azimuth) * np.cos(declination) * np.cos(hour_angle))\n",
        "sun_zenith_deg = sun_zenith* (180 / np.pi)\n",
        "plt.plot(sun_zenith_deg[50:100])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DoJ8LlyVECXL",
      "metadata": {
        "id": "DoJ8LlyVECXL"
      },
      "source": [
        "##### 7.6.2 Dew or Rime?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab355236",
      "metadata": {
        "id": "ab355236"
      },
      "outputs": [],
      "source": [
        "df_dew_rime = df_with_location\n",
        "df_dew_rime['dew'] = np.where(df_dew_rime['dew_or_rime:idx'] == 1, 1, 0)\n",
        "df_dew_rime['rime'] = np.where(df_dew_rime['dew_or_rime:idx'] == -1, 1, 0)\n",
        "df_dew_rime = df_dew_rime.drop(columns = 'dew_or_rime:idx')\n",
        "df_dew_rime.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wyiAvOeNckP_",
      "metadata": {
        "id": "wyiAvOeNckP_"
      },
      "source": [
        "# Models\n",
        "Also here we have run the same model for all the location"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UAHEmmFoEi4P",
      "metadata": {
        "id": "UAHEmmFoEi4P"
      },
      "source": [
        "## 1. Model Selection\n",
        "Our model selection was guided by the Kaggle submission scores, which indicated that CatBoost and LightGBM were the most effective. Conversely, using Random Forest or XGBoost did not align well with our approach to the problem.\n",
        "In this phase, we employed two different approaches to the problem: one involving models that we created and engineered ourselves, and the other utilizing a library known as pyCaret.\n",
        "We will show only one solution per model, not both. Every model has his own hyperparameter tuning for every location.\n",
        "We explored two approach for the hypertuning, the first was with optuna and the second with a randomsearchCV."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "johsg_WpFCG0",
      "metadata": {
        "id": "johsg_WpFCG0"
      },
      "source": [
        "### 1.1 Catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-8q8lugeFBy1",
      "metadata": {
        "id": "-8q8lugeFBy1"
      },
      "outputs": [],
      "source": [
        "    # Setup the environment in PyCaret\n",
        "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
        "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
        "\n",
        "                    #categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
        "                    #remove_outliers=True,  #Ble d√•rligere med denne\n",
        "                    html=False,\n",
        "                    experiment_name=f'exp_{location_name}')\n",
        "\n",
        "    # Create a Catboost model\n",
        "    cat = create_model('catboost')\n",
        "\n",
        "    # Tune the model\n",
        "    tuned_cat = tune_model(cat)\n",
        "\n",
        "    # Create a bagged version of the tuned model\n",
        "    bagged_cat = ensemble_model(tuned_cat, method='Bagging')\n",
        "\n",
        "    # Train on whole dataset\n",
        "    final_model = finalize_model(bagged_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B6jFGCBCFxAW",
      "metadata": {
        "id": "B6jFGCBCFxAW"
      },
      "source": [
        "### 1.2 LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spIH0K2mFzKS",
      "metadata": {
        "id": "spIH0K2mFzKS"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    param = {\n",
        "            'metric': 'mae',\n",
        "            'random_state': 42,\n",
        "            'n_estimators': trial.suggest_int('n_estimators',10,1000),\n",
        "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
        "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
        "            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
        "            'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
        "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4,1e-1),\n",
        "            'max_depth': trial.suggest_int('max_depth', 1,50),\n",
        "            'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n",
        "            'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
        "            'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100),\n",
        "            'verbosity': -1\n",
        "        }\n",
        "    model = lgb.LGBMRegressor(**param)\n",
        "    maelist = []\n",
        "    for train_index, test_index in kfold.split(data):\n",
        "        train_data = data.iloc[train_index]\n",
        "        test_data = data.iloc[test_index]\n",
        "\n",
        "        # Extract target variable for training and testing data\n",
        "        y_train = train_data['pv_measurement']\n",
        "        y_test = test_data['pv_measurement']\n",
        "\n",
        "        # Extract features for training and testing data\n",
        "        X_train = train_data.drop(columns = 'pv_measurement')\n",
        "        X_test = test_data.drop(columns = 'pv_measurement')\n",
        "\n",
        "        # Train the XGBoost model\n",
        "        model.fit(X_train, y_train,eval_set=[(X_test,y_test)])\n",
        "\n",
        "        # Make predictions on the test data\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Evaluate the model using Mean Absolute Error (MAE)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        print(\"Mean Absolute Error:\", mae)\n",
        "        maelist.append(mae)\n",
        "    # Return MAE\n",
        "    mean_mae = np.mean(maelist)\n",
        "\n",
        "    return mean_mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WePpthy0F3Xp",
      "metadata": {
        "id": "WePpthy0F3Xp"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from warnings import simplefilter\n",
        "simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "trial = study.best_trial\n",
        "model_A = lgb.LGBMRegressor(**trial.params)\n",
        "model_A.fit(X_train,y_train)\n",
        "df_test_A = df_test_A\n",
        "y_pred_A = model_A.predict(df_test_A)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NUB_ErpGGPfK",
      "metadata": {
        "id": "NUB_ErpGGPfK"
      },
      "source": [
        "### 1.3 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bWqNmeUFGRs8",
      "metadata": {
        "id": "bWqNmeUFGRs8"
      },
      "outputs": [],
      "source": [
        "param_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap,\n",
        "                'n_jobs' : [-1]\n",
        "             }\n",
        "print(param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jFxWaqy0GXpe",
      "metadata": {
        "id": "jFxWaqy0GXpe"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "rf_Model = RandomForestRegressor()\n",
        "\n",
        "y_train = train['pv_measurement']\n",
        "X_train = train.drop(columns = 'pv_measurement')\n",
        "rf_RandomGrid = RandomizedSearchCV(estimator = rf_Model, param_distributions = param_grid, cv = 10, verbose=2, n_jobs = 4)\n",
        "rf_RandomGrid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PKsSz-INGZFV",
      "metadata": {
        "id": "PKsSz-INGZFV"
      },
      "outputs": [],
      "source": [
        "model_A = RandomForestRegressor(**rf_RandomGrid.best_params_)\n",
        "model_A.fit(X_train,y_train)\n",
        "y_pred_A = model_A.predict(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TQRKxZXwGw1o",
      "metadata": {
        "id": "TQRKxZXwGw1o"
      },
      "source": [
        "### 1.4 XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gJOhwbQlGz6v",
      "metadata": {
        "id": "gJOhwbQlGz6v"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective':'reg:squarederror',\n",
        "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
        "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
        "        'gamma': trial.suggest_loguniform('gamma',  1e-3, 10.0),\n",
        "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
        "        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4,1e-0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators',10,1000),\n",
        "        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17]),\n",
        "        'random_state': 42,\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
        "        'eval_metric':'mae'\n",
        "    }\n",
        "    model = xgb.XGBRegressor(**param)\n",
        "    maelist = []\n",
        "    for train_index, test_index in tscv.split(data):\n",
        "        train_data = data.iloc[train_index]\n",
        "        test_data = data.iloc[test_index]\n",
        "\n",
        "        # Extract target variable for training and testing data\n",
        "        y_train = train_data['pv_measurement']\n",
        "        y_test = test_data['pv_measurement']\n",
        "\n",
        "        # Extract features for training and testing data\n",
        "        X_train = train_data.drop(columns = 'pv_measurement')\n",
        "        X_test = test_data.drop(columns = 'pv_measurement')\n",
        "            # Create an XGBoost DMatrix for training and testing data\n",
        "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "        # Train the XGBoost model\n",
        "        model = xgb.train(param, dtrain)\n",
        "\n",
        "        # Make predictions on the test data\n",
        "        y_pred = model.predict(dtest)\n",
        "\n",
        "        # Evaluate the model using Mean Absolute Error (MAE)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        print(\"Mean Absolute Error:\", mae)\n",
        "        maelist.append(mae)\n",
        "    # Return MAE\n",
        "    mean_mae = np.mean(maelist)\n",
        "\n",
        "    return mean_mae"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vWkHsW0uJzOY",
      "metadata": {
        "id": "vWkHsW0uJzOY"
      },
      "source": [
        "### 1.5 Automatic method for all the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vnkPQBgsJ2qt",
      "metadata": {
        "id": "vnkPQBgsJ2qt"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# Parameters for Random Forest\n",
        "rf_params = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': randint(3, 20),\n",
        "    'min_samples_split': uniform(0.01, 0.1),\n",
        "    'min_samples_leaf': uniform(0.01, 0.1),\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Parameters for Extra Trees\n",
        "et_params = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': randint(3, 20),\n",
        "    'min_samples_split': uniform(0.01, 0.1),\n",
        "    'min_samples_leaf': uniform(0.01, 0.1),\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Parameters for XGBoost\n",
        "xgb_params = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'subsample': uniform(0.5, 0.5),\n",
        "    'colsample_bytree': uniform(0.5, 0.5),\n",
        "    'min_child_weight': randint(1, 10)\n",
        "}\n",
        "\n",
        "# Parameters for LightGBM\n",
        "lgbm_params = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': randint(-1, 20),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'num_leaves': randint(20, 300),\n",
        "    'min_child_samples': randint(10, 100),\n",
        "    'subsample': uniform(0.5, 0.5),\n",
        "    'colsample_bytree': uniform(0.5, 0.5),\n",
        "    'verbose' : [-1]\n",
        "}\n",
        "\n",
        "# Parameters for CatBoost\n",
        "cat_params = {\n",
        "    'iterations': randint(100, 500),\n",
        "    'depth': randint(3, 10),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'l2_leaf_reg': randint(1, 10),\n",
        "    'border_count': randint(1, 255),\n",
        "    'subsample': uniform(0.5, 0.5)\n",
        "}\n",
        "\n",
        "# Combine all parameter dictionaries into one\n",
        "model_params = {\n",
        "    'catboost': cat_params\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Eo3X5KjnJ6Ck",
      "metadata": {
        "id": "Eo3X5KjnJ6Ck"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "\n",
        "def tune_model(X, y, model_params, n_iter=200, cv=2, verbose=1, random_state=42, n_jobs=-1):\n",
        "    # Store the best estimators here\n",
        "    best_estimators = {}\n",
        "\n",
        "    # Define a dictionary with model shorthand and actual regressor objects\n",
        "    models = {\n",
        "        'catboost': CatBoostRegressor(verbose=0, thread_count=n_jobs),\n",
        "    }\n",
        "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "    # Loop through the models and perform Randomized Search\n",
        "    for name, model in models.items():\n",
        "        if name in model_params:  # Only if model parameters were provided\n",
        "            print(f\"Tuning hyperparameters for {name}...\")\n",
        "            search = RandomizedSearchCV(\n",
        "                estimator=model,\n",
        "                param_distributions=model_params[name],\n",
        "                n_iter=n_iter,\n",
        "                cv=cv,\n",
        "                verbose=verbose,\n",
        "                random_state=random_state,\n",
        "                scoring=mae_scorer,\n",
        "                n_jobs=n_jobs\n",
        "            )\n",
        "            search.fit(X, y)\n",
        "            best_estimators[name] = search.best_estimator_\n",
        "            print(f\"Best parameters for {name}: {search.best_params_}\")\n",
        "        else:\n",
        "            print(f\"No parameters provided for {name}, skipping...\")\n",
        "\n",
        "    return best_estimators"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Xapai7SG-Bv",
      "metadata": {
        "id": "4Xapai7SG-Bv"
      },
      "source": [
        "## 2. Single model Plateu\n",
        "Using a single model, we achieved a plateau with a Kaggle score of 152. At that point, we explored combining models to attain a higher score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TogaMyanHTB_",
      "metadata": {
        "id": "TogaMyanHTB_"
      },
      "source": [
        "### 2.1 Stacking\n",
        "Our initial attempt involved stacking the predictions from the LightGBM, Catboost, and RandomForest models, and then applying Linear Regression to this new dataframe. However, this approach did not yield successful results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bZsPds50HSmb",
      "metadata": {
        "id": "bZsPds50HSmb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Generate some example time series data\n",
        "# Replace this with your own time series data\n",
        "data = df\n",
        "\n",
        "# Define the number of splits for time series cross-validation\n",
        "n_splits = 3\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits,test_size = 720)\n",
        "\n",
        "# Initialize empty arrays to store meta features and true targets\n",
        "meta_features = np.empty(data.shape[0])\n",
        "\n",
        "# Iterate through time series cross-validation splits\n",
        "for train_index, test_index in tscv.split(data):\n",
        "    train_data = data.iloc[train_index]\n",
        "    test_data = data.iloc[test_index]\n",
        "\n",
        "    # Split the data into features and target\n",
        "    X_train = train_data.drop(columns=target)\n",
        "    y_train = train_data['pv_measurement']\n",
        "    X_test = test_data.drop(columns=target)\n",
        "    y_test = test_data['pv_measurement']\n",
        "    lgbmpar = {'verbose' : -1 }\n",
        "    lgbm_model = LGBMRegressor(**lgbmpar)\n",
        "    lgbm_model.fit(X_train, y_train)\n",
        "    catapar = {'verbose':500}\n",
        "    # Initialize and train the CatBoost model\n",
        "    catboost_model = CatBoostRegressor(**catapar)\n",
        "    catboost_model.fit(X_train, y_train)\n",
        "\n",
        "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    lgbm_preds = lgbm_model.predict(X_test)\n",
        "    catboost_preds = catboost_model.predict(X_test)\n",
        "    rf_preds = rf_model.predict(X_test)\n",
        "    mae_lgbm = mean_absolute_error(y_test, lgbm_preds)\n",
        "    mae_cat = mean_absolute_error(y_test, catboost_preds)\n",
        "    mae_rf = mean_absolute_error(y_test, rf_preds)\n",
        "    print(f'Lgb : {mae_lgbm}, cat: {mae_cat}, rf: {mae_rf}')\n",
        "    # Stack the predictions horizontally to create meta features\n",
        "    stacked_features = np.column_stack((lgbm_preds, catboost_preds,rf_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ljBrHa7H3jg",
      "metadata": {
        "id": "1ljBrHa7H3jg"
      },
      "outputs": [],
      "source": [
        "# Train the meta-model on the combined feature matrix and the target values\n",
        "#weightmeans\n",
        "meta_model = LGBMRegressor()\n",
        "meta_model.fit(stacked_features, y_test)\n",
        "Linear_pred = meta_model.predict(stacked_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vX8LNmJAH8lm",
      "metadata": {
        "id": "vX8LNmJAH8lm"
      },
      "outputs": [],
      "source": [
        "lgbm_preds = lgbm_model.predict(df_test_A[feature_test])\n",
        "catboost_preds = catboost_model.predict(df_test_A[feature_test])\n",
        "rf_preds = rf_model.predict(df_test_A[feature_test])\n",
        "\n",
        "# Combine the predictions of the base models into a single feature matrix\n",
        "X_new_meta = np.column_stack((lgbm_preds, catboost_preds,rf_preds))\n",
        "df_stacked = pd.DataFrame({'lgbm': lgbm_preds, 'cat' :catboost_preds,'rf':rf_preds})\n",
        "df_stacked['media'] = df_stacked[subset_cols].mean(axis=1)\n",
        "df_stacked.head(50)\n",
        "(10 ** df_stacked['lgbm'] -1).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jNffndxkH-Ky",
      "metadata": {
        "id": "jNffndxkH-Ky"
      },
      "source": [
        "### 2.2 Ensemble modeling\n",
        "Convinced that our previous solution was missing key details, we decided to increase the complexity of our approach. We developed an ensemble model that utilized the out-of-fold technique, aiming to capture more nuanced aspects of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qMIHOegmIZHB",
      "metadata": {
        "id": "qMIHOegmIZHB"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.base import clone\n",
        "\n",
        "def stacking_ensemble_time_series(models, X, y, meta_learner=ExtraTreesRegressor(), n_folds=5):\n",
        "    # Create the time series cross-validator\n",
        "    tscv = TimeSeriesSplit(n_splits=n_folds)\n",
        "\n",
        "    # To store out-of-fold predictions\n",
        "    meta_features = np.zeros((y.shape[0], len(models)))\n",
        "\n",
        "    # Train and generate meta-features\n",
        "    for idx, (name, model) in enumerate(models.items()):\n",
        "        print(f\"Training base model: {name}\")\n",
        "        oof_predictions = np.zeros(y.shape[0])\n",
        "\n",
        "        for train_index, test_index in tscv.split(X):\n",
        "            # Split data into folds\n",
        "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "            # Clone the model to ensure we have a fresh model\n",
        "            cloned_model = clone(model)\n",
        "            cloned_model.fit(X_train, y_train['pv_measurement'])\n",
        "\n",
        "            # Generate out-of-fold predictions\n",
        "            oof_predictions[test_index] = cloned_model.predict(X_test)\n",
        "\n",
        "        # Store out-of-fold predictions as meta-features\n",
        "        meta_features[:, idx] = oof_predictions\n",
        "\n",
        "    # Retrieve the last fold for training the meta-learner\n",
        "    _, meta_train_index = list(tscv.split(X))[-1]\n",
        "    X_meta_train, y_meta_train = meta_features[meta_train_index, :], y.iloc[meta_train_index]\n",
        "\n",
        "    # Train the meta-learner on the last fold's meta-features\n",
        "    print(\"Training meta-learner...\")\n",
        "    meta_learner.fit(X_meta_train, y_meta_train['pv_measurement'])\n",
        "\n",
        "    # Fit all the base models on the full training data\n",
        "    fitted_models = {}\n",
        "    for name, model in models.items():\n",
        "        model.fit(X, y['pv_measurement'])\n",
        "        fitted_models[name] = model\n",
        "\n",
        "    # Function to make ensemble predictions\n",
        "    def make_predictions(X_new):\n",
        "        meta_features_new = np.column_stack([\n",
        "            fitted_model.predict(X_new) for _, fitted_model in fitted_models.items()\n",
        "        ])\n",
        "        return meta_learner.predict(meta_features_new)\n",
        "\n",
        "    return fitted_models, meta_learner, make_predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yP5l-yrEIeqg",
      "metadata": {
        "id": "yP5l-yrEIeqg"
      },
      "source": [
        "### 2.3 The mean of different model\n",
        "In a conversation with a teaching assistant (TA) regarding our project, we discussed hitting a plateau with a single model. The TA suggested averaging the outputs of the same model and perhaps adding another model. This strategy proved to be highly effective; in fact, our best solution was developed based on this idea.\n",
        "In this solution every model has is own Feature Engineering and Preprocessing so they are like 3 different submission for kaggle where we do the mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75kqtmDXI7lf",
      "metadata": {
        "id": "75kqtmDXI7lf"
      },
      "outputs": [],
      "source": [
        "# LightGBM with some extra features\n",
        "def process_location_ex(X, y, location_name,seeds):\n",
        "\n",
        "    # Combine feature data and target into a single DataFrame\n",
        "    data = X.copy()\n",
        "    data['target'] = y['pv_measurement']\n",
        "\n",
        "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
        "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
        "\n",
        "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
        "                    html=False,\n",
        "                    experiment_name=f'exp_{location_name}')\n",
        "\n",
        "    # Create a LightGBM model\n",
        "    lightgbm = create_model('lightgbm')\n",
        "\n",
        "    tuned_lightgbm = tune_model(lightgbm)\n",
        "\n",
        "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
        "\n",
        "    final_model = finalize_model(bagged_lightgbm)\n",
        "\n",
        "\n",
        "    return final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kjBu-tgTJDhn",
      "metadata": {
        "id": "kjBu-tgTJDhn"
      },
      "outputs": [],
      "source": [
        "# Catboost model nr 2\n",
        "def process_location_cat_2(X, y, location_name,seeds):\n",
        "\n",
        "    # Dropping some features for this one model\n",
        "    features_to_drop = ['dew_or_rime:idx', #'snow_density:kgm3',\n",
        "                        'fresh_snow_3h:cm', 'fresh_snow_1h:cm', 'snow_drift:idx',\n",
        "                        'snow_depth:cm', 'wind_speed_w_1000hPa:ms', 'prob_rime:p',\n",
        "                        'fresh_snow_6h:cm', 'snow_melt_10min:mm',\n",
        "                        'fresh_snow_12h:cm', 'rain_water:kgm2',\n",
        "                        'super_cooled_liquid_water:kgm2']\n",
        "\n",
        "    X = X.drop(columns=features_to_drop)\n",
        "\n",
        "    data = X.copy()\n",
        "    data['target'] = y['pv_measurement']\n",
        "\n",
        "    # Setup the environment in PyCaret\n",
        "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
        "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
        "                    html=False,\n",
        "                    experiment_name=f'exp_{location_name}')\n",
        "\n",
        "    cat = create_model('catboost')\n",
        "\n",
        "    tuned_cat = tune_model(cat)\n",
        "\n",
        "    bagged_cat = ensemble_model(tuned_cat, method='Bagging')\n",
        "\n",
        "    final_model = finalize_model(bagged_cat)\n",
        "\n",
        "\n",
        "    return final_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xRmbWBcQJSaF",
      "metadata": {
        "id": "xRmbWBcQJSaF"
      },
      "outputs": [],
      "source": [
        "all_predictions_cat = []\n",
        "# Catboost model nr 1\n",
        "for loc in locations:\n",
        "\n",
        "    # Load your data\n",
        "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
        "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
        "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
        "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
        "\n",
        "    # Calling preprocessing\n",
        "    X_train_cat, X_test_cat, is_day_feature1, targets_cat = preprocessing_cat(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
        "\n",
        "    # Making categorical features\n",
        "    cat_features = ['dew_or_rime:idx' ,'is_in_shadow:idx']\n",
        "    X_train_cat['dew_or_rime:idx'] = X_train_cat['dew_or_rime:idx'].astype(int)\n",
        "    X_train_cat['is_in_shadow:idx'] = X_train_cat['is_in_shadow:idx'].astype(int)\n",
        "    X_test_cat['dew_or_rime:idx'] = X_test_cat['dew_or_rime:idx'].astype(int)\n",
        "    X_test_cat['is_in_shadow:idx'] = X_test_cat['is_in_shadow:idx'].astype(int)\n",
        "\n",
        "    model_cat = CatBoostRegressor(\n",
        "        loss_function='MAE',\n",
        "        learning_rate=0.1,\n",
        "        verbose=200,\n",
        "        cat_features=cat_features,\n",
        "        random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "    # Training\n",
        "    model_cat.fit(X_train_cat,targets_cat['pv_measurement'])\n",
        "\n",
        "    # Prediction\n",
        "    predictions_cat = model_cat.predict(X_test_cat)\n",
        "    feature_importances = model_cat.get_feature_importance()\n",
        "    adjusted_final_predictions_cat = predictions_cat * is_day_feature1['is_day:idx']\n",
        "\n",
        "    adjusted_final_predictions_cat = np.clip(adjusted_final_predictions_cat, 0, None)\n",
        "\n",
        "    all_predictions_cat.append(adjusted_final_predictions_cat)\n",
        "\n",
        "# Changing final list to array\n",
        "all_predictions_cat = np.array(all_predictions_cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oyNxJeALJVxq",
      "metadata": {
        "id": "oyNxJeALJVxq"
      },
      "outputs": [],
      "source": [
        "all_pred = 0.25*all_predictions_cat+0.25 * all_predictions_lGBM_e+ 0.25* all_predictions_cat_2+0.25*all_predictions_cat_3"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0b9afd23",
        "qG383UWxCPjy",
        "gXFSSLefC4QG",
        "9mPCkfN8AnAD",
        "LBtoGsyVHFYe",
        "VZ4yH3wbJAfF",
        "zDmf9Uv88nQl",
        "eTpVCXdt-XtM",
        "bfcn9YeEAGYB",
        "KVuhlxP1A970",
        "wyiAvOeNckP_",
        "johsg_WpFCG0",
        "B6jFGCBCFxAW",
        "NUB_ErpGGPfK",
        "TQRKxZXwGw1o",
        "vWkHsW0uJzOY",
        "TogaMyanHTB_",
        "jNffndxkH-Ky",
        "yP5l-yrEIeqg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
