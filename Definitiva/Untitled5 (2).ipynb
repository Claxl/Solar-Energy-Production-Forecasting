{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eCvptl2PqlMY",
    "outputId": "4b68714a-5b32-40d1-cada-d7fdd70d795b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# Sopprime tutti i FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# ADD TIME FEATURES\n",
    "def add_time_features(df, time_column):\n",
    "    '''\n",
    "        This function will add some time feature based on the param 'time_columns'\n",
    "\n",
    "        Params:\n",
    "            df-> Dataframe with the column contained in 'time_column'\n",
    "            time_column -> the column that is a datetime object\n",
    "\n",
    "        Returns:\n",
    "            A dataframe with time features\n",
    "    '''\n",
    "\n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Make sure the time column is in datetime format\n",
    "    df['hour'] = df[time_column].dt.hour\n",
    "    df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "    df['month'] = df[time_column].dt.month\n",
    "    df['day_of_year'] = df[time_column].dt.dayofyear\n",
    "    df['week_of_year'] = df[time_column].dt.isocalendar().week\n",
    "    df['year'] = df[time_column].dt.year\n",
    "    #df['sin_hour'] = np.sin(np.pi * df[time_column].dt.hour/24.)\n",
    "    #df['sin_month'] = np.sin(np.pi * df[time_column].dt.month/12.)\n",
    "    #why these feature? Who knows\n",
    "    return df\n",
    "\n",
    "def plot_targets(targets):\n",
    "    '''\n",
    "        Plot the target, by a giving date\n",
    "\n",
    "        Params:\n",
    "            Targets-> A dataframe with the target value\n",
    "            Start_date -> the start date\n",
    "            End_date -> the end date\n",
    "\n",
    "        Returns:\n",
    "            Sto cazzo\n",
    "    '''\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(targets, label='PV Measurement', color='blue')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('PV Measurement')\n",
    "    plt.title('PV Measurement Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def to_datetime(df,column):\n",
    "    '''\n",
    "        Make the column in datetime format\n",
    "    '''\n",
    "    return pd.to_datetime(df[column])\n",
    "\n",
    "def resampling(df,column):\n",
    "    '''\n",
    "        Resample df to 1 hour using mean() as aggregator and drop rows where all columns are NaN\n",
    "\n",
    "        Params :\n",
    "            df -> the dataframe to be resampled\n",
    "            column -> the time column\n",
    "    '''\n",
    "    return df.set_index(keys = column).resample('1H').mean().dropna(how='all').reset_index()\n",
    "\n",
    "def filter_df(df,columnlist):\n",
    "    return df.drop(columns = columnlist)\n",
    "\n",
    "def extract_data_calc(df):\n",
    "    '''\n",
    "    This function create a dataframe with 'date_forecast' as index and the column 'date_calc' resampled by '1H'.\n",
    "    If there's no data in a specific bin, the resulting value for that bin would be NaN (not a number).\n",
    "    Params:\n",
    "        df -> dataframe with 'date_forecast' and 'date_calc' columns.\n",
    "            'date_calc' is expected to contain data that the user wants to resample or analyze.\n",
    "    Returns:\n",
    "        A dataframe with 'date_calc' resampled.\n",
    "    '''\n",
    "    return df.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "\n",
    "\n",
    "\n",
    "def is_estimated_feature(df):\n",
    "    '''\n",
    "        This function will create some time feature and estimated information. It's need to let the model understand is\n",
    "        estimated value.\n",
    "        Params:\n",
    "            df -> It MUST be an estimated dataframe, that contains 'data_forecast' as datetime type\n",
    "        Returns:\n",
    "            A dataframe with 'time_dummy', 'time_delta' and 'is_estimated'\n",
    "    '''\n",
    "    df['time_dummy'] = (df['date_forecast'] - df['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
    "    df['time_delta'] = (df['date_calc'] - df['date_forecast']).dt.total_seconds() / 3600\n",
    "    df['is_estimated'] = 1\n",
    "    return df\n",
    "\n",
    "def delete_stationarity(df):\n",
    "    '''\n",
    "    Removes constant stretches of data within a DataFrame where the 'pv_measurement' column does not change.\n",
    "    The function identifies blocks of data where the 'pv_measurement' stays constant for more than two consecutive\n",
    "    points and removes these blocks to address data stationarity.\n",
    "\n",
    "    params:\n",
    "        df -> DataFrame\n",
    "              A pandas DataFrame with a 'pv_measurement' column which contains the data from which to remove stationarity.\n",
    "\n",
    "    return:\n",
    "        The DataFrame with constant stretches of data removed from the 'pv_measurement' column.\n",
    "    '''\n",
    "\n",
    "    #Calculate the difference, this need for check the constant\n",
    "    df['diff'] = df['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    # Create an indicator for constant stretches\n",
    "    df['constant'] = (df['diff'] == 0).astype(int)\n",
    "\n",
    "    # Use the indicator to mark stretches. The diff() function here identifies change-points.\n",
    "    df['block'] = (df['constant'].diff() != 0).astype(int).cumsum()\n",
    "\n",
    "    # Get the size of each constant block\n",
    "    block_sizes = df.groupby('block')['constant'].sum()\n",
    "\n",
    "    # Identify blocks that are constant for more than N consecutive time points (in this case 2)\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "\n",
    "    # Remove the constant\n",
    "    filtered_df = df[~df['block'].isin(constant_blocks)]\n",
    "\n",
    "    return filtered_df.drop(columns=['diff', 'constant', 'block'])\n",
    "\n",
    "\n",
    "def impute_nan(df):\n",
    "    '''\n",
    "        This function will impute the Nan in the give dataframe\n",
    "\n",
    "        Params:\n",
    "            df -> the dataframe to be imputed\n",
    "        Return:\n",
    "            The dataframe imputed\n",
    "    '''\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    return df\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def remove_outliers(df, column, method='IQR', **kwargs):\n",
    "    '''\n",
    "    Removes outliers from a specific column in a pandas DataFrame based on the selected method.\n",
    "    Additionally, prints the number of outliers removed.\n",
    "\n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "        The pandas DataFrame from which to remove outliers.\n",
    "    column : str\n",
    "        The name of the column from which to remove outliers.\n",
    "    method : str, optional\n",
    "        The method used to identify and remove outliers. Accepted values are 'IQR' for Interquartile Range or\n",
    "        'Z-score'. The default is 'IQR'.\n",
    "    **kwargs : additional keyword arguments\n",
    "        Additional parameters required for the specified outlier removal method.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame\n",
    "        A new DataFrame with outliers removed from the specified column.\n",
    "    '''\n",
    "\n",
    "    if method == 'IQR':\n",
    "        # Calculate the IQR (Interquartile Range) for the column\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Determine upper and lower bounds for outlier detection\n",
    "        lower_bound = Q1 - (3 * IQR)\n",
    "        upper_bound = Q3 + (3 * IQR)\n",
    "\n",
    "        # Filtering before removing to determine the number of outliers\n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "        non_outliers = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "    elif method == 'Z-score':\n",
    "        # Calculate Z-scores for the column\n",
    "        z_scores = stats.zscore(df[column].dropna())\n",
    "\n",
    "        # Define a threshold for identifying outliers\n",
    "        threshold = kwargs.get('threshold', 3) # Default threshold is 3\n",
    "\n",
    "        # Create masks for outliers and non-outliers\n",
    "        mask = (abs(z_scores) < threshold)\n",
    "        non_outliers = df[mask]\n",
    "        outliers = df[~mask]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Method not recognized. Use 'IQR' or 'Z-score'.\")\n",
    "\n",
    "    # Print the number of outliers removed\n",
    "    num_outliers = len(outliers)\n",
    "    #if(num_outliers > 0):\n",
    "       # print(f'For {column} there was outliers {num_outliers}')\n",
    "\n",
    "    # Return the DataFrame without outliers\n",
    "    return non_outliers\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `data` is your DataFrame and you want to remove outliers from the 'price' column using IQR method:\n",
    "# cleaned_data = remove_outliers(data, 'price', method='IQR')\n",
    "\n",
    "# Or using Z-score method with a specific threshold:\n",
    "# cleaned_data = remove_outliers(data, 'price', method='Z-score', threshold=2.5)\n",
    "\n",
    "\n",
    "def one_hot_encoding(df, columns_to_encode):\n",
    "    '''\n",
    "        Perform one-hot encoding on the selected columns\n",
    "        Params:\n",
    "            df: Dataframe\n",
    "            columns_to_encode: List of columns\n",
    "        Return\n",
    "            Dataframe with columns encoded\n",
    "    '''\n",
    "    one_hot_encoded_df = pd.get_dummies(df[columns_to_encode], columns=columns_to_encode)\n",
    "    return pd.concat([df, one_hot_encoded_df], axis=1)\n",
    "\n",
    "def should_process_column(column_name):\n",
    "    \"\"\"\n",
    "    Check if the column should be processed based on its name.\n",
    "\n",
    "    Args:\n",
    "    column_name (str): The name of the column to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the column should be processed, False otherwise.\n",
    "    \"\"\"\n",
    "    excluded_substrings = ['idx', 'time', 'estimated']\n",
    "    return not any(substring in column_name for substring in excluded_substrings)\n",
    "\n",
    "def fix_encoding(df):\n",
    "    df[['dew_or_rime:idx_-1.0', 'dew_or_rime:idx_-0.75', 'dew_or_rime:idx_-0.5', 'dew_or_rime:idx_-0.25', 'dew_or_rime:idx_0.75','is_estimated_0']] = 0\n",
    "    return df\n",
    "\n",
    "def preprocessing(targets, observed, estimated, test):\n",
    "    '''\n",
    "        This function makes all the preprocessing needed for the correct run of the model, it will perform:\n",
    "            - Resampling\n",
    "            - Filtering\n",
    "            - Imputation\n",
    "            - Outliers removal\n",
    "            - Categorical Encoding\n",
    "\n",
    "        Params:\n",
    "            targets -> dataframe of the target parquet\n",
    "            observed -> dataframe of observed train data\n",
    "            estimated -> dataframe of estimated train data\n",
    "            test -> dataframe of test data\n",
    "        Returns:\n",
    "            train_data -> dataframe of all data ready to train\n",
    "            test_data -> dataframe of all data ready to test\n",
    "            is_day -> dataframe of is_day categorical feature for post processing\n",
    "\n",
    "    '''\n",
    "    targets['time'] = to_datetime(targets,'time')\n",
    "    estimated['date_forecast'] = to_datetime(estimated,'date_forecast')\n",
    "    observed['date_forecast'] = to_datetime(observed,'date_forecast')\n",
    "    test['date_forecast'] = to_datetime(test,'date_forecast')\n",
    "\n",
    "    observed_resampled = resampling(observed,'date_forecast')\n",
    "    estimated_resampled = resampling(estimated,'date_forecast')\n",
    "    test_resampled = resampling(test,'date_forecast')\n",
    "\n",
    "    date_calc_resampled_observed = extract_data_calc(estimated)\n",
    "    date_calc_resampled_test = extract_data_calc(test)\n",
    "\n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_observed, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_test, left_on='date_forecast', right_index=True)\n",
    "\n",
    "    is_day = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    test_resampled = filter_df(test_resampled,['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = filter_df(observed_resampled,['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    estimated_resampled = filter_df(estimated_resampled,[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "\n",
    "    #This MUST be zero because is not estimated.\n",
    "    observed_resampled['time_dummy'] = 0\n",
    "    observed_resampled['is_estimated'] = 0\n",
    "    observed_resampled['time_delta'] = 0\n",
    "\n",
    "    estimated_resampled = is_estimated_feature(estimated_resampled)\n",
    "    test_resampled = is_estimated_feature(test_resampled)\n",
    "\n",
    "    X = pd.concat([observed_resampled,estimated_resampled],axis = 0)\n",
    "    train_data = pd.merge(targets, X, how='inner', left_on='time', right_on='date_forecast')\n",
    "\n",
    "    train_data = add_time_features(train_data, 'time')\n",
    "    test_data = add_time_features(test_resampled, 'date_forecast')\n",
    "\n",
    "    train_data = delete_stationarity(train_data)\n",
    "\n",
    "    train_data = filter_df(train_data, ['time','date_calc'])\n",
    "    test_data = filter_df(test_resampled, ['date_calc'])\n",
    "\n",
    "    train_data = impute_nan(train_data)\n",
    "    test_data = impute_nan(test_data)\n",
    "\n",
    "    train_data = one_hot_encoding(train_data, ['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'])\n",
    "    test_data = one_hot_encoding(test_data, ['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'])\n",
    "    test_data = fix_encoding(test_data)\n",
    "    test_data = test_data[train_data.drop(columns = 'pv_measurement').columns]\n",
    "    return train_data, test_data, is_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eCvptl2PqlMY",
    "outputId": "4b68714a-5b32-40d1-cada-d7fdd70d795b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "def train_data_split(n_splits = 5):\n",
    "    '''\n",
    "        This function return the cross_validator split\n",
    "    '''\n",
    "    return TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "import re\n",
    "def regexdf(df):\n",
    "    '''\n",
    "        This function let lgbm work, this because it cannot accept ':'\n",
    "    '''\n",
    "    return df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_-]+', '', x))\n",
    "\n",
    "\n",
    "\n",
    "def final_model(df,model,param,X_test):\n",
    "    '''\n",
    "        This function will retrain on the bagged model so the model is trained on ALL TRAIN DATA.\n",
    "\n",
    "        Params:\n",
    "            df -> Train data\n",
    "            model -> your model\n",
    "            param -> your hyperparameter\n",
    "            X_test -> Test Data\n",
    "        Return:\n",
    "            the prediction\n",
    "    '''\n",
    "    X_train = df.drop(columns = 'pv_measurement')\n",
    "    y_train = df['pv_measurement']\n",
    "    model.fit(X_train,y_train)\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eCvptl2PqlMY",
    "outputId": "4b68714a-5b32-40d1-cada-d7fdd70d795b"
   },
   "outputs": [],
   "source": [
    "def post_process(prediction, is_day):\n",
    "    '''\n",
    "        This function will post_process the predicition, by multiplying the is_day on that prediction.\n",
    "        I.E.\n",
    "            This is for rescale the prediction that can be too high in a 0.5 day moment.\n",
    "        Params:\n",
    "            Prediction -> your prediction\n",
    "            is_day -> the is_day dataframe\n",
    "        Return:\n",
    "            the clipped version of the adjusted prediction, it's clipped for any eventual negative prediction\n",
    "    '''\n",
    "    adjusted_predictions = prediction * is_day['is_day:idx']\n",
    "    return np.clip(adjusted_predictions, 0, None)\n",
    "\n",
    "def submission(predictions):\n",
    "    '''\n",
    "        This function will create a ready to deliver file\n",
    "\n",
    "        Params:\n",
    "            predictions -> the entire prediction for all the location\n",
    "\n",
    "        Returns:\n",
    "            stocazzo\n",
    "        Save:\n",
    "            Submission file\n",
    "\n",
    "    '''\n",
    "    all_predictions = np.array(predictions).flatten()\n",
    "    # Save the final_predictions to CSV\n",
    "    df = pd.DataFrame(all_predictions, columns=['prediction'])\n",
    "    df['id'] = df.index\n",
    "    df = df[['id', 'prediction']]\n",
    "    df.to_csv('TestFECla3.csv', index=False)\n",
    "\n",
    "def feature_engineering(df):\n",
    "    '''\n",
    "        This function will create new feature from the interaction of different feature of df dataframe.\n",
    "\n",
    "        Params:\n",
    "            df: dataframe\n",
    "        Return:\n",
    "            FE dataframe\n",
    "    '''\n",
    "\n",
    "    df['direct_diffuse_rad_interaction'] = df['direct_rad:W'] * df['diffuse_rad:W']\n",
    "    df['raddir'] = (df['direct_rad:W'] ) * (df['absolute_humidity_2m:gm3'])\n",
    "    df['effectivehum'] = (df['absolute_humidity_2m:gm3']) * (df['direct_rad:W'])\n",
    "    df['Radiazione_solare_effettiva'] = (df['direct_rad:W'] + df['diffuse_rad:W'] ) * (df['effective_cloud_cover:p'])\n",
    "    df['direct_radW_squared'] = df['direct_rad:W'] ** 2\n",
    "    df['clearcloud'] =   df['clear_sky_rad:W']*df['total_cloud_cover:p']\n",
    "    df['radiation_squared'] = df['clear_sky_rad:W'] ** 2\n",
    "    df['effectivehum'] = (df['absolute_humidity_2m:gm3']) * (df['direct_rad:W'])\n",
    "\n",
    "   # df = df[df.index.month.isin([4, 5, 6, 7, 8, 9])]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "def stacking_ensemble_time_series(models, X, y, meta_learner=LinearRegression(), n_folds=5):\n",
    "    # Create the time series cross-validator\n",
    "    tscv = TimeSeriesSplit(n_splits=n_folds)\n",
    "    \n",
    "    # To store out-of-fold predictions\n",
    "    meta_features = np.zeros((y.shape[0], len(models)))\n",
    "    \n",
    "    # Train and generate meta-features\n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        print(f\"Training base model: {name}\")\n",
    "        oof_predictions = np.zeros(y.shape[0])\n",
    "        \n",
    "        for train_index, test_index in tscv.split(X):\n",
    "            # Split data into folds\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            \n",
    "            # Clone the model to ensure we have a fresh model\n",
    "            cloned_model = clone(model)\n",
    "            cloned_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Generate out-of-fold predictions\n",
    "            oof_predictions[test_index] = cloned_model.predict(X_test)\n",
    "        \n",
    "        # Store out-of-fold predictions as meta-features\n",
    "        meta_features[:, idx] = oof_predictions\n",
    "    \n",
    "    # Retrieve the last fold for training the meta-learner\n",
    "    _, meta_train_index = list(tscv.split(X))[-1]\n",
    "    X_meta_train, y_meta_train = meta_features[meta_train_index, :], y[meta_train_index]\n",
    "    \n",
    "    # Train the meta-learner on the last fold's meta-features\n",
    "    print(\"Training meta-learner...\")\n",
    "    meta_learner.fit(X_meta_train, y_meta_train)\n",
    "    \n",
    "    # Fit all the base models on the full training data\n",
    "    fitted_models = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X, y)\n",
    "        fitted_models[name] = model\n",
    "    \n",
    "    # Function to make ensemble predictions\n",
    "    def make_predictions(X_new):\n",
    "        meta_features_new = np.column_stack([\n",
    "            fitted_model.predict(X_new) for _, fitted_model in fitted_models.items()\n",
    "        ])\n",
    "        return meta_learner.predict(meta_features_new)\n",
    "    \n",
    "    return fitted_models, meta_learner, make_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "def train_and_predict(train_df, test_df, window_size, num_predictions):\n",
    "    \"\"\"\n",
    "    Addestra un modello LightGBM con il train_df e fa previsioni ricorsive utilizzando test_df.\n",
    "\n",
    "    Parameters:\n",
    "    train_df (pandas.DataFrame): DataFrame di training contenente la colonna 'pv_measurement'.\n",
    "    test_df (pandas.DataFrame): DataFrame di test vuoto, utilizzato per accumulare le previsioni.\n",
    "    window_size (int): Dimensione della finestra per la media mobile.\n",
    "    num_predictions (int): Numero totale di valori da predire.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame contenente le previsioni.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcolo della rolling mean sul train_df\n",
    "    train_df['rolling_mean'] = train_df['pv_measurement'].rolling(window=window_size).mean().shift(-window_size + 1)\n",
    "    train_df = train_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Preparazione dei dati per LightGBM\n",
    "    features = ['rolling_mean']\n",
    "    target = 'pv_measurement'\n",
    "    train_set = lgb.Dataset(train_df[features], label=train_df[target])\n",
    "    \n",
    "    # Parametri per LightGBM (da ottimizzare secondo il problema)\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    # Addestramento del modello\n",
    "    model = lgb.train(params, train_set, num_boost_round=100)\n",
    "    \n",
    "    # Inizializzazione di test_df con la rolling mean degli ultimi 24 valori di train_df\n",
    "    last_values = train_df['pv_measurement'].tail(window_size).tolist()\n",
    "    test_df = pd.DataFrame({'pv_measurement': last_values})\n",
    "    test_df['rolling_mean'] = test_df['pv_measurement'].rolling(window=window_size).mean()\n",
    "    print(test_df)\n",
    "    # Eseguiamo le previsioni\n",
    "    predictions = []\n",
    "    for _ in range(num_predictions):\n",
    "        # Preparazione dell'input per la previsione\n",
    "        input_data = test_df['rolling_mean'].iloc[-1].reshape(1, -1)\n",
    "        # Previsione\n",
    "        prediction = model.predict(input_data)[0]\n",
    "        predictions.append(prediction)\n",
    "        # Aggiornamento del test_df con la nuova previsione\n",
    "        new_row = pd.DataFrame({'pv_measurement': prediction}, index=[0])\n",
    "        test_df = pd.concat([test_df, new_row], ignore_index=True)\n",
    "        test_df['rolling_mean'] = test_df['pv_measurement'].rolling(window=window_size).mean()\n",
    "\n",
    "    # Conversione della lista di previsioni in DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['predicted_pv_measurement'])\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Uso della funzione\n",
    "# Assicurati che 'train_df' sia il DataFrame con la tua serie temporale di training e 'test_df' sia un DataFrame vuoto\n",
    "# predictions = train_and_predict(train_df, test_df, 24, 720)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eCvptl2PqlMY",
    "outputId": "4b68714a-5b32-40d1-cada-d7fdd70d795b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1677.1140005513764\n",
      "284.633682228361\n",
      "222.04759635842396\n"
     ]
    }
   ],
   "source": [
    "locations = ['A','B','C']\n",
    "all_predictions = []\n",
    "params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'verbose': -1\n",
    "}\n",
    "for loc in locations:\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    train_data, test_data, is_day= preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    train_data = feature_engineering(train_data)\n",
    "    test_data = feature_engineering(test_data)\n",
    "    train_data = regexdf(train_data)\n",
    "    test_data = regexdf(test_data)\n",
    "    \n",
    "    train_data = add_temporal_aggregations_to_df(train_data)\n",
    "    test_data = add_temporal_aggregations_to_df(test_data)\n",
    "    train_data = train_data.drop(columns = ['date_forecast'])\n",
    "    test_data = test_data.drop(columns = ['date_forecast'])\n",
    "    \n",
    "    train = train_data.iloc[:-720]\n",
    "    test = train_data.iloc[-720:]\n",
    "    target = 'pv_measurement'\n",
    "    train_set = lgb.Dataset(train_data.drop(columns = target), label=train_data[target])\n",
    "    model = lgb.train(params, train_set, num_boost_round=100)\n",
    "    X_test = test.drop(columns = target)\n",
    "    y_test = test[target]\n",
    "    predictions = model.predict(test_data)\n",
    "    \n",
    "    predictions_fix = post_process(predictions,is_day)\n",
    "    print(mean_absolute_error(y_test,predictions_fix))\n",
    "    all_predictions.append(predictions_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "Vti-ZMM7qney"
   },
   "outputs": [],
   "source": [
    "'''sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission\n",
    "sample_submission = sample_submission[['id']].merge(final_df[['id', 'prediction']], on='id', how='left')\n",
    "sample_submission.to_csv('my_first_submission.csv', index=False)'''\n",
    "\n",
    "final_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df.to_csv('TestAgg1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pv_measurement</th>\n",
       "      <th>absolute_humidity_2mgm3</th>\n",
       "      <th>air_density_2mkgm3</th>\n",
       "      <th>ceiling_height_aglm</th>\n",
       "      <th>clear_sky_energy_1hJ</th>\n",
       "      <th>clear_sky_radW</th>\n",
       "      <th>cloud_base_aglm</th>\n",
       "      <th>dew_or_rimeidx</th>\n",
       "      <th>dew_point_2mK</th>\n",
       "      <th>diffuse_radW</th>\n",
       "      <th>...</th>\n",
       "      <th>sun_elevation_lag1</th>\n",
       "      <th>weekly_snow_sum</th>\n",
       "      <th>hourly_mean_wind_speed</th>\n",
       "      <th>daily_temp_change</th>\n",
       "      <th>is_summer</th>\n",
       "      <th>is_winter</th>\n",
       "      <th>is_autumn</th>\n",
       "      <th>is_spring</th>\n",
       "      <th>extreme_weather</th>\n",
       "      <th>trend_energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000004</th>\n",
       "      <td>19.36</td>\n",
       "      <td>8.950</td>\n",
       "      <td>1.21800</td>\n",
       "      <td>1003.500000</td>\n",
       "      <td>3.246815e+04</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>1003.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.500000</td>\n",
       "      <td>11.975000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2207.125</td>\n",
       "      <td>3.029467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000005</th>\n",
       "      <td>251.02</td>\n",
       "      <td>9.250</td>\n",
       "      <td>1.21650</td>\n",
       "      <td>809.375000</td>\n",
       "      <td>1.794991e+05</td>\n",
       "      <td>84.375000</td>\n",
       "      <td>809.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.049988</td>\n",
       "      <td>45.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.051250</td>\n",
       "      <td>2207.125</td>\n",
       "      <td>3.029467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.794991e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000006</th>\n",
       "      <td>263.78</td>\n",
       "      <td>9.525</td>\n",
       "      <td>1.21300</td>\n",
       "      <td>757.775024</td>\n",
       "      <td>4.781178e+05</td>\n",
       "      <td>186.649994</td>\n",
       "      <td>757.775024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.524994</td>\n",
       "      <td>89.525002</td>\n",
       "      <td>...</td>\n",
       "      <td>8.071000</td>\n",
       "      <td>2207.125</td>\n",
       "      <td>3.029467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9.562356e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000007</th>\n",
       "      <td>522.72</td>\n",
       "      <td>9.700</td>\n",
       "      <td>1.20750</td>\n",
       "      <td>705.650024</td>\n",
       "      <td>8.926679e+05</td>\n",
       "      <td>311.525024</td>\n",
       "      <td>705.650024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.799988</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13.956500</td>\n",
       "      <td>2207.125</td>\n",
       "      <td>3.029467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.678004e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000000008</th>\n",
       "      <td>904.42</td>\n",
       "      <td>9.550</td>\n",
       "      <td>1.20500</td>\n",
       "      <td>669.650024</td>\n",
       "      <td>1.357902e+06</td>\n",
       "      <td>442.750000</td>\n",
       "      <td>669.650024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.600006</td>\n",
       "      <td>167.100006</td>\n",
       "      <td>...</td>\n",
       "      <td>20.406250</td>\n",
       "      <td>2207.125</td>\n",
       "      <td>3.029467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.431608e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000032965</th>\n",
       "      <td>2525.38</td>\n",
       "      <td>2.750</td>\n",
       "      <td>1.30900</td>\n",
       "      <td>2077.850098</td>\n",
       "      <td>6.655242e+05</td>\n",
       "      <td>237.574997</td>\n",
       "      <td>110.199997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>265.899994</td>\n",
       "      <td>86.849998</td>\n",
       "      <td>...</td>\n",
       "      <td>10.605500</td>\n",
       "      <td>2207.125</td>\n",
       "      <td>3.029467</td>\n",
       "      <td>-4.524994</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.257641e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000032966</th>\n",
       "      <td>3254.46</td>\n",
       "      <td>3.150</td>\n",
       "      <td>1.29375</td>\n",
       "      <td>2077.850098</td>\n",
       "      <td>1.024199e+06</td>\n",
       "      <td>327.200012</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.299988</td>\n",
       "      <td>97.425003</td>\n",
       "      <td>...</td>\n",
       "      <td>16.151001</td>\n",
       "      <td>2207.125</td>\n",
       "      <td>3.029467</td>\n",
       "      <td>-3.524994</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.935531e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000032967</th>\n",
       "      <td>3666.30</td>\n",
       "      <td>3.325</td>\n",
       "      <td>1.28300</td>\n",
       "      <td>2077.850098</td>\n",
       "      <td>1.295218e+06</td>\n",
       "      <td>386.600006</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>268.149994</td>\n",
       "      <td>84.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.556751</td>\n",
       "      <td>2207.125</td>\n",
       "      <td>3.029467</td>\n",
       "      <td>-1.524994</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.447832e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000032968</th>\n",
       "      <td>3757.82</td>\n",
       "      <td>3.050</td>\n",
       "      <td>1.27450</td>\n",
       "      <td>2077.850098</td>\n",
       "      <td>1.443859e+06</td>\n",
       "      <td>409.100006</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.200012</td>\n",
       "      <td>79.824997</td>\n",
       "      <td>...</td>\n",
       "      <td>23.451250</td>\n",
       "      <td>2207.125</td>\n",
       "      <td>3.029467</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.728893e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00.000032969</th>\n",
       "      <td>3608.00</td>\n",
       "      <td>2.800</td>\n",
       "      <td>1.27100</td>\n",
       "      <td>2077.850098</td>\n",
       "      <td>1.454395e+06</td>\n",
       "      <td>392.424988</td>\n",
       "      <td>1170.850098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.125000</td>\n",
       "      <td>78.150002</td>\n",
       "      <td>...</td>\n",
       "      <td>24.550501</td>\n",
       "      <td>2207.125</td>\n",
       "      <td>3.029467</td>\n",
       "      <td>1.174988</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2.748952e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18902 rows Ã— 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               pv_measurement  absolute_humidity_2mgm3  \\\n",
       "1970-01-01 00:00:00.000000004           19.36                    8.950   \n",
       "1970-01-01 00:00:00.000000005          251.02                    9.250   \n",
       "1970-01-01 00:00:00.000000006          263.78                    9.525   \n",
       "1970-01-01 00:00:00.000000007          522.72                    9.700   \n",
       "1970-01-01 00:00:00.000000008          904.42                    9.550   \n",
       "...                                       ...                      ...   \n",
       "1970-01-01 00:00:00.000032965         2525.38                    2.750   \n",
       "1970-01-01 00:00:00.000032966         3254.46                    3.150   \n",
       "1970-01-01 00:00:00.000032967         3666.30                    3.325   \n",
       "1970-01-01 00:00:00.000032968         3757.82                    3.050   \n",
       "1970-01-01 00:00:00.000032969         3608.00                    2.800   \n",
       "\n",
       "                               air_density_2mkgm3  ceiling_height_aglm  \\\n",
       "1970-01-01 00:00:00.000000004             1.21800          1003.500000   \n",
       "1970-01-01 00:00:00.000000005             1.21650           809.375000   \n",
       "1970-01-01 00:00:00.000000006             1.21300           757.775024   \n",
       "1970-01-01 00:00:00.000000007             1.20750           705.650024   \n",
       "1970-01-01 00:00:00.000000008             1.20500           669.650024   \n",
       "...                                           ...                  ...   \n",
       "1970-01-01 00:00:00.000032965             1.30900          2077.850098   \n",
       "1970-01-01 00:00:00.000032966             1.29375          2077.850098   \n",
       "1970-01-01 00:00:00.000032967             1.28300          2077.850098   \n",
       "1970-01-01 00:00:00.000032968             1.27450          2077.850098   \n",
       "1970-01-01 00:00:00.000032969             1.27100          2077.850098   \n",
       "\n",
       "                               clear_sky_energy_1hJ  clear_sky_radW  \\\n",
       "1970-01-01 00:00:00.000000004          3.246815e+04       23.100000   \n",
       "1970-01-01 00:00:00.000000005          1.794991e+05       84.375000   \n",
       "1970-01-01 00:00:00.000000006          4.781178e+05      186.649994   \n",
       "1970-01-01 00:00:00.000000007          8.926679e+05      311.525024   \n",
       "1970-01-01 00:00:00.000000008          1.357902e+06      442.750000   \n",
       "...                                             ...             ...   \n",
       "1970-01-01 00:00:00.000032965          6.655242e+05      237.574997   \n",
       "1970-01-01 00:00:00.000032966          1.024199e+06      327.200012   \n",
       "1970-01-01 00:00:00.000032967          1.295218e+06      386.600006   \n",
       "1970-01-01 00:00:00.000032968          1.443859e+06      409.100006   \n",
       "1970-01-01 00:00:00.000032969          1.454395e+06      392.424988   \n",
       "\n",
       "                               cloud_base_aglm  dew_or_rimeidx  dew_point_2mK  \\\n",
       "1970-01-01 00:00:00.000000004      1003.500000             0.0     282.500000   \n",
       "1970-01-01 00:00:00.000000005       809.375000             0.0     283.049988   \n",
       "1970-01-01 00:00:00.000000006       757.775024             0.0     283.524994   \n",
       "1970-01-01 00:00:00.000000007       705.650024             0.0     283.799988   \n",
       "1970-01-01 00:00:00.000000008       669.650024             0.0     283.600006   \n",
       "...                                        ...             ...            ...   \n",
       "1970-01-01 00:00:00.000032965       110.199997             0.0     265.899994   \n",
       "1970-01-01 00:00:00.000032966       128.000000             0.0     267.299988   \n",
       "1970-01-01 00:00:00.000032967        29.500000             0.0     268.149994   \n",
       "1970-01-01 00:00:00.000032968        29.500000             0.0     267.200012   \n",
       "1970-01-01 00:00:00.000032969      1170.850098             0.0     266.125000   \n",
       "\n",
       "                               diffuse_radW  ...  sun_elevation_lag1  \\\n",
       "1970-01-01 00:00:00.000000004     11.975000  ...                 NaN   \n",
       "1970-01-01 00:00:00.000000005     45.125000  ...            3.051250   \n",
       "1970-01-01 00:00:00.000000006     89.525002  ...            8.071000   \n",
       "1970-01-01 00:00:00.000000007    139.000000  ...           13.956500   \n",
       "1970-01-01 00:00:00.000000008    167.100006  ...           20.406250   \n",
       "...                                     ...  ...                 ...   \n",
       "1970-01-01 00:00:00.000032965     86.849998  ...           10.605500   \n",
       "1970-01-01 00:00:00.000032966     97.425003  ...           16.151001   \n",
       "1970-01-01 00:00:00.000032967     84.250000  ...           20.556751   \n",
       "1970-01-01 00:00:00.000032968     79.824997  ...           23.451250   \n",
       "1970-01-01 00:00:00.000032969     78.150002  ...           24.550501   \n",
       "\n",
       "                               weekly_snow_sum  hourly_mean_wind_speed  \\\n",
       "1970-01-01 00:00:00.000000004         2207.125                3.029467   \n",
       "1970-01-01 00:00:00.000000005         2207.125                3.029467   \n",
       "1970-01-01 00:00:00.000000006         2207.125                3.029467   \n",
       "1970-01-01 00:00:00.000000007         2207.125                3.029467   \n",
       "1970-01-01 00:00:00.000000008         2207.125                3.029467   \n",
       "...                                        ...                     ...   \n",
       "1970-01-01 00:00:00.000032965         2207.125                3.029467   \n",
       "1970-01-01 00:00:00.000032966         2207.125                3.029467   \n",
       "1970-01-01 00:00:00.000032967         2207.125                3.029467   \n",
       "1970-01-01 00:00:00.000032968         2207.125                3.029467   \n",
       "1970-01-01 00:00:00.000032969         2207.125                3.029467   \n",
       "\n",
       "                               daily_temp_change  is_summer  is_winter  \\\n",
       "1970-01-01 00:00:00.000000004                NaN      False       True   \n",
       "1970-01-01 00:00:00.000000005                NaN      False       True   \n",
       "1970-01-01 00:00:00.000000006                NaN      False       True   \n",
       "1970-01-01 00:00:00.000000007                NaN      False       True   \n",
       "1970-01-01 00:00:00.000000008                NaN      False       True   \n",
       "...                                          ...        ...        ...   \n",
       "1970-01-01 00:00:00.000032965          -4.524994      False       True   \n",
       "1970-01-01 00:00:00.000032966          -3.524994      False       True   \n",
       "1970-01-01 00:00:00.000032967          -1.524994      False       True   \n",
       "1970-01-01 00:00:00.000032968           0.250000      False       True   \n",
       "1970-01-01 00:00:00.000032969           1.174988      False       True   \n",
       "\n",
       "                               is_autumn  is_spring  extreme_weather  \\\n",
       "1970-01-01 00:00:00.000000004      False      False            False   \n",
       "1970-01-01 00:00:00.000000005      False      False            False   \n",
       "1970-01-01 00:00:00.000000006      False      False            False   \n",
       "1970-01-01 00:00:00.000000007      False      False            False   \n",
       "1970-01-01 00:00:00.000000008      False      False            False   \n",
       "...                                  ...        ...              ...   \n",
       "1970-01-01 00:00:00.000032965      False      False            False   \n",
       "1970-01-01 00:00:00.000032966      False      False            False   \n",
       "1970-01-01 00:00:00.000032967      False      False            False   \n",
       "1970-01-01 00:00:00.000032968      False      False            False   \n",
       "1970-01-01 00:00:00.000032969      False      False            False   \n",
       "\n",
       "                               trend_energy  \n",
       "1970-01-01 00:00:00.000000004  0.000000e+00  \n",
       "1970-01-01 00:00:00.000000005  1.794991e+05  \n",
       "1970-01-01 00:00:00.000000006  9.562356e+05  \n",
       "1970-01-01 00:00:00.000000007  2.678004e+06  \n",
       "1970-01-01 00:00:00.000000008  5.431608e+06  \n",
       "...                                     ...  \n",
       "1970-01-01 00:00:00.000032965  1.257641e+10  \n",
       "1970-01-01 00:00:00.000032966  1.935531e+10  \n",
       "1970-01-01 00:00:00.000032967  2.447832e+10  \n",
       "1970-01-01 00:00:00.000032968  2.728893e+10  \n",
       "1970-01-01 00:00:00.000032969  2.748952e+10  \n",
       "\n",
       "[18902 rows x 94 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 75)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_aggregations_to_df(df):\n",
    "    # Assicurati che il DataFrame abbia un indice temporale\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Aggregazioni giornaliere\n",
    "   # df['daily_sum_energy'] = df['clear_sky_energy_1hJ'].resample('D').transform('sum')\n",
    "    #df['daily_mean_humidity'] = df['absolute_humidity_2mgm3'].resample('D').transform('mean')\n",
    "    #df['daily_max_temp'] = df['t_1000hPaK'].resample('D').transform('max')\n",
    "\n",
    "    # Aggregazioni settimanali/mensili\n",
    "    #df['weekly_precip'] = df['precip_5minmm'].resample('W').transform('sum')\n",
    "    #df['monthly_pressure_mean'] = df['msl_pressurehPa'].resample('M').transform('mean')\n",
    "\n",
    "    # Media mobile\n",
    "    #df['rolling_7d_cloud_cover'] = df['total_cloud_coverp'].rolling(window=7).mean()\n",
    "\n",
    "    # Trasformazioni cicliche per l'ora\n",
    "    hour = df.index.hour\n",
    "    df['hour_sin'] = np.sin(hour * (2. * np.pi / 24))\n",
    "    df['hour_cos'] = np.cos(hour * (2. * np.pi / 24))\n",
    "\n",
    "    # Differenze temporali\n",
    "   # df['energy_diff'] = df['clear_sky_energy_1hJ'].diff()\n",
    "\n",
    "    # Lag features\n",
    "   # df['sun_elevation_lag1'] = df['sun_elevationd'].shift(1)\n",
    "\n",
    "    ## Aggregazioni basate su condizioni meteorologiche\n",
    "    #df['weekly_snow_sum'] = df['fresh_snow_24hcm'].resample('W').transform('sum')\n",
    "#\n",
    "\n",
    "        # Aggregazioni orarie\n",
    "   # df['hourly_mean_wind_speed'] = df['wind_speed_10mms'].resample('H').transform('mean')\n",
    "\n",
    "    # Variazioni rispetto al giorno precedente\n",
    "    #df['daily_temp_change'] = df['t_1000hPaK'].diff(periods=24)\n",
    "\n",
    "    # Indicatori stagionali\n",
    "    df['is_summer'] = df.index.month.isin([6, 7, 8])\n",
    "    df['is_winter'] = df.index.month.isin([12, 1, 2])\n",
    "    df['is_autumn'] = df.index.month.isin([9,10,11])\n",
    "    df['is_spring'] = df.index.month.isin([3, 4, 5])\n",
    "\n",
    "    # Condizioni meteorologiche estreme\n",
    "    #df['extreme_weather'] = (df['wind_speed_10mms'] > 50) | (df['precip_5minmm'] > 10)\n",
    "\n",
    "    # Tendenze\n",
    "    #df['trend_energy'] = np.arange(len(df)) * df['clear_sky_energy_1hJ'].values\n",
    "    # Puoi continuare ad aggiungere altre aggregazioni qui...\n",
    "\n",
    "    # Rimuovi le colonne con valori NaN dovuti al calcolo delle aggregazioni se necessario\n",
    "    df.fillna(df.median())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Utilizzo:\n",
    "# df Ã¨ il tuo DataFrame con una colonna 'timestamp'\n",
    "# df = add_temporal_aggregations_to_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pv_measurement', 'absolute_humidity_2mgm3', 'air_density_2mkgm3',\n",
       "       'ceiling_height_aglm', 'clear_sky_energy_1hJ', 'clear_sky_radW',\n",
       "       'cloud_base_aglm', 'dew_or_rimeidx', 'dew_point_2mK', 'diffuse_radW',\n",
       "       'diffuse_rad_1hJ', 'direct_radW', 'direct_rad_1hJ',\n",
       "       'effective_cloud_coverp', 'fresh_snow_12hcm', 'fresh_snow_1hcm',\n",
       "       'fresh_snow_24hcm', 'fresh_snow_3hcm', 'fresh_snow_6hcm',\n",
       "       'is_in_shadowidx', 'msl_pressurehPa', 'precip_5minmm',\n",
       "       'precip_type_5minidx', 'pressure_100mhPa', 'pressure_50mhPa',\n",
       "       'prob_rimep', 'rain_waterkgm2', 'relative_humidity_1000hPap',\n",
       "       'sfc_pressurehPa', 'snow_depthcm', 'snow_driftidx', 'snow_melt_10minmm',\n",
       "       'snow_waterkgm2', 'sun_azimuthd', 'sun_elevationd',\n",
       "       'super_cooled_liquid_waterkgm2', 't_1000hPaK', 'total_cloud_coverp',\n",
       "       'visibilitym', 'wind_speed_10mms', 'wind_speed_u_10mms',\n",
       "       'wind_speed_v_10mms', 'wind_speed_w_1000hPams', 'time_dummy',\n",
       "       'is_estimated', 'time_delta', 'hour', 'day_of_week', 'month',\n",
       "       'day_of_year', 'week_of_year', 'year', 'dew_or_rimeidx_-10',\n",
       "       'dew_or_rimeidx_-075', 'dew_or_rimeidx_-05', 'dew_or_rimeidx_-025',\n",
       "       'dew_or_rimeidx_00', 'dew_or_rimeidx_025', 'dew_or_rimeidx_05',\n",
       "       'dew_or_rimeidx_075', 'dew_or_rimeidx_10', 'is_in_shadowidx_00',\n",
       "       'is_in_shadowidx_025', 'is_in_shadowidx_05', 'is_in_shadowidx_075',\n",
       "       'is_in_shadowidx_10', 'is_estimated_0', 'is_estimated_1',\n",
       "       'direct_diffuse_rad_interaction', 'raddir', 'effectivehum',\n",
       "       'Radiazione_solare_effettiva', 'direct_radW_squared', 'clearcloud',\n",
       "       'radiation_squared', 'rolling_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       absolute_humidity_2mgm3  air_density_2mkgm3  ceiling_height_aglm  \\\n",
      "0                        6.625             1.22075          2287.250000   \n",
      "1                        6.275             1.21425          2679.074951   \n",
      "2                        8.350             1.22675           983.799988   \n",
      "3                        8.175             1.22550          1195.349976   \n",
      "4                        8.000             1.22600          1308.599976   \n",
      "...                        ...                 ...                  ...   \n",
      "11018                    4.400             1.27550          1456.574951   \n",
      "11019                    4.400             1.27850          1476.349976   \n",
      "11020                    4.400             1.27900          1516.300049   \n",
      "11021                    4.400             1.27975          1240.599976   \n",
      "11022                    4.400             1.27975          1484.500000   \n",
      "\n",
      "       clear_sky_energy_1hJ  clear_sky_radW  cloud_base_aglm  dew_or_rimeidx  \\\n",
      "0              1.320844e+06      421.225006      2287.250000             0.0   \n",
      "1              1.681730e+06      508.125000      2679.074951             0.0   \n",
      "2              1.876148e+06      488.125000       983.799988             0.0   \n",
      "3              1.600012e+06      396.049988      1195.349976             0.0   \n",
      "4              1.223671e+06      280.899994      1308.599976             0.0   \n",
      "...                     ...             ...              ...             ...   \n",
      "11018          8.401015e+04        4.175000       551.224976             0.0   \n",
      "11019          2.206800e+03        0.000000       564.099976             0.0   \n",
      "11020          0.000000e+00        0.000000       578.700012             0.0   \n",
      "11021          0.000000e+00        0.000000       551.500000             0.0   \n",
      "11022          0.000000e+00        0.000000       558.674988             0.0   \n",
      "\n",
      "       dew_point_2mK  diffuse_radW  diffuse_rad_1hJ  ...  \\\n",
      "0         278.200012     67.974998    233009.921875  ...   \n",
      "1         277.424988     76.625000    260351.078125  ...   \n",
      "2         281.375000     77.199997    245385.468750  ...   \n",
      "3         281.075012     93.474998    307192.062500  ...   \n",
      "4         280.750000     84.900002    321067.031250  ...   \n",
      "...              ...           ...              ...  ...   \n",
      "11018     272.024994      2.775000     54774.000000  ...   \n",
      "11019     271.950012      0.000000      4984.049805  ...   \n",
      "11020     271.899994      0.000000         0.000000  ...   \n",
      "11021     271.950012      0.000000         0.000000  ...   \n",
      "11022     271.950012      0.000000         0.000000  ...   \n",
      "\n",
      "       direct_radW_squared^2  direct_radW_squared clearcloud  \\\n",
      "0               1.307488e+10                    1.502753e+09   \n",
      "1               2.718429e+10                    7.225846e+09   \n",
      "2               7.596912e-02                    1.345394e+04   \n",
      "3               1.362025e+03                    1.461647e+06   \n",
      "4               1.347935e+04                    3.235176e+06   \n",
      "...                      ...                             ...   \n",
      "11018           0.000000e+00                    0.000000e+00   \n",
      "11019           0.000000e+00                    0.000000e+00   \n",
      "11020           0.000000e+00                    0.000000e+00   \n",
      "11021           0.000000e+00                    0.000000e+00   \n",
      "11022           0.000000e+00                    0.000000e+00   \n",
      "\n",
      "       direct_radW_squared radiation_squared  \\\n",
      "0                               2.028837e+10   \n",
      "1                               4.256966e+10   \n",
      "2                               6.567206e+04   \n",
      "3                               5.788854e+06   \n",
      "4                               9.160896e+06   \n",
      "...                                      ...   \n",
      "11018                           0.000000e+00   \n",
      "11019                           0.000000e+00   \n",
      "11020                           0.000000e+00   \n",
      "11021                           0.000000e+00   \n",
      "11022                           0.000000e+00   \n",
      "\n",
      "       direct_radW_squared rolling_mean  clearcloud^2  \\\n",
      "0                          1.568819e+07  1.727179e+08   \n",
      "1                          0.000000e+00  1.920699e+09   \n",
      "2                          2.701125e+01  2.382660e+09   \n",
      "3                          4.340101e+03  1.568556e+09   \n",
      "4                          1.934236e+04  7.764738e+08   \n",
      "...                                 ...           ...   \n",
      "11018                      0.000000e+00  1.664655e+05   \n",
      "11019                      0.000000e+00  0.000000e+00   \n",
      "11020                      0.000000e+00  0.000000e+00   \n",
      "11021                     -0.000000e+00  0.000000e+00   \n",
      "11022                     -0.000000e+00  0.000000e+00   \n",
      "\n",
      "       clearcloud radiation_squared  clearcloud rolling_mean  \\\n",
      "0                      2.331831e+09             1.803113e+06   \n",
      "1                      1.131542e+10             0.000000e+00   \n",
      "2                      1.163036e+10             4.783625e+06   \n",
      "3                      6.212266e+09             4.657548e+06   \n",
      "4                      2.198704e+09             4.642356e+06   \n",
      "...                             ...                      ...   \n",
      "11018                  7.111726e+03             2.079177e+04   \n",
      "11019                  0.000000e+00             0.000000e+00   \n",
      "11020                  0.000000e+00             0.000000e+00   \n",
      "11021                  0.000000e+00            -0.000000e+00   \n",
      "11022                  0.000000e+00            -0.000000e+00   \n",
      "\n",
      "       radiation_squared^2  radiation_squared rolling_mean  rolling_mean^2  \n",
      "0             3.148158e+10                    2.434346e+07      18823.8400  \n",
      "1             6.666260e+10                    0.000000e+00          0.0000  \n",
      "2             5.677069e+10                    2.335007e+07       9604.0000  \n",
      "3             2.460368e+10                    1.844622e+07      13829.7600  \n",
      "4             6.225968e+09                    1.314554e+07      27755.5600  \n",
      "...                    ...                             ...             ...  \n",
      "11018         3.038266e+02                    8.882645e+02       2596.9216  \n",
      "11019         0.000000e+00                    0.000000e+00          8.6436  \n",
      "11020         0.000000e+00                    0.000000e+00          0.0000  \n",
      "11021         0.000000e+00                   -0.000000e+00          0.0000  \n",
      "11022         0.000000e+00                   -0.000000e+00          0.0000  \n",
      "\n",
      "[11023 rows x 2925 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "\n",
    "df = train_data\n",
    "\n",
    "# Seleziona le colonne di interesse (sostituisci con le tue features reali)\n",
    "features = df.drop(columns = target) # Aggiungi altre features se necessario\n",
    "\n",
    "# Inizializza PolynomialFeatures con il grado desiderato\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "\n",
    "# Adatta e trasforma le features selezionate\n",
    "poly_features = poly.fit_transform(features)\n",
    "\n",
    "# Ottieni i nomi delle nuove features polinomiali\n",
    "feature_names = poly.get_feature_names_out(features.columns)\n",
    "\n",
    "# Crea un nuovo DataFrame con le features polinomiali\n",
    "poly_df = pd.DataFrame(poly_features, columns=feature_names)\n",
    "\n",
    "# Stampa il nuovo DataFrame con le features polinomiali\n",
    "print(poly_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
