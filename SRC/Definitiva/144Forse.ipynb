{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pycaret.regression import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we made 3 sets of time features to get models that capture diffrent important aspects from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features nr 1\n",
    "def add_time_features(df, time_column):\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Make sure the time column is in datetime format\n",
    "\n",
    "    # Extract various time features\n",
    "    df['hour'] = df[time_column].dt.hour\n",
    "    df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "    df['month'] = df[time_column].dt.month\n",
    "    df['day_of_year'] = df[time_column].dt.dayofyear\n",
    "    df['week_of_year'] = df[time_column].dt.isocalendar().week \n",
    "    df['year'] = df[time_column].dt.year\n",
    "    #df['sin_hour'] = np.sin(np.pi * df[time_column].dt.hour/24.)\n",
    "    #df['sin_month'] = np.sin(np.pi * df[time_column].dt.month/12.)\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    #df = df.drop(['hour'])\n",
    "    #print(df['sin_hour'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features nr 2\n",
    "def add_time_features_cat(df, time_column):\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Make sure the time column is in datetime format\n",
    "    \n",
    "    df['sin_hour'] = np.sin(np.pi * df[time_column].dt.hour/23.)\n",
    "    df['sin_month'] = np.sin(np.pi * df[time_column].dt.month/12.)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features nr 3\n",
    "def add_time_features_cat_2(df, time_column):\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Make sure the time column is in datetime format\n",
    "\n",
    "    df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "    df['sin_hour'] = np.sin(2*np.pi * df[time_column].dt.hour/23.)\n",
    "    df['sin_month'] = np.sin(2*np.pi * df[time_column].dt.month/12.)\n",
    "    df['cos_hour'] = np.cos(2*np.pi * df[time_column].dt.hour/23.)\n",
    "    df['cos_month'] = np.cos(2*np.pi * df[time_column].dt.month/12.)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added a fuction to plot the target data, we quickly realized that for some periodes the target value was saturated over a long duration. This dosent make sense as poweroutput for solar panels should be fluctuating with the weather data, so we removed this part of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_targets(targets, start_date, end_date):\n",
    "    # Slice the dataframe based on the provided start and end dates\n",
    "    targets_subset = targets[(targets['time'] >= start_date) & (targets['time'] <= end_date)]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(targets_subset['time'], targets_subset['pv_measurement'], label='PV Measurement', color='blue')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('PV Measurement')\n",
    "    plt.title('PV Measurement Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of several preprocessing functions where we remove some features, remove noise and add some extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing LigtGBM + catboost\n",
    "def preprocessing(targets, observed, estimated, test):\n",
    "    \n",
    "    # Ensure the datetime columns are in datetime format\n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "    \n",
    "    # Start the resampling from 15min to 1 hour\n",
    "    date_calc_resampled_ob = estimated.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    date_calc_resampled_te = test.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    \n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_ob, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_te, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    #Save the is_day feature as this says a lot about when the power output is zero or not\n",
    "    is_day_feature = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    \n",
    "    #Drop some features that is noise\n",
    "    test_resampled = test_resampled.drop(columns =['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = observed_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    estimated_resampled = estimated_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    first_date = targets['time'].min()\n",
    "    last_date = targets['time'].max()\n",
    "    \n",
    "    # Printing the results\n",
    "    print(f\"The dataset starts from {first_date} and ends at {last_date}\")\n",
    "\n",
    "    start_date = '2017-07-01'  \n",
    "    end_date = '2024-08-30'  \n",
    "\n",
    "    # Make some extra date features that capture the diffrence in obsverved and estimated data\n",
    "    def process_data(observed, estimated, test):\n",
    "    \n",
    "    # 1. Create time-delta for estimated data\n",
    "      estimated['time_dummy'] = (estimated['date_forecast'] - estimated['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
    "      observed['time_dummy'] = 0 \n",
    "      test['time_dummy'] = (test['date_forecast'] - test['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
    "      \n",
    "      estimated['time_delta'] = (estimated['date_calc'] - estimated['date_forecast']).dt.total_seconds() / 3600\n",
    "      observed['time_delta'] = 0  # since observed data is not forecasting ahead\n",
    "      test['time_delta'] = (test['date_calc'] - test['date_forecast']).dt.total_seconds() / 3600\n",
    "      \n",
    "      # 2. Add indicator variable for estimated data\n",
    "      estimated['is_estimated'] = 1\n",
    "      observed['is_estimated'] = 0\n",
    "      test['is_estimated'] = 1\n",
    "      # Merge or concatenate data\n",
    "      df = pd.concat([observed, estimated], axis=0).sort_values(by='date_forecast')\n",
    "      \n",
    "      return df, test\n",
    "    \n",
    "    # Add extra features\n",
    "    weather_data, test_resampled = process_data(observed_resampled, estimated_resampled, test_resampled)\n",
    "    \n",
    "    # Merge with target values\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "\n",
    "    # Add the time-based features\n",
    "    merged_data = add_time_features(merged_data, 'time')  \n",
    "    test_resampled = add_time_features(test_resampled, 'date_forecast') \n",
    "    \n",
    "    # Remove data where targes are zero as its no reason to train for this\n",
    "    merged_data = merged_data[merged_data['pv_measurement'] != 0]\n",
    "\n",
    "    # Removing data where the power output is saturated\n",
    "    # Step 1: Calculate the difference\n",
    "    merged_data['diff'] = merged_data['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    # Step 2: Create an indicator where diff is zero\n",
    "    merged_data['constant'] = (merged_data['diff'] == 0).astype(int)\n",
    "\n",
    "    # Step 3: Use the indicator where diff is zero. The diff() function here identifies change-points.\n",
    "    merged_data['block'] = (merged_data['constant'].diff() != 0).astype(int).cumsum()\n",
    "    block_sizes = merged_data.groupby('block')['constant'].sum()\n",
    "\n",
    "    # Identify blocks that are constant for more than 2 consecutive time points (you can adjust this threshold)\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "\n",
    "    # Step 4: Remove the constant where diff is zero\n",
    "    filtered_data = merged_data[~merged_data['block'].isin(constant_blocks)]\n",
    "\n",
    "    # Drop time and temporary features\n",
    "    targets_ny = filtered_data[ ['time', 'pv_measurement']]\n",
    "    filtered_data = filtered_data.drop(columns=['diff', 'constant', 'block'])\n",
    "    \n",
    "    plot_targets(targets_ny, start_date, end_date)\n",
    "    \n",
    "    # Drop time features\n",
    "    filtered_data = filtered_data.drop(columns=['time', 'pv_measurement','date_calc'])\n",
    "    test_resampled = test_resampled.drop(columns=[ 'date_forecast', 'date_calc'])\n",
    "    \n",
    "    return filtered_data, test_resampled, is_day_feature, targets_ny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried here to make a preprocessing for a model that trained on all three locations, but we didnt get it to work\n",
    "def preprocessing_cat_2(targets, observed, estimated, test):\n",
    "   #bruk lasso regularisering\n",
    "   #annen learing rate, 0.0001\n",
    "    # Ensure the datetime columns are in datetime format\n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "    \n",
    "    # Resample observed, estimated, and test data to 1 hour using mean() as aggregator\n",
    "    # and drop rows where all columns are NaN\n",
    "    # Convert 'time' to datetime if it isn't already\n",
    "    \n",
    "\n",
    "# Extract hour and group by it to get the index of maximum value in each group\n",
    "    '''idx = targets.groupby([targets['time'].dt.date, targets['time'].dt.hour])['pv_measurement'].idxmax()\n",
    "\n",
    "\n",
    "# Use the indices to get the corresponding rows from the original dataframe\n",
    "    targets_max = targets.loc[idx].reset_index(drop=True)\n",
    "    targets_mean = targets.set_index('time').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    pd.set_option('display.max_rows', None)'''\n",
    "    #print(targets_max, 'max')\n",
    "    #print(targets_mean, 'mean')\n",
    "    \n",
    "\n",
    "# Assuming you already have your dataframes: observed, estimated, test\n",
    "\n",
    "\n",
    "    '''def process_data(df):\n",
    "      # Shift date by 30 minutes\n",
    "      df['date_forecast'] = df['date_forecast'] + pd.Timedelta(minutes=30)\n",
    "      \n",
    "      # Resample to 1H frequency\n",
    "      resampled = df.set_index('date_forecast').resample('1H').mean().reset_index()\n",
    "      \n",
    "      # Drop the last row if it's the last half hour of data\n",
    "      \n",
    "      resampled = resampled.iloc[:-1]\n",
    "      \n",
    "      return resampled\n",
    "    pd.set_option('display.max_rows', None)\n",
    "\n",
    "    \n",
    "    print(estimated)\n",
    "    observed_resampled = process_data(observed)\n",
    "    estimated_resampled = process_data(estimated)\n",
    "    test_resampled = process_data(test)'''\n",
    "    #This code will first shift the data by 30 minutes, then resample it into 1-hour intervals, and finally drop the last row if it represents less than a full hour of data.\n",
    "    \n",
    "    # Start the resampling from 30 minutes past the hour\n",
    "    date_calc_resampled_ob = estimated.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    date_calc_resampled_te = test.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    #print(date_calc_resampled_ob)\n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_ob, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_te, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    #print(estimated_resampled.dtypes)\n",
    "    #print(estimated_resampled[['date_forecast','date_calc']])\n",
    "    is_day_feature = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    \n",
    "    test_resampled = test_resampled.drop(columns =['is_day:idx', 'snow_density:kgm3'])\n",
    "    observed_resampled = observed_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3'])\n",
    "    estimated_resampled = estimated_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3'])\n",
    "    first_date = targets['time'].min()\n",
    "    last_date = targets['time'].max()\n",
    "    \n",
    "    # Printing the results\n",
    "    print(f\"The dataset starts from {first_date} and ends at {last_date}\")\n",
    "\n",
    "    start_date = '2017-07-01'  # Replace with desired start date\n",
    "    end_date = '2024-08-30'  # Replace with desired end date\n",
    "\n",
    "    def process_data(observed, estimated, test):\n",
    "    # Assuming 'date_forecast' is the datetime column for both dataframes.\n",
    "    \n",
    "    # 1. Create time-delta for estimated data\n",
    "      pd.set_option('display.max_rows', None)\n",
    "      #print(estimated['time_delta'])\n",
    "      estimated['time_delta'] = (estimated['date_calc'] - estimated['date_forecast']).dt.total_seconds() / 3600\n",
    "      observed['time_delta'] = 0  # since observed data is not forecasting ahead\n",
    "      test['time_delta'] = (test['date_calc'] - test['date_forecast']).dt.total_seconds() / 3600\n",
    "      pd.set_option('display.max_rows', None)\n",
    "      #print(estimated['time_delta'])\n",
    "      #print(test['time_delta'])\n",
    "      # 2. Add indicator variable for estimated data\n",
    "      estimated['is_estimated'] = 1\n",
    "      observed['is_estimated'] = 0\n",
    "      test['is_estimated'] = 1\n",
    "      # Merge or concatenate data\n",
    "      df = pd.concat([observed, estimated], axis=0).sort_values(by='date_forecast')\n",
    "      #print(df['time_delta'])\n",
    "      return df, test\n",
    "    observed_resampled = observed_resampled[observed_resampled['date_forecast'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    estimated_resampled = estimated_resampled[estimated_resampled['date_forecast'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    weather_data, test_resampled = process_data(observed_resampled, estimated_resampled, test_resampled)\n",
    "    # Merge the observed and estimated data\n",
    "    #weather_data = pd.concat([observed_resampled, estimated_resampled])\n",
    "    targets = targets[targets['time'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    # Merge with target values\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "    # Add the time-based features\n",
    "    merged_data = add_time_features_cat_2(merged_data, 'time')  \n",
    "    test_resampled = add_time_features_cat_2(test_resampled, 'date_forecast') \n",
    "    if merged_data.empty:\n",
    "      print(f\"merged_data is empty for location \")\n",
    "    merged_data = merged_data[merged_data['pv_measurement'] != 0]\n",
    "    \n",
    "    # Step 1: Calculate the difference\n",
    "    merged_data['diff'] = merged_data['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    # Step 2: Create an indicator for constant stretches\n",
    "    merged_data['constant'] = (merged_data['diff'] == 0).astype(int)\n",
    "\n",
    "    # Step 3: Use the indicator to mark stretches. The diff() function here identifies change-points.\n",
    "    merged_data['block'] = (merged_data['constant'].diff() != 0).astype(int).cumsum()\n",
    "\n",
    "    # Get the size of each constant block\n",
    "    block_sizes = merged_data.groupby('block')['constant'].sum()\n",
    "\n",
    "    # Identify blocks that are constant for more than 2 consecutive time points (you can adjust this threshold)\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "\n",
    "    # Step 4: Remove the constant stretches\n",
    "    filtered_data = merged_data[~merged_data['block'].isin(constant_blocks)]\n",
    "    #print(targets.dtypes)\n",
    "    # Clean up auxiliary columns\n",
    "    targets_ny = filtered_data[ ['time', 'pv_measurement']]\n",
    "    filtered_data = filtered_data.drop(columns=['diff', 'constant', 'block'])\n",
    "    \n",
    "    plot_targets(targets_ny, start_date, end_date)\n",
    "    if observed_resampled.empty:\n",
    "      print(f\"observed_resampled is empty for location \")\n",
    "    # Drop non-feature columns\n",
    "    print(test_resampled[ 'date_forecast'])\n",
    "    filtered_data = filtered_data.drop(columns=['time', 'pv_measurement','date_calc'])\n",
    "    test_resampled = test_resampled.drop(columns=[ 'date_forecast', 'date_calc'])\n",
    "    #print(test_resampled.dtypes)\n",
    "    #print(filtered_data.dtypes)\n",
    "    return filtered_data, test_resampled, is_day_feature, targets_ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing nr 2 for some catboost models\n",
    "def preprocessing_cat(targets, observed, estimated, test):\n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "\n",
    "    \n",
    "    # Start the resampling from 15min to 1 hour\n",
    "    date_calc_resampled_ob = estimated.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    date_calc_resampled_te = test.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    \n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_ob, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_te, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    #Save the is_day feature as this says a lot about when the power output is zero or not\n",
    "    is_day_feature = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    \n",
    "    #Drop some features that is noise\n",
    "    test_resampled = test_resampled.drop(columns =['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = observed_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    estimated_resampled = estimated_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    first_date = targets['time'].min()\n",
    "    last_date = targets['time'].max()\n",
    "    \n",
    "    # Printing the results\n",
    "    print(f\"The dataset starts from {first_date} and ends at {last_date}\")\n",
    "\n",
    "    start_date = '2017-07-01'  \n",
    "    end_date = '2024-08-30'\n",
    "\n",
    "    def process_data(observed, estimated, test):\n",
    "      # Add indicator variable for estimated data\n",
    "      estimated['is_estimated'] = 1\n",
    "      observed['is_estimated'] = 0\n",
    "      test['is_estimated'] = 1\n",
    "\n",
    "      # Merge or concatenate data\n",
    "      df = pd.concat([observed, estimated], axis=0).sort_values(by='date_forecast')\n",
    "      \n",
    "      return df, test\n",
    "   \n",
    "    # Filter observed and estimated data for April to August\n",
    "    observed_resampled = observed_resampled[observed_resampled['date_forecast'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    estimated_resampled = estimated_resampled[estimated_resampled['date_forecast'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "\n",
    "    # Merge the observed and estimated data\n",
    "    weather_data, test_resampled = process_data(observed_resampled, estimated_resampled, test_resampled)\n",
    "\n",
    "    # Merge with target values filtering for the same months\n",
    "    targets = targets[targets['time'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "    merged_data = add_time_features_cat(merged_data, 'time')  \n",
    "    test_resampled = add_time_features_cat(test_resampled, 'date_forecast')\n",
    "\n",
    "    # Remove data when targets are zero\n",
    "    merged_data = merged_data[merged_data['pv_measurement'] != 0]\n",
    "\n",
    "    # Removing data where the power output is saturated\n",
    "    # Step 1: Calculate the difference\n",
    "    merged_data['diff'] = merged_data['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    # Step 2: Create an indicator where diff is zero\n",
    "    merged_data['constant'] = (merged_data['diff'] == 0).astype(int)\n",
    "\n",
    "    # Step 3: Use the indicator to mark where diff is zero. The diff() function here identifies change-points.\n",
    "    merged_data['block'] = (merged_data['constant'].diff() != 0).astype(int).cumsum()\n",
    "\n",
    "    # Get the size of each constant block\n",
    "    block_sizes = merged_data.groupby('block')['constant'].sum()\n",
    "\n",
    "    # Identify blocks that are constant for more than 2 consecutive time points (you can adjust this threshold)\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "\n",
    "    # Step 4: Remove where diff is zero\n",
    "    filtered_data = merged_data[~merged_data['block'].isin(constant_blocks)]\n",
    "    \n",
    "    # Drop time and temporary features\n",
    "    targets_ny = filtered_data[ ['time', 'pv_measurement']]\n",
    "    filtered_data = filtered_data.drop(columns=['diff', 'constant', 'block'])\n",
    "\n",
    "\n",
    "    start_date = '2017-07-01'  # Replace with desired start date\n",
    "    end_date = '2024-08-30'  # Replace with desired end date\n",
    "\n",
    "    plot_targets(targets_ny, start_date, end_date)\n",
    "    if observed_resampled.empty:\n",
    "        print(f\"observed_resampled is empty for location \")\n",
    "    \n",
    "    # Drop some time features\n",
    "    filtered_data = filtered_data.drop(columns=['time', 'date_forecast', 'pv_measurement','date_calc'])\n",
    "    test_resampled = test_resampled.drop(columns=[ 'date_forecast','date_calc'])\n",
    "    \n",
    "    return filtered_data, test_resampled, is_day_feature, targets_ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "def process_location_rf(X, y, location_name):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "\n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=123,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    #remove_outliers=True,  #Got worse with this one\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Random forest\n",
    "    rf= create_model('rf')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_rf = tune_model(rf)#, early_stopping=True, fold=15)\n",
    "    print(tuned_rf)\n",
    "    \n",
    "    # Create a boosted version of the tuned model\n",
    "    boosting_rf = ensemble_model(tuned_rf, method='Boosting')\n",
    "\n",
    "    # Finalize the model - this will train it on the complete dataset\n",
    "    final_model = finalize_model(boosting_rf)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked model, tested with this but didnt quite figure it out, so we ended up not using this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack model\n",
    "def process_location_stacked(X,X_cat, y,y_cat, location_name):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=123,train_size=0.8,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    #remove_outliers=True,  #Ble dårligere med denne\n",
    "                    html=False,\n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "    tuned_lightgbm = tune_model(lightgbm)\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "    \n",
    "    # RF\n",
    "    rf= create_model('rf')\n",
    "    tuned_rf = tune_model(rf, choose_better=True)\n",
    "    boosting_rf = ensemble_model(tuned_rf, method='Boosting')\n",
    "    \n",
    "    data_cat = X_cat.copy()\n",
    "    data_cat['target'] = y_cat['pv_measurement']\n",
    "    exp_reg = setup(data=data_cat, target='target', session_id=123,train_size=0.8,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    #remove_outliers=True,  #Ble dårligere med denne\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "    \n",
    "    # Cat\n",
    "    cat = create_model('catboost')\n",
    "    tuned_cat = tune_model(cat, choose_better=True)\n",
    "    bagged_cat = ensemble_model(tuned_cat, method='Bagging')\n",
    "    \n",
    "    # Stacking\n",
    "    stacked = stack_models(estimator_list = [bagged_lightgbm, boosting_rf, bagged_cat], meta_model = bagged_lightgbm)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(stacked, f'final_model_for_location_{location_name}')\n",
    "\n",
    "    return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with some extra features\n",
    "def process_location_ex(X, y, location_name,seeds):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "\n",
    "    # Added some extra features to this one model, did it here so we could reuse the same preprocesssing function on diffrent models\n",
    "    # Feature Combination 1: Solar Radiation and Cloud Cover Combination\n",
    "    data['weighted_rad'] = ((data['direct_rad:W'] * (1 - data['total_cloud_cover:p']/100)) +\n",
    "                        (data['diffuse_rad:W'] * (data['total_cloud_cover:p']/100)))\n",
    "\n",
    "    # Feature Combination 2: Atmospheric Conditions Combination\n",
    "    data['adjusted_clear_sky_rad'] = (data['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * data['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (data['air_density_2m:kgm3'] - 1.225)))  # Adjusted based on humidity and air density\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    #remove_outliers=True,  #Ble dårligere med denne\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm)#, early_stopping=True, fold=15)\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM model\n",
    "def process_location(X, y, location_name, seeds):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    #remove_outliers=True,  #Ble dårligere med denne\n",
    "                    html=False,\n",
    "                    #silent=True, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm)#, early_stopping=True, fold=15)\n",
    "    \n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cat model 2\n",
    "def process_location_cat_2(X, y, location_name,seeds):\n",
    "    \n",
    "    # Dropping some features for this one model\n",
    "    features_to_drop = ['dew_or_rime:idx', #'snow_density:kgm3',\n",
    "                        'fresh_snow_3h:cm', 'fresh_snow_1h:cm', 'snow_drift:idx', \n",
    "                        'snow_depth:cm', 'wind_speed_w_1000hPa:ms', 'prob_rime:p', \n",
    "                        'fresh_snow_6h:cm', 'snow_melt_10min:mm', \n",
    "                        'fresh_snow_12h:cm', 'rain_water:kgm2', \n",
    "                        'super_cooled_liquid_water:kgm2']\n",
    "    \n",
    "    X = X.drop(columns=features_to_drop)\n",
    "    \n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    #categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    #remove_outliers=True,  #Ble dårligere med denne\n",
    "                    html=False,\n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a Catboost model\n",
    "    cat = create_model('catboost')\n",
    "\n",
    "    # Tune the model\n",
    "    tuned_cat = tune_model(cat)\n",
    "    \n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_cat = ensemble_model(tuned_cat, method='Bagging')\n",
    "\n",
    "    # Train on whole dataset\n",
    "    final_model = finalize_model(bagged_cat)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global lists to save predictions in\n",
    "locations = ['A', 'B', 'C']\n",
    "all_predictions_lGBM = []\n",
    "all_predictions_lGBM_e = []\n",
    "all_predictions_rf = []\n",
    "all_predictions_lasso = []\n",
    "all_predictions_cat = []\n",
    "all_predictions_cat_2 = []\n",
    "final_df_list = [] \n",
    "all_pred_stacked =[]\n",
    "all_predictions_cat_3=[]\n",
    "\n",
    "all_X_train_cat = pd.DataFrame()\n",
    "all_X_test_cat = pd.DataFrame()\n",
    "all_is_day_feature1 = pd.Series(dtype='float64')\n",
    "all_targets_cat = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM training and predictions\n",
    "for loc in locations:\n",
    "\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Calling preprocessing\n",
    "    X_train_1, X_test_1, is_day_feature_1, targets_1 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    \n",
    "    # Adding the extra features to the test set as well\n",
    "    X_train_1 = X_train_1.drop(columns=['date_forecast'])\n",
    "    X_test_1['weighted_rad'] = ((X_test_1['direct_rad:W'] * (1 - X_test_1['total_cloud_cover:p']/100)) +\n",
    "                        (X_test_1['diffuse_rad:W'] * (X_test_1['total_cloud_cover:p']/100)))\n",
    "\n",
    "    X_test_1['adjusted_clear_sky_rad'] = (X_test_1['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * X_test_1['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (X_test_1['air_density_2m:kgm3'] - 1.225)))\n",
    "    \n",
    "    # Training and prediction for diffrent seeds\n",
    "    total_predictions_light = None\n",
    "    seeds = [42]\n",
    "    for seed in seeds: \n",
    "        final_model_lGBM_e = process_location_ex(X_train_1, targets_1, loc, seed)\n",
    "        predictions_lGBM_e = predict_model(final_model_lGBM_e, data=X_test_1)\n",
    "        final_predictions_lGBM_e = predictions_lGBM_e['prediction_label']\n",
    "        if total_predictions_light is None:\n",
    "            total_predictions_light = np.zeros_like(final_predictions_lGBM_e)\n",
    "        total_predictions_light += final_predictions_lGBM_e\n",
    "\n",
    "    mean_pred_light = total_predictions_light/len(seeds)\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_lGBM_e = mean_pred_light * is_day_feature_1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_lGBM_e = np.clip(adjusted_final_predictions_lGBM_e, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_lGBM_e.append([adjusted_final_predictions_lGBM_e])\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cat_1 training and predictions\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train_cat, X_test_cat, is_day_feature1, targets_cat = preprocessing_cat(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Making categorical features\n",
    "    cat_features = ['dew_or_rime:idx' ,'is_in_shadow:idx']\n",
    "    X_train_cat['dew_or_rime:idx'] = X_train_cat['dew_or_rime:idx'].astype(int)\n",
    "    X_train_cat['is_in_shadow:idx'] = X_train_cat['is_in_shadow:idx'].astype(int)\n",
    "    X_test_cat['dew_or_rime:idx'] = X_test_cat['dew_or_rime:idx'].astype(int)\n",
    "    X_test_cat['is_in_shadow:idx'] = X_test_cat['is_in_shadow:idx'].astype(int)\n",
    "\n",
    "    # Catboooooooozt fun\n",
    "    model_cat = CatBoostRegressor(\n",
    "        loss_function='MAE', \n",
    "        learning_rate=0.05, \n",
    "        verbose=200,\n",
    "        cat_features=cat_features,\n",
    "        random_state=42, \n",
    "        n_estimators=20000,\n",
    "        early_stopping_rounds=50,)\n",
    "\n",
    "    X_train_cat1, X_val_cat1, y_train_cat1, y_val_cat1 = train_test_split(X_train_cat, targets_cat, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Training\n",
    "    model_cat.fit(X_train_cat1, y_train_cat1['pv_measurement'],eval_set=(X_val_cat1, y_val_cat1['pv_measurement']),)\n",
    "\n",
    "    # Prediction\n",
    "    predictions_cat = model_cat.predict(X_test_cat)\n",
    "    \n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat = predictions_cat * is_day_feature1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat = np.clip(adjusted_final_predictions_cat, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat.append(adjusted_final_predictions_cat)\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_cat = np.array(all_predictions_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost nr 2\n",
    "for loc in locations:\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train, X_test, is_day_feature, targets = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Dropping some winter months\n",
    "    X_train_cat_2 = X_train[X_train['date_forecast'].dt.month.isin([3,4, 5, 6, 7, 8, 9,10])]\n",
    "\n",
    "    # Dropping date feature\n",
    "    X_train_cat_2 = X_train_cat_2.drop(columns=['date_forecast'])\n",
    "\n",
    "    # Training and prediction for diffrent seeds\n",
    "    seeds = [123]\n",
    "    total_predictions_cat_2 = None\n",
    "    for seed in seeds: \n",
    "        final_model_cat_2 = process_location_cat_2(X_train_cat_2, targets, loc,seed)#its aactually a catboost wohoo\n",
    "        predictions_cat_2 = predict_model(final_model_cat_2, X_test)\n",
    "        final_predictions_cat_2 = predictions_cat_2['prediction_label']\n",
    "        if total_predictions_cat_2 is None:\n",
    "            total_predictions_cat_2 = np.zeros_like(final_predictions_cat_2)\n",
    "            total_predictions_cat_2+=final_predictions_cat_2\n",
    "\n",
    "    mean_pred_cat_2 = total_predictions_cat_2/len(seeds)\n",
    "\n",
    "    adjusted_final_predictions_cat_2 = mean_pred_cat_2 * is_day_feature['is_day:idx']\n",
    "    adjusted_final_predictions_cat_2 = np.clip(adjusted_final_predictions_cat_2, 0, None)\n",
    "    all_predictions_cat_2.append([adjusted_final_predictions_cat_2])\n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_rf = []\n",
    "# Random forest\n",
    "for loc in locations:\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    X_train_rf, X_test_rf, is_day_feature_rf, targets_rf = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    \n",
    "    \n",
    "    y_train = targets\n",
    "   \n",
    "    # Filter observed and estimated data for April to August\n",
    "    X_train_rf = X_train_rf[X_train_rf['date_forecast'].dt.month.isin([3, 4, 5, 6, 7, 8, 9])]\n",
    " \n",
    "    X_train_rf = X_train_rf.drop(columns=['date_forecast'])\n",
    "    \n",
    "    X_train_rf['sin_sun_azimuth'] = np.sin(np.radians(X_train_rf['sun_azimuth:d']))\n",
    "    X_train_rf['cos_sun_azimuth'] = np.cos(np.radians(X_train_rf['sun_azimuth:d']))\n",
    "    \n",
    "    X_test_rf['sin_sun_azimuth'] = np.sin(np.radians(X_test_rf['sun_azimuth:d']))\n",
    "    X_test_rf['cos_sun_azimuth'] = np.cos(np.radians(X_test_rf['sun_azimuth:d']))\n",
    "\n",
    "    # Now drop the original 'sun_azimuth' feature\n",
    "    X_train_rf.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    X_test_rf.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    \n",
    "    X_test_1['weighted_rad'] = ((X_test_1['direct_rad:W'] * (1 - X_test_1['total_cloud_cover:p']/100)) +\n",
    "                        (X_test_1['diffuse_rad:W'] * (X_test_1['total_cloud_cover:p']/100)))\n",
    "\n",
    "# Feature Combination 2: Atmospheric Conditions Combination\n",
    "    X_test_1['adjusted_clear_sky_rad'] = (X_test_1['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * X_test_1['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (X_test_1['air_density_2m:kgm3'] - 1.225)))\n",
    "    \n",
    "    final_model_rf = process_location_rf(X_train_rf, targets_rf, loc)\n",
    "    predictions_rf = predict_model(final_model_rf, data=X_test_rf)\n",
    "    \n",
    "    final_predictions_rf = predictions_rf['prediction_label']\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Multiply final predictions with the 'is_day:idx' values\n",
    "    #adjusted_final_predictions_lGBM = final_predictions_lGBM * is_day_feature['is_day:idx']\n",
    "    adjusted_final_predictions_rf = final_predictions_rf * is_day_feature['is_day:idx']\n",
    "   \n",
    "    \n",
    "    # Cliping values under zero to zero\n",
    "    #adjusted_final_predictions_lGBM = np.clip(adjusted_final_predictions_lGBM, 0, None)\n",
    "    \n",
    "    adjusted_final_predictions_rf = np.clip(adjusted_final_predictions_rf, 0, None)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Store predictions\n",
    "    #all_predictions_lGBM.append([adjusted_final_predictions_lGBM])\n",
    "    \n",
    "    all_predictions_rf.append([adjusted_final_predictions_rf]) \n",
    "     \n",
    "     \n",
    "    \n",
    " \n",
    "all_predictions_rf = np.array(all_predictions_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e).flatten()\n",
    "all_predictions_rf = np.array(all_predictions_rf).flatten()\n",
    "#all_predictions_lasso = np.array(all_predictions_cat_2).flatten()\n",
    "all_predictions_cat = np.array(all_predictions_cat).flatten()\n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3).flatten()\n",
    "#all_predictions_cat_2 = np.array(all_predictions_cat_2).flatten()'''\n",
    "#all_pred_stacked = np.array(all_pred_stacked).flatten()\n",
    "all_pred = 0.25*all_predictions_cat+0.25 * all_predictions_lGBM_e+0.35*all_predictions_cat_3+ 0.15*all_predictions_rf#+ 0.45*all_predictions_lGBM  +  0.1*all_predictions_cat + 0.25*all_predictions_lasso + 0.1*all_predictions_rf\n",
    "#all_pred = all_predictions_rf\n",
    "print(all_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission\n",
    "sample_submission = sample_submission[['id']].merge(final_df[['id', 'prediction']], on='id', how='left')\n",
    "sample_submission.to_csv('my_first_submission.csv', index=False)'''\n",
    "\n",
    "final_predictions = all_pred\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df.to_csv('final_predictions1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd  # assuming you're using pandas for your data manipulation\n",
    "\n",
    "# Function to get the next run_id\n",
    "def get_next_run_id():\n",
    "    counter_file = 'run_counter.txt'\n",
    "\n",
    "    if not os.path.exists(counter_file):\n",
    "        with open(counter_file, 'w') as file:\n",
    "            file.write('1')\n",
    "            return 'run1'\n",
    "\n",
    "    with open(counter_file, 'r') as file:\n",
    "        current_count = int(file.read())\n",
    "\n",
    "    new_run_id = f\"run{current_count}\"\n",
    "\n",
    "    with open(counter_file, 'w') as file:\n",
    "        file.write(str(current_count + 1))\n",
    "\n",
    "    return new_run_id\n",
    "\n",
    "# Get the next run_id\n",
    "run_id = get_next_run_id()\n",
    "\n",
    "# Your code that generates final_df_save goes here\n",
    "\n",
    "# Save the predictions to a CSV, including the run_id in the filename\n",
    "save.to_csv(f'predictions_{run_id}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "# Directory where the prediction files are saved\n",
    "pred_dir = os.getcwd()\n",
    "\n",
    "regex = re.compile(r'predictions_run\\d+.*\\.csv$')\n",
    "\n",
    "# List all prediction CSV files in the directory that match the pattern\n",
    "pred_files = [f for f in os.listdir(pred_dir) if regex.match(f)]\n",
    "\n",
    "# Create a dictionary of dataframes, each containing data from the prediction files\n",
    "dfs = {}\n",
    "for file in pred_files:\n",
    "    # The key will be the filename without extension, you can customize this part as needed\n",
    "    key = file.replace('.csv', '')  \n",
    "    dfs[key] = pd.read_csv(os.path.join(pred_dir, file))\n",
    "\n",
    "# Assuming all prediction files have the same locations\n",
    "locations = dfs[list(dfs.keys())[0]]['location'].unique()\n",
    "\n",
    "# Define a color palette\n",
    "colors = plt.cm.tab10.colors  # Built-in color palette\n",
    "\n",
    "\n",
    "# Create a product of styles and markers to cycle through\n",
    "\n",
    "\n",
    "for loc in locations:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    color_idx = 0  # Create a cycle iterator\n",
    "    \n",
    "    # Plot each dataframe on the same plot\n",
    "    for run_id, df in dfs.items():\n",
    "        temp_df = df[df['location'] == loc]\n",
    "        \n",
    "        plt.plot(temp_df['time'], temp_df['prediction'], label=f'{run_id} for Location {loc}', color=colors[color_idx])\n",
    "        color_idx = (color_idx + 1) % len(colors)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.title(f'Predictions Over Time for Location {loc}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the comparison figure\n",
    "    plt.savefig(f'Location_{loc}_comparisons.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['absolute_humidity_2m:gm3', 'air_density_2m:kgm3',\n",
       "       'ceiling_height_agl:m', 'clear_sky_energy_1h:J', 'clear_sky_rad:W',\n",
       "       'cloud_base_agl:m', 'dew_or_rime:idx', 'dew_point_2m:K',\n",
       "       'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J',\n",
       "       'effective_cloud_cover:p', 'fresh_snow_12h:cm', 'fresh_snow_1h:cm',\n",
       "       'fresh_snow_24h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm',\n",
       "       'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
       "       'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
       "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
       "       'sfc_pressure:hPa', 'snow_depth:cm', 'snow_drift:idx',\n",
       "       'snow_melt_10min:mm', 'snow_water:kgm2', 'sun_azimuth:d',\n",
       "       'sun_elevation:d', 'super_cooled_liquid_water:kgm2', 't_1000hPa:K',\n",
       "       'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms',\n",
       "       'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms', 'wind_speed_w_1000hPa:ms',\n",
       "       'time_dummy', 'time_delta', 'is_estimated', 'hour', 'day_of_week',\n",
       "       'month', 'day_of_year', 'week_of_year', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-name",
   "language": "python",
   "name": "pycaret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
