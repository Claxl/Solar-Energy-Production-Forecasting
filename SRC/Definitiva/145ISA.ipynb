{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycaret.regression import *\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD TIME FEATURES\n",
    "def add_time_features(df, time_column, mode = 'lgbm'):\n",
    "    '''\n",
    "        This function will add some time feature based on the param 'time_columns'\n",
    "        \n",
    "        Params:\n",
    "            df-> Dataframe with the column contained in 'time_column'\n",
    "            time_column -> the column that is a datetime object\n",
    "        \n",
    "        Returns:\n",
    "            A dataframe with time features\n",
    "    '''\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Make sure the time column is in datetime format\n",
    "    if mode == 'lgbm':\n",
    "        df['hour'] = df[time_column].dt.hour\n",
    "        df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "        df['month'] = df[time_column].dt.month\n",
    "        df['day_of_year'] = df[time_column].dt.dayofyear\n",
    "        df['week_of_year'] = df[time_column].dt.isocalendar().week\n",
    "        df['year'] = df[time_column].dt.year\n",
    "    elif mode == 'cat_boost':\n",
    "        df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "        df['sin_hour'] = np.sin(2*np.pi * df[time_column].dt.hour/23.)\n",
    "        df['sin_month'] = np.sin(2*np.pi * df[time_column].dt.month/12.)\n",
    "        df['cos_hour'] = np.cos(2*np.pi * df[time_column].dt.hour/23.)\n",
    "        df['cos_month'] = np.cos(2*np.pi * df[time_column].dt.month/12.)\n",
    "    elif mode == 'cat':\n",
    "        df['sin_hour'] = np.sin(2*np.pi * df[time_column].dt.hour/23.)\n",
    "        df['sin_month'] = np.sin(2*np.pi * df[time_column].dt.month/12.)   \n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(df,column):\n",
    "    '''\n",
    "        Make the column in datetime format\n",
    "    '''\n",
    "    return pd.to_datetime(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(df,column):\n",
    "    '''\n",
    "        Resample df to 1 hour using mean() as aggregator and drop rows where all columns are NaN\n",
    "        \n",
    "        Params :\n",
    "            df -> the dataframe to be resampled\n",
    "            column -> the time column\n",
    "    '''\n",
    "    return df.set_index(keys = column).resample('1H').mean().dropna(how='all').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df,columnlist):\n",
    "    return df.drop(columns = columnlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_calc(df):\n",
    "    '''\n",
    "    This function create a dataframe with 'date_forecast' as index and the column 'date_calc' resampled by '1H'.\n",
    "    If there's no data in a specific bin, the resulting value for that bin would be NaN (not a number).\n",
    "    Params:\n",
    "        df -> dataframe with 'date_forecast' and 'date_calc' columns.\n",
    "            'date_calc' is expected to contain data that the user wants to resample or analyze.\n",
    "    Returns:\n",
    "        A dataframe with 'date_calc' resampled.\n",
    "    '''\n",
    "    return df.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_estimated_feature(df):\n",
    "    '''\n",
    "        This function will create some time feature and estimated information. It's need to let the model understand is\n",
    "        estimated value.\n",
    "        Params:\n",
    "            df -> It MUST be an estimated dataframe, that contains 'data_forecast' as datetime type\n",
    "        Returns:\n",
    "            A dataframe with 'time_dummy', 'time_delta' and 'is_estimated'     \n",
    "    '''\n",
    "    df['time_delta'] = (df['date_calc'] - df['date_forecast']).dt.total_seconds() / 3600\n",
    "    df['is_estimated'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stationarity(df):\n",
    "    '''\n",
    "    Removes constant stretches of data within a DataFrame where the 'pv_measurement' column does not change.    \n",
    "    The function identifies blocks of data where the 'pv_measurement' stays constant for more than two consecutive\n",
    "    points and removes these blocks to address data stationarity.\n",
    "\n",
    "    params:\n",
    "        df -> DataFrame\n",
    "              A pandas DataFrame with a 'pv_measurement' column which contains the data from which to remove stationarity.\n",
    "        \n",
    "    return:\n",
    "        The DataFrame with constant stretches of data removed from the 'pv_measurement' column.\n",
    "    '''\n",
    "    \n",
    "    #Calculate the difference, this need for check the constant\n",
    "    df['diff'] = df['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    # Create an indicator for constant stretches\n",
    "    df['constant'] = (df['diff'] == 0).astype(int)\n",
    "\n",
    "    # Use the indicator to mark stretches. The diff() function here identifies change-points.\n",
    "    df['block'] = (df['constant'].diff() != 0).astype(int).cumsum()\n",
    "\n",
    "    # Get the size of each constant block\n",
    "    block_sizes = df.groupby('block')['constant'].sum()\n",
    "\n",
    "    # Identify blocks that are constant for more than N consecutive time points (in this case 2)\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "    \n",
    "    # Remove the constant\n",
    "    filtered_df = df[~df['block'].isin(constant_blocks)]\n",
    "        \n",
    "    return filtered_df.drop(columns=['diff', 'constant', 'block'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(targets, observed, estimated, test, mode: str = 'lgbm'):\n",
    "    '''\n",
    "        This function makes all the preprocessing needed for the correct run of the model, it will perform:\n",
    "            - Resampling\n",
    "            - Filtering\n",
    "            - Imputation\n",
    "            - Outliers removal\n",
    "            - Categorical Encoding\n",
    "        \n",
    "        Params:\n",
    "            targets -> dataframe of the target parquet\n",
    "            observed -> dataframe of observed train data\n",
    "            estimated -> dataframe of estimated train data\n",
    "            test -> dataframe of test data\n",
    "        Returns:\n",
    "            train_data -> dataframe of all data ready to train\n",
    "            test_data -> dataframe of all data ready to test\n",
    "            is_day -> dataframe of is_day categorical feature for post processing\n",
    "    \n",
    "    '''    \n",
    "    targets['time'] = to_datetime(targets,'time')\n",
    "    estimated['date_forecast'] = to_datetime(estimated,'date_forecast')\n",
    "    observed['date_forecast'] = to_datetime(observed,'date_forecast')\n",
    "    test['date_forecast'] = to_datetime(test,'date_forecast')\n",
    "\n",
    "    observed_resampled = resampling(observed,'date_forecast')\n",
    "    estimated_resampled = resampling(estimated,'date_forecast')\n",
    "    test_resampled = resampling(test,'date_forecast')\n",
    "    \n",
    "    date_calc_resampled_observed = extract_data_calc(estimated)\n",
    "    date_calc_resampled_test = extract_data_calc(test)\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_observed, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_test, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    is_day = test_resampled[['date_forecast', 'is_day:idx']]   \n",
    "    test_resampled = filter_df(test_resampled,['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = filter_df(observed_resampled,['is_day:idx', 'snow_density:kgm3','elevation:m']) \n",
    "    estimated_resampled = filter_df(estimated_resampled,[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    \n",
    "    #This MUST be zero because is not estimated.\n",
    "    observed_resampled['is_estimated'] = 0\n",
    "    observed_resampled['time_delta'] = 0\n",
    "    \n",
    "    estimated_resampled = is_estimated_feature(estimated_resampled)\n",
    "    test_resampled = is_estimated_feature(test_resampled)\n",
    "    \n",
    "    X = pd.concat([observed_resampled,estimated_resampled],axis = 0)\n",
    "    train_data = pd.merge(targets, X, how='inner', left_on='time', right_on='date_forecast')\n",
    "    if mode == 'lgbm':\n",
    "        train_data = add_time_features(train_data, 'time')\n",
    "        test_data = add_time_features(test_resampled, 'date_forecast')\n",
    "    elif mode == 'cat':\n",
    "        train_data = add_time_features(train_data, 'time', mode = 'cat')\n",
    "        test_data = add_time_features(test_resampled, 'date_forecast', mode = 'cat')\n",
    "        train_data = train_data[train_data['date_forecast'].dt.month.isin([4,5,6,7,8])]\n",
    "        test_data = test_data[test_data['date_forecast'].dt.month.isin([4,5,6,7,8])]\n",
    "    elif mode == 'cat_boost':\n",
    "        train_data = add_time_features(train_data, 'time', mode = 'cat_boost')\n",
    "        test_data = add_time_features(test_resampled, 'date_forecast', mode = 'cat_boost')\n",
    "\n",
    "    \n",
    "    train_data = delete_stationarity(train_data)\n",
    "    \n",
    "    train_data = filter_df(train_data, ['time','date_calc'])\n",
    "    test_data = filter_df(test_resampled, ['date_calc'])\n",
    "\n",
    "    return train_data, test_data, is_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with some extra features\n",
    "def process_location(X, y, location_name,seeds):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm ,optimize='MAE',n_iter=100,early_stopping=True,early_stopping_max_iters=10)\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_FEATURES = [\n",
    "    \"direct_rad:W\",\n",
    "    \"clear_sky_rad:W\",\n",
    "    \"diffuse_rad:W\",\n",
    "    \"direct_rad_1h:J\",\n",
    "    \"is_in_shadow:idx\",\n",
    "    \"clear_sky_energy_1h:J\",\n",
    "    \"effective_cloud_cover:p\",\n",
    "    \"visibility:m\",\n",
    "    \"total_cloud_cover:p\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    # Added some extra features to this one model, did it here so we could reuse the same preprocesssing function on diffrent models\n",
    "    # Feature Combination 1: Solar Radiation and Cloud Cover Combination\n",
    "    data['radcloud'] = ((data['direct_rad:W'] * (1 - data['total_cloud_cover:p']/100)) +\n",
    "                        (data['diffuse_rad:W'] * (data['total_cloud_cover:p']/100)))\n",
    "\n",
    "    # Feature Combination 2: Atmospheric Conditions Combination\n",
    "    data['adjusted_clear_sky_rad'] = (data['clear_sky_rad:W'] * np.exp(-0.0001 * data['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (data['air_density_2m:kgm3'] - 1.225)))  # Adjusted based on humidity and air density\n",
    "    data['solar_incidence_factor'] = np.cos(np.radians(90 - data['sun_elevation:d'])) * np.cos(np.radians(data['sun_azimuth:d']))\n",
    "    data['seasonal_conversion_efficiency'] = data['radcloud'] * (1 - data['relative_humidity_1000hPa:p']/100) * (data['msl_pressure:hPa'] / 1013.25)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regex(df):\n",
    "    '''\n",
    "        This function let lgbm work, this because it cannot accept ':'\n",
    "    '''\n",
    "    return df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_-]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global lists to save predictions in\n",
    "locations = ['A', 'B', 'C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM training and predictions\n",
    "all_predictions_lGBM_e = []\n",
    "for loc in locations:\n",
    "\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Calling preprocessing\n",
    "    train, test, is_day_feature = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "   \n",
    "    targets = pd.DataFrame( {'pv_measurement': train['pv_measurement']})\n",
    "    X_train = train.drop(columns=['date_forecast','pv_measurement'])\n",
    "    X_train = feature_engineering(X_train)\n",
    "    X_test = test.drop(columns=['date_forecast'])\n",
    "    X_test = feature_engineering(X_test)\n",
    "    \n",
    "    # Training and prediction for diffrent seeds\n",
    "    total_predictions_light = None\n",
    "    seeds = [42]\n",
    "    for seed in seeds: \n",
    "        final_model_lGBM_e = process_location(X_train, targets, loc, seed)\n",
    "        predictions_lGBM_e = predict_model(final_model_lGBM_e, data=X_test)\n",
    "        final_predictions_lGBM_e = predictions_lGBM_e['prediction_label']\n",
    "        if total_predictions_light is None:\n",
    "            total_predictions_light = np.zeros_like(final_predictions_lGBM_e)\n",
    "        total_predictions_light += final_predictions_lGBM_e\n",
    "\n",
    "    mean_pred_light = total_predictions_light/len(seeds)\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_lGBM_e = mean_pred_light * is_day_feature['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_lGBM_e = np.clip(adjusted_final_predictions_lGBM_e, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_lGBM_e.append([adjusted_final_predictions_lGBM_e])\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['dew_or_rime:idx' ,'is_in_shadow:idx']\n",
    "cat_params = { 'A': {\n",
    "                        \"iterations\": 5000,\n",
    "                        \"learning_rate\": 0.034867396508006264,\n",
    "                        \"depth\": 8,\n",
    "                        \"l2_leaf_reg\": 1,\n",
    "                        \"loss_function\": \"MAE\",\n",
    "                        \"border_count\": 92,\n",
    "                        \"verbose\": 500,\n",
    "                        \"subsample\": 0.7641850606486046,\n",
    "                        'early_stopping_rounds': 100,\n",
    "                        'cat_features': cat_features,\n",
    "                        'random_state': 42, \n",
    "                    },\n",
    "              'B': {\n",
    "                        \"iterations\": 5000,\n",
    "                        \"learning_rate\": 0.037511244177544326,\n",
    "                        \"depth\": 6,\n",
    "                        \"l2_leaf_reg\": 5,\n",
    "                        \"loss_function\": \"MAE\",\n",
    "                        \"border_count\": 128,\n",
    "                        \"verbose\": 500,\n",
    "                        \"subsample\": 0.8012204629505595,\n",
    "                        'early_stopping_rounds': 100,\n",
    "                        'cat_features': cat_features,\n",
    "                        'random_state': 42, \n",
    "                    },\n",
    "              'C': {\"iterations\": 5000, \n",
    "                    \"learning_rate\": 0.03425599789981457,\n",
    "                    \"depth\": 8,\n",
    "                    \"l2_leaf_reg\": 4,\n",
    "                    \"loss_function\": \"MAE\", \n",
    "                    \"border_count\": 218, \n",
    "                    \"verbose\": 500, \n",
    "                    \"subsample\": 0.6848272280307022, \n",
    "                    'early_stopping_rounds': 100,\n",
    "                    'cat_features': cat_features,\n",
    "                    'random_state': 42, }\n",
    "}\n",
    "              \n",
    "cat_params_no_feature =  { 'A': {\n",
    "                        \"iterations\": 5000,\n",
    "                        \"learning_rate\": 0.034867396508006264,\n",
    "                        \"depth\": 8,\n",
    "                        \"l2_leaf_reg\": 1,\n",
    "                        \"loss_function\": \"MAE\",\n",
    "                        \"border_count\": 92,\n",
    "                        \"verbose\": 500,\n",
    "                        \"subsample\": 0.7641850606486046,\n",
    "                        'early_stopping_rounds': 100,\n",
    "                        'random_state': 42, \n",
    "                    },\n",
    "              'B': {\n",
    "                        \"iterations\": 5000,\n",
    "                        \"learning_rate\": 0.037511244177544326,\n",
    "                        \"depth\": 6,\n",
    "                        \"l2_leaf_reg\": 5,\n",
    "                        \"loss_function\": \"MAE\",\n",
    "                        \"border_count\": 128,\n",
    "                        \"verbose\": 500,\n",
    "                        \"subsample\": 0.8012204629505595,\n",
    "                        'early_stopping_rounds': 100,\n",
    "                        'random_state': 42, \n",
    "                    },\n",
    "              'C': {\"iterations\": 5000, \n",
    "                    \"learning_rate\": 0.03425599789981457,\n",
    "                    \"depth\": 8,\n",
    "                    \"l2_leaf_reg\": 4,\n",
    "                    \"loss_function\": \"MAE\", \n",
    "                    \"border_count\": 218, \n",
    "                    \"verbose\": 500, \n",
    "                    \"subsample\": 0.6848272280307022, \n",
    "                    'early_stopping_rounds': 100,\n",
    "                    'random_state': 42, }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CATegorical(df):\n",
    "    df['dew_or_rime:idx'] = df['dew_or_rime:idx'].astype(int)\n",
    "    df['is_in_shadow:idx'] = df['is_in_shadow:idx'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat = []\n",
    "\n",
    "from sklearn.impute import IterativeImputer\n",
    "# Cat_1 training and predictions\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train_cat, X_test_cat, is_day_feature1 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated,mode = 'cat')\n",
    "    X_train_cat.drop(columns=['date_forecast'], inplace=True)\n",
    "    print(f'Doing... {loc}')\n",
    "\n",
    "    imputer = IterativeImputer(max_iter=5, random_state=42)\n",
    "    for col in X_train_cat.columns:\n",
    "        X_train_cat[col] = imputer.fit_transform(np.array(X_train_cat[col]).reshape(-1,1))\n",
    "    for col in X_test_cat.columns:\n",
    "        X_test_cat[col] = imputer.fit_transform(np.array(X_test_cat[col]).reshape(-1,1))\n",
    "    \n",
    "    targets_cat = pd.DataFrame( {'pv_measurement': X_train_cat['pv_measurement']})\n",
    "    X_train_cat = X_train_cat.drop(columns=['pv_measurement'])\n",
    "    X_train_cat = CATegorical(X_train_cat)\n",
    "    X_test_cat = CATegorical(X_test_cat)\n",
    "\n",
    "    model_cat = CatBoostRegressor(**cat_params[loc])\n",
    "\n",
    "    X_train_cat1, X_val_cat1, y_train_cat1, y_val_cat1 = train_test_split(X_train_cat, targets_cat, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Training\n",
    "    model_cat.fit(X_train_cat1, y_train_cat1['pv_measurement'],eval_set=(X_val_cat1, y_val_cat1['pv_measurement']),)\n",
    "\n",
    "    # Prediction\n",
    "    predictions_cat = model_cat.predict(X_test_cat[model_cat.feature_names_])\n",
    "    \n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat = predictions_cat * is_day_feature1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat = np.clip(adjusted_final_predictions_cat, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat.append(adjusted_final_predictions_cat)\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_cat = np.array(all_predictions_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat_2 = []\n",
    "\n",
    "# Catboost nr 3\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    X_train_3, X_test_3, is_day_feature_3 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    X_train_3.drop(columns=['date_forecast'], inplace=True)\n",
    "    imputer = IterativeImputer(max_iter=5, random_state=42)\n",
    "    for col in X_train_3.columns:\n",
    "        X_train_3[col] = imputer.fit_transform(np.array(X_train_3[col]).reshape(-1,1))\n",
    "    for col in X_test_3.columns:\n",
    "        X_test_3[col] = imputer.fit_transform(np.array(X_test_3[col]).reshape(-1,1))\n",
    "    targets_3 = pd.DataFrame( {'pv_measurement': X_train_3['pv_measurement']})\n",
    "    X_train_3 = X_train_3.drop(columns=['pv_measurement'])\n",
    "    \n",
    "    model_cat_3 = CatBoostRegressor(**cat_params_no_feature[loc])\n",
    "    X_train_3 = feature_engineering(X_train_3)\n",
    "    X_test_3 = feature_engineering(X_test_3)\n",
    "\n",
    "    # Create 'sin_sun_azimuth' and 'cos_sun_azimuth' from 'sun_azimuth' in radians\n",
    "    X_train_3['sin_sun_azimuth'] = np.sin(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_train_3['cos_sun_azimuth'] = np.cos(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_test_3['sin_sun_azimuth'] = np.sin(np.radians(X_test_3['sun_azimuth:d']))\n",
    "    X_test_3['cos_sun_azimuth'] = np.cos(np.radians(X_test_3['sun_azimuth:d']))\n",
    "\n",
    "    # Now drop the original 'sun_azimuth' feature\n",
    "    X_train_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    X_test_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train_cat_3, X_test_cat_3, y_train_cat_3, y_test_cat_3 = train_test_split(X_train_3, targets_3, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model_cat_3.fit(X_train_cat_3, y_train_cat_3['pv_measurement'],eval_set=(X_test_cat_3, y_test_cat_3['pv_measurement']),)  \n",
    "    \n",
    "    # Pred\n",
    "    pred_cat_2 = model_cat_3.predict(X_test_3[model_cat_3.feature_names_])\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat_2 = pred_cat_2 * is_day_feature_3['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat_2 = np.clip(adjusted_final_predictions_cat_2, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat_2.append(adjusted_final_predictions_cat_2) \n",
    "\n",
    "# Changing final list to array   \n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat_3 = []\n",
    "# Catboost nr 3\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    X_train_3, X_test_3, is_day_feature_3 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated,mode = 'catboost')\n",
    "    X_train_3.drop(columns=['date_forecast'], inplace=True)\n",
    "    imputer = IterativeImputer(max_iter=5, random_state=42)\n",
    "    for col in X_train_3.columns:\n",
    "        X_train_3[col] = imputer.fit_transform(np.array(X_train_3[col]).reshape(-1,1))\n",
    "    for col in X_test_3.columns:\n",
    "        X_test_3[col] = imputer.fit_transform(np.array(X_test_3[col]).reshape(-1,1))\n",
    "    targets_3 = pd.DataFrame( {'pv_measurement': X_train_3['pv_measurement']})\n",
    "    X_train_3 = X_train_3.drop(columns=['pv_measurement'])\n",
    "    \n",
    "    model_cat_3 = CatBoostRegressor(**cat_params_no_feature[loc])\n",
    "   # X_train_3 = feature_engineering(X_train_3)\n",
    "   # X_test_3 = feature_engineering(X_test_3)\n",
    "\n",
    "    # Create 'sin_sun_azimuth' and 'cos_sun_azimuth' from 'sun_azimuth' in radians\n",
    "    X_train_3['sin_sun_azimuth'] = np.sin(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_train_3['cos_sun_azimuth'] = np.cos(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_test_3['sin_sun_azimuth'] = np.sin(np.radians(X_test_3['sun_azimuth:d']))\n",
    "    X_test_3['cos_sun_azimuth'] = np.cos(np.radians(X_test_3['sun_azimuth:d']))\n",
    "\n",
    "    # Now drop the original 'sun_azimuth' feature\n",
    "    X_train_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    X_test_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train_cat_3, X_test_cat_3, y_train_cat_3, y_test_cat_3 = train_test_split(X_train_3, targets_3, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model_cat_3.fit(X_train_cat_3, y_train_cat_3['pv_measurement'],eval_set=(X_test_cat_3, y_test_cat_3['pv_measurement']),)  \n",
    "    \n",
    "    # Pred\n",
    "    pred_cat_3 = model_cat_3.predict(X_test_3[model_cat_3.feature_names_])\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat_3 = pred_cat_3 * is_day_feature_3['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat_3 = np.clip(adjusted_final_predictions_cat_3, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat_3.append(adjusted_final_predictions_cat_3) \n",
    "\n",
    "# Changing final list to array   \n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with some extra features\n",
    "def process_location_xgb(X, y, location_name,seeds):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('xgboost')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm, optimize='MAE',early_stopping=True,early_stopping_max_iters=100)\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MAE        MSE     RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                     \n",
      "0     41.3532  5063.6689  71.1595  0.8991  0.7087  0.5641\n",
      "1     44.4771  5887.4707  76.7299  0.8792  0.6708  0.6331\n",
      "2     44.0571  5441.9141  73.7693  0.8788  0.7011  0.4971\n",
      "3     37.6023  3837.0566  61.9440  0.9193  0.7106  0.5198\n",
      "4     43.9118  5673.2422  75.3209  0.8809  0.6758  0.5680\n",
      "5     41.9584  5037.9727  70.9787  0.8993  0.6519  0.5430\n",
      "6     43.9647  5516.7354  74.2747  0.8823  0.7940  0.6082\n",
      "7     45.0832  6017.8247  77.5746  0.8631  0.7907  0.8307\n",
      "8     44.8390  5954.8374  77.1676  0.8744  0.7082  0.6193\n",
      "9     41.4639  5491.3062  74.1033  0.8680  0.6624  0.5832\n",
      "Mean  42.8711  5392.2029  73.3023  0.8844  0.7074  0.5967\n",
      "Std    2.1888   609.2559   4.3569  0.0160  0.0468  0.0880\n",
      "Transformation Pipeline and Model Successfully Saved\n",
      "                        Description        Value\n",
      "0                        Session id          123\n",
      "1                            Target       target\n",
      "2                       Target type   Regression\n",
      "3               Original data shape  (11023, 56)\n",
      "4            Transformed data shape  (11023, 68)\n",
      "5       Transformed train set shape   (7716, 68)\n",
      "6        Transformed test set shape   (3307, 68)\n",
      "7                  Ordinal features            1\n",
      "8                  Numeric features           52\n",
      "9              Categorical features            3\n",
      "10         Rows with missing values        23.0%\n",
      "11                       Preprocess         True\n",
      "12                  Imputation type    iterative\n",
      "13  Iterative imputation iterations            5\n",
      "14        Numeric iterative imputer     lightgbm\n",
      "15    Categorical iterative imputer     lightgbm\n",
      "16         Maximum one-hot encoding           25\n",
      "17                  Encoding method         None\n",
      "18                   Fold Generator        KFold\n",
      "19                      Fold Number           10\n",
      "20                         CPU Jobs           -1\n",
      "21                          Use GPU        False\n",
      "22                   Log Experiment        False\n",
      "23                  Experiment Name        exp_C\n",
      "24                              USI         f748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MAE        MSE     RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                     \n",
      "0     45.2625  5623.4331  74.9896  0.8875  0.7610  0.6306\n",
      "1     41.4932  5022.3350  70.8684  0.8924  0.7472  0.6514\n",
      "2     43.4137  6220.1274  78.8678  0.8504  0.7258  0.6338\n",
      "3     41.7869  5195.7363  72.0815  0.8860  0.7950  0.7583\n",
      "4     47.6616  6776.6821  82.3206  0.8512  0.7446  0.5330\n",
      "5     45.1653  6171.8315  78.5610  0.8727  0.6967  0.5457\n",
      "6     42.8435  5246.7451  72.4344  0.8777  0.7740  0.6210\n",
      "7     46.8483  5839.6104  76.4173  0.8667  0.7823  0.6272\n",
      "8     40.1953  4660.3511  68.2668  0.8978  0.7559  0.6395\n",
      "9     48.1545  6512.3594  80.6992  0.8660  0.6914  0.5267\n",
      "Mean  44.2825  5726.9211  75.5507  0.8748  0.7474  0.6167\n",
      "Std    2.6129   658.0469   4.3611  0.0156  0.0326  0.0652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MAE        MSE     RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                     \n",
      "0     44.3402  5385.5034  73.3860  0.8922  0.7274  0.5496\n",
      "1     39.4099  4482.0000  66.9477  0.9040  0.6871  0.6184\n",
      "2     43.6661  5939.7202  77.0696  0.8571  0.7301  0.6621\n",
      "3     39.1597  4675.4556  68.3773  0.8974  0.7233  0.6315\n",
      "4     45.9394  6408.5015  80.0531  0.8593  0.6615  0.5366\n",
      "5     45.2852  5876.8018  76.6603  0.8788  0.7507  0.5685\n",
      "6     41.9569  4995.2900  70.6774  0.8835  0.7594  0.6181\n",
      "7     45.6832  5625.2056  75.0014  0.8716  0.7521  0.5755\n",
      "8     40.6453  4804.5625  69.3149  0.8946  0.7304  0.6872\n",
      "9     47.9766  6483.3159  80.5190  0.8666  0.6420  0.5049\n",
      "Mean  43.4063  5467.6356  73.8007  0.8805  0.7164  0.5952\n",
      "Std    2.8406   678.2693   4.5930  0.0157  0.0378  0.0549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MAE        MSE     RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                     \n",
      "0     42.9893  5137.0322  71.6731  0.8972  0.6793  0.5666\n",
      "1     39.6269  4606.0547  67.8679  0.9013  0.6428  0.5673\n",
      "2     42.1229  5548.8276  74.4905  0.8665  0.6709  0.6071\n",
      "3     40.0424  4889.0815  69.9220  0.8927  0.6926  0.7584\n",
      "4     45.0504  6048.3354  77.7710  0.8672  0.6112  0.5108\n",
      "5     43.9826  5680.3647  75.3682  0.8828  0.6765  0.6499\n",
      "6     41.1172  4932.3711  70.2308  0.8850  0.7050  0.6045\n",
      "7     42.4068  5038.0610  70.9793  0.8850  0.6994  0.5835\n",
      "8     39.2616  4416.5195  66.4569  0.9031  0.6671  0.6324\n",
      "9     46.4418  6021.0352  77.5953  0.8761  0.6038  0.4813\n",
      "Mean  42.3042  5231.7683  72.2355  0.8857  0.6649  0.5962\n",
      "Std    2.2570   538.6286   3.7149  0.0124  0.0333  0.0728\n",
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    }
   ],
   "source": [
    "# LightGBM training and predictions\n",
    "all_predictions_xb = []\n",
    "all_predictions_xb_e =[]\n",
    "\n",
    "for loc in locations:\n",
    "\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Calling preprocessing\n",
    "    train, test, is_day_feature = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "   \n",
    "    targets = pd.DataFrame( {'pv_measurement': train['pv_measurement']})\n",
    "    X_train = train.drop(columns=['date_forecast','pv_measurement'])\n",
    "    X_train = feature_engineering(X_train)\n",
    "    X_test = test.drop(columns=['date_forecast'])\n",
    "    X_test = feature_engineering(X_test)\n",
    "    \n",
    "    # Training and prediction for diffrent seeds\n",
    "    total_predictions_light = None\n",
    "    seeds = [42]\n",
    "    for seed in seeds: \n",
    "        final_model_lGBM_e = process_location_xgb(X_train, targets, loc, seed)\n",
    "        predictions_lGBM_e = predict_model(final_model_lGBM_e, data=X_test)\n",
    "        final_predictions_lGBM_e = predictions_lGBM_e['prediction_label']\n",
    "        if total_predictions_light is None:\n",
    "            total_predictions_light = np.zeros_like(final_predictions_lGBM_e)\n",
    "        total_predictions_light += final_predictions_lGBM_e\n",
    "\n",
    "    mean_pred_light = total_predictions_light/len(seeds)\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    all_predictions_xb = mean_pred_light * is_day_feature['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    all_predictions_xb = np.clip(all_predictions_xb, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_xb_e.append([all_predictions_xb])\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_xb_e = np.array(all_predictions_xb_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2160,) (720,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\claxl\\Documents\\GitHub\\MLProject\\Definitiva\\145ISA.ipynb Cella 23\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m all_predictions_cat_3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(all_predictions_cat_3)\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m all_predictions_xb \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(all_predictions_xb)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m all_pred \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39;49m\u001b[39m*\u001b[39;49mall_predictions_cat\u001b[39m+\u001b[39;49m\u001b[39m0.2\u001b[39;49m \u001b[39m*\u001b[39;49m all_predictions_lGBM_e\u001b[39m+\u001b[39;49m\u001b[39m0.2\u001b[39;49m\u001b[39m*\u001b[39;49mall_predictions_cat_2 \u001b[39m+\u001b[39;49m \u001b[39m0.2\u001b[39;49m\u001b[39m*\u001b[39;49mall_predictions_cat_3 \u001b[39m+\u001b[39;49m \u001b[39m0.2\u001b[39;49m\u001b[39m*\u001b[39;49mall_predictions_xb\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(all_pred\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2160,) (720,) "
     ]
    }
   ],
   "source": [
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e).flatten()\n",
    "all_predictions_cat = np.array(all_predictions_cat).flatten()\n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2).flatten()\n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3).flatten()\n",
    "all_predictions_xb_e = np.array(all_predictions_xb_e).flatten()\n",
    "all_pred = 0.2*all_predictions_cat+0.2 * all_predictions_lGBM_e+0.2*all_predictions_cat_2 + 0.2*all_predictions_cat_3 + 0.2*all_predictions_xb_e\n",
    "print(all_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = all_pred\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('best_score1.csv')\n",
    "df2 = pd.read_csv('best_score2.csv')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot delle differenze tra df e df1\n",
    "plt.plot(df1['prediction'] - df['prediction'])\n",
    "plt.title('Differenze tra df1 e df')\n",
    "plt.xlabel('Indice')\n",
    "plt.ylabel('Differenza')\n",
    "plt.show()\n",
    "\n",
    "# plot delle differenze tra df e df2\n",
    "plt.plot(df2['prediction'] - df['prediction'])\n",
    "plt.title('Differenze tra df2 e df')\n",
    "plt.xlabel('Indice')\n",
    "plt.ylabel('Differenza')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-name",
   "language": "python",
   "name": "pycaret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
