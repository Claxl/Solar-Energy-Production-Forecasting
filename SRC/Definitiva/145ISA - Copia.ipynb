{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycaret.regression import *\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD TIME FEATURES\n",
    "def add_time_features(df, time_column, mode = 'lgbm'):\n",
    "    '''\n",
    "        This function will add some time feature based on the param 'time_columns'\n",
    "        \n",
    "        Params:\n",
    "            df-> Dataframe with the column contained in 'time_column'\n",
    "            time_column -> the column that is a datetime object\n",
    "        \n",
    "        Returns:\n",
    "            A dataframe with time features\n",
    "    '''\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Make sure the time column is in datetime format\n",
    "    if mode == 'lgbm':\n",
    "        df['hour'] = df[time_column].dt.hour\n",
    "        df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "        df['month'] = df[time_column].dt.month\n",
    "        df['day_of_year'] = df[time_column].dt.dayofyear\n",
    "        df['week_of_year'] = df[time_column].dt.isocalendar().week\n",
    "        df['year'] = df[time_column].dt.year\n",
    "    elif mode == 'cat_boost':\n",
    "        df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "        df['sin_hour'] = np.sin(2*np.pi * df[time_column].dt.hour/23.)\n",
    "        df['sin_month'] = np.sin(2*np.pi * df[time_column].dt.month/12.)\n",
    "        df['cos_hour'] = np.cos(2*np.pi * df[time_column].dt.hour/23.)\n",
    "        df['cos_month'] = np.cos(2*np.pi * df[time_column].dt.month/12.)\n",
    "    elif mode == 'cat':\n",
    "        df['sin_hour'] = np.sin(2*np.pi * df[time_column].dt.hour/23.)\n",
    "        df['sin_month'] = np.sin(2*np.pi * df[time_column].dt.month/12.)   \n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(df,column):\n",
    "    '''\n",
    "        Make the column in datetime format\n",
    "    '''\n",
    "    return pd.to_datetime(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(df,column):\n",
    "    '''\n",
    "        Resample df to 1 hour using mean() as aggregator and drop rows where all columns are NaN\n",
    "        \n",
    "        Params :\n",
    "            df -> the dataframe to be resampled\n",
    "            column -> the time column\n",
    "    '''\n",
    "    return df.set_index(keys = column).resample('1H').mean().dropna(how='all').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df,columnlist):\n",
    "    return df.drop(columns = columnlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_calc(df):\n",
    "    '''\n",
    "    This function create a dataframe with 'date_forecast' as index and the column 'date_calc' resampled by '1H'.\n",
    "    If there's no data in a specific bin, the resulting value for that bin would be NaN (not a number).\n",
    "    Params:\n",
    "        df -> dataframe with 'date_forecast' and 'date_calc' columns.\n",
    "            'date_calc' is expected to contain data that the user wants to resample or analyze.\n",
    "    Returns:\n",
    "        A dataframe with 'date_calc' resampled.\n",
    "    '''\n",
    "    return df.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_estimated_feature(df):\n",
    "    '''\n",
    "        This function will create some time feature and estimated information. It's need to let the model understand is\n",
    "        estimated value.\n",
    "        Params:\n",
    "            df -> It MUST be an estimated dataframe, that contains 'data_forecast' as datetime type\n",
    "        Returns:\n",
    "            A dataframe with 'time_dummy', 'time_delta' and 'is_estimated'     \n",
    "    '''\n",
    "    df['time_delta'] = (df['date_calc'] - df['date_forecast']).dt.total_seconds() / 3600\n",
    "    df['is_estimated'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stationarity(df):\n",
    "    '''\n",
    "    Removes constant stretches of data within a DataFrame where the 'pv_measurement' column does not change.    \n",
    "    The function identifies blocks of data where the 'pv_measurement' stays constant for more than two consecutive\n",
    "    points and removes these blocks to address data stationarity.\n",
    "\n",
    "    params:\n",
    "        df -> DataFrame\n",
    "              A pandas DataFrame with a 'pv_measurement' column which contains the data from which to remove stationarity.\n",
    "        \n",
    "    return:\n",
    "        The DataFrame with constant stretches of data removed from the 'pv_measurement' column.\n",
    "    '''\n",
    "    \n",
    "    #Calculate the difference, this need for check the constant\n",
    "    df['diff'] = df['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    # Create an indicator for constant stretches\n",
    "    df['constant'] = (df['diff'] == 0).astype(int)\n",
    "\n",
    "    # Use the indicator to mark stretches. The diff() function here identifies change-points.\n",
    "    df['block'] = (df['constant'].diff() != 0).astype(int).cumsum()\n",
    "\n",
    "    # Get the size of each constant block\n",
    "    block_sizes = df.groupby('block')['constant'].sum()\n",
    "\n",
    "    # Identify blocks that are constant for more than N consecutive time points (in this case 2)\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "    \n",
    "    # Remove the constant\n",
    "    filtered_df = df[~df['block'].isin(constant_blocks)]\n",
    "        \n",
    "    return filtered_df.drop(columns=['diff', 'constant', 'block'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(targets, observed, estimated, test, mode: str = 'lgbm'):\n",
    "    '''\n",
    "        This function makes all the preprocessing needed for the correct run of the model, it will perform:\n",
    "            - Resampling\n",
    "            - Filtering\n",
    "            - Imputation\n",
    "            - Outliers removal\n",
    "            - Categorical Encoding\n",
    "        \n",
    "        Params:\n",
    "            targets -> dataframe of the target parquet\n",
    "            observed -> dataframe of observed train data\n",
    "            estimated -> dataframe of estimated train data\n",
    "            test -> dataframe of test data\n",
    "        Returns:\n",
    "            train_data -> dataframe of all data ready to train\n",
    "            test_data -> dataframe of all data ready to test\n",
    "            is_day -> dataframe of is_day categorical feature for post processing\n",
    "    \n",
    "    '''    \n",
    "    targets['time'] = to_datetime(targets,'time')\n",
    "    estimated['date_forecast'] = to_datetime(estimated,'date_forecast')\n",
    "    observed['date_forecast'] = to_datetime(observed,'date_forecast')\n",
    "    test['date_forecast'] = to_datetime(test,'date_forecast')\n",
    "\n",
    "    observed_resampled = resampling(observed,'date_forecast')\n",
    "    estimated_resampled = resampling(estimated,'date_forecast')\n",
    "    test_resampled = resampling(test,'date_forecast')\n",
    "    \n",
    "    date_calc_resampled_observed = extract_data_calc(estimated)\n",
    "    date_calc_resampled_test = extract_data_calc(test)\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_observed, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_test, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    is_day = test_resampled[['date_forecast', 'is_day:idx']]   \n",
    "    test_resampled = filter_df(test_resampled,['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = filter_df(observed_resampled,['is_day:idx', 'snow_density:kgm3','elevation:m']) \n",
    "    estimated_resampled = filter_df(estimated_resampled,[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    \n",
    "    #This MUST be zero because is not estimated.\n",
    "    observed_resampled['is_estimated'] = 0\n",
    "    observed_resampled['time_delta'] = 0\n",
    "    \n",
    "    estimated_resampled = is_estimated_feature(estimated_resampled)\n",
    "    test_resampled = is_estimated_feature(test_resampled)\n",
    "    \n",
    "    X = pd.concat([observed_resampled,estimated_resampled],axis = 0)\n",
    "    train_data = pd.merge(targets, X, how='inner', left_on='time', right_on='date_forecast')\n",
    "    if mode == 'lgbm':\n",
    "        train_data = add_time_features(train_data, 'time')\n",
    "        test_data = add_time_features(test_resampled, 'date_forecast')\n",
    "    elif mode == 'cat':\n",
    "        train_data = add_time_features(train_data, 'time', mode = 'cat')\n",
    "        test_data = add_time_features(test_resampled, 'date_forecast', mode = 'cat')\n",
    "        train_data = train_data[train_data['date_forecast'].dt.month.isin([4,5,6,7,8])]\n",
    "        test_data = test_data[test_data['date_forecast'].dt.month.isin([4,5,6,7,8])]\n",
    "    elif mode == 'cat_boost':\n",
    "        train_data = add_time_features(train_data, 'time', mode = 'cat_boost')\n",
    "        test_data = add_time_features(test_resampled, 'date_forecast', mode = 'cat_boost')\n",
    "\n",
    "    \n",
    "    train_data = delete_stationarity(train_data)\n",
    "    \n",
    "    train_data = filter_df(train_data, ['time','date_calc'])\n",
    "    test_data = filter_df(test_resampled, ['date_calc'])\n",
    "\n",
    "    return train_data, test_data, is_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with some extra features\n",
    "def process_location(X, y, location_name,seeds):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm ,optimize='MAE',n_iter = 100,early_stopping=True,early_stopping_max_iters=100)\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    # Added some extra features to this one model, did it here so we could reuse the same preprocesssing function on diffrent models\n",
    "    # Feature Combination 1: Solar Radiation and Cloud Cover Combination\n",
    "    data['radcloud'] = ((data['direct_rad:W'] * (1 - data['total_cloud_cover:p']/100)) +\n",
    "                        (data['diffuse_rad:W'] * (data['total_cloud_cover:p']/100)))\n",
    "\n",
    "    # Feature Combination 2: Atmospheric Conditions Combination\n",
    "    data['adjusted_clear_sky_rad'] = (data['clear_sky_rad:W'] * np.exp(-0.0001 * data['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (data['air_density_2m:kgm3'] - 1.225)))  # Adjusted based on humidity and air density\n",
    "    data['solar_incidence_factor'] = np.cos(np.radians(90 - data['sun_elevation:d'])) * np.cos(np.radians(data['sun_azimuth:d']))\n",
    "    data['seasonal_conversion_efficiency'] = data['radcloud'] * (1 - data['relative_humidity_1000hPa:p']/100) * (data['msl_pressure:hPa'] / 1013.25)\n",
    "    \n",
    "    data['humidity_cloud_radiation_index'] = (data['direct_rad:W'] * (1 - data['absolute_humidity_2m:gm3']/100)) * (1 - data['total_cloud_cover:p']/100)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regex(df):\n",
    "    '''\n",
    "        This function let lgbm work, this because it cannot accept ':'\n",
    "    '''\n",
    "    return df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_-]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global lists to save predictions in\n",
    "locations = ['A', 'B', 'C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Description        Value\n",
      "0                        Session id           42\n",
      "1                            Target       target\n",
      "2                       Target type   Regression\n",
      "3               Original data shape  (19622, 56)\n",
      "4            Transformed data shape  (19622, 68)\n",
      "5       Transformed train set shape  (13735, 68)\n",
      "6        Transformed test set shape   (5887, 68)\n",
      "7                  Ordinal features            1\n",
      "8                  Numeric features           52\n",
      "9              Categorical features            3\n",
      "10         Rows with missing values        20.2%\n",
      "11                       Preprocess         True\n",
      "12                  Imputation type    iterative\n",
      "13  Iterative imputation iterations            5\n",
      "14        Numeric iterative imputer     lightgbm\n",
      "15    Categorical iterative imputer     lightgbm\n",
      "16         Maximum one-hot encoding           25\n",
      "17                  Encoding method         None\n",
      "18                   Fold Generator        KFold\n",
      "19                      Fold Number           10\n",
      "20                         CPU Jobs           -1\n",
      "21                          Use GPU        False\n",
      "22                   Log Experiment        False\n",
      "23                  Experiment Name        exp_A\n",
      "24                              USI         817d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           MAE          MSE      RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                         \n",
      "0     305.4166  268394.3057  518.0679  0.8637  1.0436  1.5201\n",
      "1     303.6288  263868.1251  513.6810  0.8532  1.1167  2.4717\n",
      "2     311.4408  277964.0180  527.2229  0.8542  1.0960  2.9018\n",
      "3     325.6151  298803.8543  546.6295  0.8478  1.0584  1.8151\n",
      "4     307.5917  245188.9140  495.1655  0.8701  1.0879  2.8423\n",
      "5     301.0274  270695.6740  520.2842  0.8568  1.1853  2.8307\n",
      "6     309.0800  273860.0814  523.3164  0.8367  1.0922  3.4744\n",
      "7     305.5958  264913.3114  514.6973  0.8607  1.1037  3.1679\n",
      "8     313.7290  272511.2623  522.0261  0.8534  1.0982  2.3937\n",
      "9     309.2977  287990.4250  536.6474  0.8420  1.0385  2.0300\n",
      "Mean  309.2423  272418.9971  521.7738  0.8539  1.0920  2.5448\n",
      "Std     6.5017   13643.9023   13.0793  0.0094  0.0398  0.5853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\claxl\\Documents\\GitHub\\MLProject\\Definitiva\\145ISA - Copia.ipynb Cella 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA%20-%20Copia.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m seeds \u001b[39m=\u001b[39m [\u001b[39m42\u001b[39m,\u001b[39m123\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA%20-%20Copia.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m seed \u001b[39min\u001b[39;00m seeds: \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA%20-%20Copia.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     final_model_lGBM_e \u001b[39m=\u001b[39m process_location(X_train, targets, loc, seed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA%20-%20Copia.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     predictions_lGBM_e \u001b[39m=\u001b[39m predict_model(final_model_lGBM_e, data\u001b[39m=\u001b[39mX_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA%20-%20Copia.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     final_predictions_lGBM_e \u001b[39m=\u001b[39m predictions_lGBM_e[\u001b[39m'\u001b[39m\u001b[39mprediction_label\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;32mc:\\Users\\claxl\\Documents\\GitHub\\MLProject\\Definitiva\\145ISA - Copia.ipynb Cella 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA%20-%20Copia.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m lightgbm \u001b[39m=\u001b[39m create_model(\u001b[39m'\u001b[39m\u001b[39mlightgbm\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA%20-%20Copia.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Tune the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA%20-%20Copia.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m tuned_lightgbm \u001b[39m=\u001b[39m tune_model(lightgbm ,optimize\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMAE\u001b[39;49m\u001b[39m'\u001b[39;49m,n_iter \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,early_stopping_max_iters\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA%20-%20Copia.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Create a bagged version of the tuned model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/claxl/Documents/GitHub/MLProject/Definitiva/145ISA%20-%20Copia.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m bagged_lightgbm \u001b[39m=\u001b[39m ensemble_model(tuned_lightgbm, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBagging\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\pycaret\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m globals_d[name] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 965\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\pycaret\\regression\\functional.py:1197\u001b[0m, in \u001b[0;36mtune_model\u001b[1;34m(estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[39m@check_if_global_is_not_none\u001b[39m(\u001b[39mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m   1006\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_model\u001b[39m(\n\u001b[0;32m   1007\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1026\u001b[0m ):\n\u001b[0;32m   1027\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[39m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[39m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \n\u001b[0;32m   1195\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     \u001b[39mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[39m.\u001b[39;49mtune_model(\n\u001b[0;32m   1198\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1199\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[0;32m   1200\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[0;32m   1201\u001b[0m         n_iter\u001b[39m=\u001b[39;49mn_iter,\n\u001b[0;32m   1202\u001b[0m         custom_grid\u001b[39m=\u001b[39;49mcustom_grid,\n\u001b[0;32m   1203\u001b[0m         optimize\u001b[39m=\u001b[39;49moptimize,\n\u001b[0;32m   1204\u001b[0m         custom_scorer\u001b[39m=\u001b[39;49mcustom_scorer,\n\u001b[0;32m   1205\u001b[0m         search_library\u001b[39m=\u001b[39;49msearch_library,\n\u001b[0;32m   1206\u001b[0m         search_algorithm\u001b[39m=\u001b[39;49msearch_algorithm,\n\u001b[0;32m   1207\u001b[0m         early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[0;32m   1208\u001b[0m         early_stopping_max_iters\u001b[39m=\u001b[39;49mearly_stopping_max_iters,\n\u001b[0;32m   1209\u001b[0m         choose_better\u001b[39m=\u001b[39;49mchoose_better,\n\u001b[0;32m   1210\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m   1211\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m   1212\u001b[0m         return_tuner\u001b[39m=\u001b[39;49mreturn_tuner,\n\u001b[0;32m   1213\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1214\u001b[0m         tuner_verbose\u001b[39m=\u001b[39;49mtuner_verbose,\n\u001b[0;32m   1215\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m   1216\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1217\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\pycaret\\regression\\oop.py:1499\u001b[0m, in \u001b[0;36mRegressionExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_model\u001b[39m(\n\u001b[0;32m   1308\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1309\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1328\u001b[0m ):\n\u001b[0;32m   1329\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \n\u001b[0;32m   1497\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1499\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtune_model(\n\u001b[0;32m   1500\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1501\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[0;32m   1502\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[0;32m   1503\u001b[0m         n_iter\u001b[39m=\u001b[39;49mn_iter,\n\u001b[0;32m   1504\u001b[0m         custom_grid\u001b[39m=\u001b[39;49mcustom_grid,\n\u001b[0;32m   1505\u001b[0m         optimize\u001b[39m=\u001b[39;49moptimize,\n\u001b[0;32m   1506\u001b[0m         custom_scorer\u001b[39m=\u001b[39;49mcustom_scorer,\n\u001b[0;32m   1507\u001b[0m         search_library\u001b[39m=\u001b[39;49msearch_library,\n\u001b[0;32m   1508\u001b[0m         search_algorithm\u001b[39m=\u001b[39;49msearch_algorithm,\n\u001b[0;32m   1509\u001b[0m         early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[0;32m   1510\u001b[0m         early_stopping_max_iters\u001b[39m=\u001b[39;49mearly_stopping_max_iters,\n\u001b[0;32m   1511\u001b[0m         choose_better\u001b[39m=\u001b[39;49mchoose_better,\n\u001b[0;32m   1512\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[0;32m   1513\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m   1514\u001b[0m         return_tuner\u001b[39m=\u001b[39;49mreturn_tuner,\n\u001b[0;32m   1515\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1516\u001b[0m         tuner_verbose\u001b[39m=\u001b[39;49mtuner_verbose,\n\u001b[0;32m   1517\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m   1518\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1519\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:2665\u001b[0m, in \u001b[0;36m_SupervisedExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   2657\u001b[0m     \u001b[39mwith\u001b[39;00m patch(\n\u001b[0;32m   2658\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._search.sample_without_replacement\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2659\u001b[0m         pycaret\u001b[39m.\u001b[39minternal\u001b[39m.\u001b[39mpatches\u001b[39m.\u001b[39msklearn\u001b[39m.\u001b[39m_mp_sample_without_replacement,\n\u001b[0;32m   2660\u001b[0m     ):\n\u001b[0;32m   2661\u001b[0m         \u001b[39mwith\u001b[39;00m patch(\n\u001b[0;32m   2662\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._search.ParameterGrid.__getitem__\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2663\u001b[0m             pycaret\u001b[39m.\u001b[39minternal\u001b[39m.\u001b[39mpatches\u001b[39m.\u001b[39msklearn\u001b[39m.\u001b[39m_mp_ParameterGrid_getitem,\n\u001b[0;32m   2664\u001b[0m         ):\n\u001b[1;32m-> 2665\u001b[0m             model_grid\u001b[39m.\u001b[39;49mfit(data_X, data_y, groups\u001b[39m=\u001b[39;49mgroups, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[0;32m   2666\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2667\u001b[0m     model_grid\u001b[39m.\u001b[39mfit(data_X, data_y, groups\u001b[39m=\u001b[39mgroups, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1769\u001b[0m         ParameterSampler(\n\u001b[0;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1771\u001b[0m         )\n\u001b[0;32m   1772\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\claxl\\anaconda3\\envs\\pycaret\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LightGBM training and predictions\n",
    "all_predictions_lGBM_e = []\n",
    "for loc in locations:\n",
    "\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Calling preprocessing\n",
    "    train, test, is_day_feature = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "   \n",
    "    targets = pd.DataFrame( {'pv_measurement': train['pv_measurement']})\n",
    "    X_train = train.drop(columns=['date_forecast','pv_measurement'])\n",
    "    X_train = feature_engineering(X_train)\n",
    "    X_test = test.drop(columns=['date_forecast'])\n",
    "    X_test = feature_engineering(X_test)\n",
    "    \n",
    "    # Training and prediction for diffrent seeds\n",
    "    total_predictions_light = None\n",
    "    seeds = [42,123]\n",
    "    for seed in seeds: \n",
    "        final_model_lGBM_e = process_location(X_train, targets, loc, seed)\n",
    "        predictions_lGBM_e = predict_model(final_model_lGBM_e, data=X_test)\n",
    "        final_predictions_lGBM_e = predictions_lGBM_e['prediction_label']\n",
    "        if total_predictions_light is None:\n",
    "            total_predictions_light = np.zeros_like(final_predictions_lGBM_e)\n",
    "        total_predictions_light += final_predictions_lGBM_e\n",
    "\n",
    "    mean_pred_light = total_predictions_light/len(seeds)\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_lGBM_e = mean_pred_light * is_day_feature['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_lGBM_e = np.clip(adjusted_final_predictions_lGBM_e, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_lGBM_e.append([adjusted_final_predictions_lGBM_e])\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['dew_or_rime:idx' ,'is_in_shadow:idx']\n",
    "cat_params = { 'A': {\n",
    "                        \"iterations\": 5000,\n",
    "                        \"learning_rate\": 0.034867396508006264,\n",
    "                        \"depth\": 8,\n",
    "                        \"l2_leaf_reg\": 1,\n",
    "                        \"loss_function\": \"MAE\",\n",
    "                        \"border_count\": 92,\n",
    "                        \"verbose\": 500,\n",
    "                        \"subsample\": 0.7641850606486046,\n",
    "                        'early_stopping_rounds': 100,\n",
    "                        'cat_features': cat_features,\n",
    "                        'random_state': 42, \n",
    "                    },\n",
    "              'B': {\n",
    "                        \"iterations\": 5000,\n",
    "                        \"learning_rate\": 0.037511244177544326,\n",
    "                        \"depth\": 6,\n",
    "                        \"l2_leaf_reg\": 5,\n",
    "                        \"loss_function\": \"MAE\",\n",
    "                        \"border_count\": 128,\n",
    "                        \"verbose\": 500,\n",
    "                        \"subsample\": 0.8012204629505595,\n",
    "                        'early_stopping_rounds': 100,\n",
    "                        'cat_features': cat_features,\n",
    "                        'random_state': 42, \n",
    "                    },\n",
    "              'C': {\"iterations\": 5000, \n",
    "                    \"learning_rate\": 0.03425599789981457,\n",
    "                    \"depth\": 8,\n",
    "                    \"l2_leaf_reg\": 4,\n",
    "                    \"loss_function\": \"MAE\", \n",
    "                    \"border_count\": 218, \n",
    "                    \"verbose\": 500, \n",
    "                    \"subsample\": 0.6848272280307022, \n",
    "                    'early_stopping_rounds': 100,\n",
    "                    'cat_features': cat_features,\n",
    "                    'random_state': 42, }\n",
    "}\n",
    "              \n",
    "cat_params_no_feature =  { 'A': {\n",
    "                        \"iterations\": 5000,\n",
    "                        \"learning_rate\": 0.034867396508006264,\n",
    "                        \"depth\": 8,\n",
    "                        \"l2_leaf_reg\": 1,\n",
    "                        \"loss_function\": \"MAE\",\n",
    "                        \"border_count\": 92,\n",
    "                        \"verbose\": 500,\n",
    "                        \"subsample\": 0.7641850606486046,\n",
    "                        'early_stopping_rounds': 100,\n",
    "                        'random_state': 42, \n",
    "                    },\n",
    "              'B': {\n",
    "                        \"iterations\": 5000,\n",
    "                        \"learning_rate\": 0.037511244177544326,\n",
    "                        \"depth\": 6,\n",
    "                        \"l2_leaf_reg\": 5,\n",
    "                        \"loss_function\": \"MAE\",\n",
    "                        \"border_count\": 128,\n",
    "                        \"verbose\": 500,\n",
    "                        \"subsample\": 0.8012204629505595,\n",
    "                        'early_stopping_rounds': 100,\n",
    "                        'random_state': 42, \n",
    "                    },\n",
    "              'C': {\"iterations\": 5000, \n",
    "                    \"learning_rate\": 0.03425599789981457,\n",
    "                    \"depth\": 8,\n",
    "                    \"l2_leaf_reg\": 4,\n",
    "                    \"loss_function\": \"MAE\", \n",
    "                    \"border_count\": 218, \n",
    "                    \"verbose\": 500, \n",
    "                    \"subsample\": 0.6848272280307022, \n",
    "                    'early_stopping_rounds': 100,\n",
    "                    'random_state': 42, }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CATegorical(df):\n",
    "    df['dew_or_rime:idx'] = df['dew_or_rime:idx'].astype(int)\n",
    "    df['is_in_shadow:idx'] = df['is_in_shadow:idx'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat = []\n",
    "\n",
    "from sklearn.impute import IterativeImputer\n",
    "# Cat_1 training and predictions\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train_cat, X_test_cat, is_day_feature1 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated,mode = 'cat')\n",
    "    X_train_cat.drop(columns=['date_forecast'], inplace=True)\n",
    "    print(f'Doing... {loc}')\n",
    "\n",
    "    imputer = IterativeImputer(max_iter=5, random_state=42)\n",
    "    for col in X_train_cat.columns:\n",
    "        X_train_cat[col] = imputer.fit_transform(np.array(X_train_cat[col]).reshape(-1,1))\n",
    "    for col in X_test_cat.columns:\n",
    "        X_test_cat[col] = imputer.fit_transform(np.array(X_test_cat[col]).reshape(-1,1))\n",
    "    \n",
    "    targets_cat = pd.DataFrame( {'pv_measurement': X_train_cat['pv_measurement']})\n",
    "    X_train_cat = X_train_cat.drop(columns=['pv_measurement'])\n",
    "    X_train_cat = CATegorical(X_train_cat)\n",
    "    X_test_cat = CATegorical(X_test_cat)\n",
    "\n",
    "    model_cat = CatBoostRegressor(**cat_params[loc])\n",
    "\n",
    "    X_train_cat1, X_val_cat1, y_train_cat1, y_val_cat1 = train_test_split(X_train_cat, targets_cat, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Training\n",
    "    model_cat.fit(X_train_cat1, y_train_cat1['pv_measurement'],eval_set=(X_val_cat1, y_val_cat1['pv_measurement']),)\n",
    "\n",
    "    # Prediction\n",
    "    predictions_cat = model_cat.predict(X_test_cat[model_cat.feature_names_])\n",
    "    \n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat = predictions_cat * is_day_feature1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat = np.clip(adjusted_final_predictions_cat, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat.append(adjusted_final_predictions_cat)\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_cat = np.array(all_predictions_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat_2 = []\n",
    "\n",
    "# Catboost nr 3\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    X_train_3, X_test_3, is_day_feature_3 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    X_train_3.drop(columns=['date_forecast'], inplace=True)\n",
    "    imputer = IterativeImputer(max_iter=5, random_state=42)\n",
    "    for col in X_train_3.columns:\n",
    "        X_train_3[col] = imputer.fit_transform(np.array(X_train_3[col]).reshape(-1,1))\n",
    "    for col in X_test_3.columns:\n",
    "        X_test_3[col] = imputer.fit_transform(np.array(X_test_3[col]).reshape(-1,1))\n",
    "    targets_3 = pd.DataFrame( {'pv_measurement': X_train_3['pv_measurement']})\n",
    "    X_train_3 = X_train_3.drop(columns=['pv_measurement'])\n",
    "    \n",
    "    model_cat_3 = CatBoostRegressor(**cat_params_no_feature[loc])\n",
    "    X_train_3 = feature_engineering(X_train_3)\n",
    "    X_test_3 = feature_engineering(X_test_3)\n",
    "\n",
    "    # Create 'sin_sun_azimuth' and 'cos_sun_azimuth' from 'sun_azimuth' in radians\n",
    "    X_train_3['sin_sun_azimuth'] = np.sin(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_train_3['cos_sun_azimuth'] = np.cos(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_test_3['sin_sun_azimuth'] = np.sin(np.radians(X_test_3['sun_azimuth:d']))\n",
    "    X_test_3['cos_sun_azimuth'] = np.cos(np.radians(X_test_3['sun_azimuth:d']))\n",
    "\n",
    "    # Now drop the original 'sun_azimuth' feature\n",
    "    X_train_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    X_test_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train_cat_3, X_test_cat_3, y_train_cat_3, y_test_cat_3 = train_test_split(X_train_3, targets_3, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model_cat_3.fit(X_train_cat_3, y_train_cat_3['pv_measurement'],eval_set=(X_test_cat_3, y_test_cat_3['pv_measurement']),)  \n",
    "    \n",
    "    # Pred\n",
    "    pred_cat_2 = model_cat_3.predict(X_test_3[model_cat_3.feature_names_])\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat_2 = pred_cat_2 * is_day_feature_3['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat_2 = np.clip(adjusted_final_predictions_cat_2, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat_2.append(adjusted_final_predictions_cat_2) \n",
    "\n",
    "# Changing final list to array   \n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat_3 = []\n",
    "# Catboost nr 3\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    X_train_3, X_test_3, is_day_feature_3 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated,mode = 'catboost')\n",
    "    X_train_3.drop(columns=['date_forecast'], inplace=True)\n",
    "    imputer = IterativeImputer(max_iter=5, random_state=42)\n",
    "    for col in X_train_3.columns:\n",
    "        X_train_3[col] = imputer.fit_transform(np.array(X_train_3[col]).reshape(-1,1))\n",
    "    for col in X_test_3.columns:\n",
    "        X_test_3[col] = imputer.fit_transform(np.array(X_test_3[col]).reshape(-1,1))\n",
    "    targets_3 = pd.DataFrame( {'pv_measurement': X_train_3['pv_measurement']})\n",
    "    X_train_3 = X_train_3.drop(columns=['pv_measurement'])\n",
    "    \n",
    "    model_cat_3 = CatBoostRegressor(**cat_params_no_feature[loc])\n",
    "   # X_train_3 = feature_engineering(X_train_3)\n",
    "   # X_test_3 = feature_engineering(X_test_3)\n",
    "\n",
    "    # Create 'sin_sun_azimuth' and 'cos_sun_azimuth' from 'sun_azimuth' in radians\n",
    "    X_train_3['sin_sun_azimuth'] = np.sin(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_train_3['cos_sun_azimuth'] = np.cos(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_test_3['sin_sun_azimuth'] = np.sin(np.radians(X_test_3['sun_azimuth:d']))\n",
    "    X_test_3['cos_sun_azimuth'] = np.cos(np.radians(X_test_3['sun_azimuth:d']))\n",
    "\n",
    "    # Now drop the original 'sun_azimuth' feature\n",
    "    X_train_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    X_test_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train_cat_3, X_test_cat_3, y_train_cat_3, y_test_cat_3 = train_test_split(X_train_3, targets_3, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model_cat_3.fit(X_train_cat_3, y_train_cat_3['pv_measurement'],eval_set=(X_test_cat_3, y_test_cat_3['pv_measurement']),)  \n",
    "    \n",
    "    # Pred\n",
    "    pred_cat_3 = model_cat_3.predict(X_test_3[model_cat_3.feature_names_])\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat_3 = pred_cat_3 * is_day_feature_3['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat_3 = np.clip(adjusted_final_predictions_cat_3, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat_3.append(adjusted_final_predictions_cat_3) \n",
    "\n",
    "# Changing final list to array   \n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with some extra features\n",
    "def process_location_xgb(X, y, location_name,seeds):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('xgboost')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm, optimize='MAE',early_stopping=True,early_stopping_max_iters=100)\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e).flatten()\n",
    "all_predictions_cat = np.array(all_predictions_cat).flatten()\n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2).flatten()\n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3).flatten()\n",
    "all_pred = 0.25*all_predictions_cat+0.25 * all_predictions_lGBM_e+0.25*all_predictions_cat_2 + 0.25*all_predictions_cat_3\n",
    "print(all_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = all_pred\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('best_score1.csv')\n",
    "df2 = pd.read_csv('best_score2.csv')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot delle differenze tra df e df1\n",
    "plt.plot(df1['prediction'] - df['prediction'])\n",
    "plt.title('Differenze tra df1 e df')\n",
    "plt.xlabel('Indice')\n",
    "plt.ylabel('Differenza')\n",
    "plt.show()\n",
    "\n",
    "# plot delle differenze tra df e df2\n",
    "plt.plot(df2['prediction'] - df['prediction'])\n",
    "plt.title('Differenze tra df2 e df')\n",
    "plt.xlabel('Indice')\n",
    "plt.ylabel('Differenza')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-name",
   "language": "python",
   "name": "pycaret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
