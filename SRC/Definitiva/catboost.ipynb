{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcmtI0eomDnS",
        "outputId": "07f6991f-cb7f-4b7c-c319-342fbc58bd04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.2\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: __MACOSX/._data         \n",
            "  inflating: data/.DS_Store          \n",
            "  inflating: __MACOSX/data/._.DS_Store  \n",
            "  inflating: data/test.csv           \n",
            "  inflating: __MACOSX/data/._test.csv  \n",
            "   creating: data/A/\n",
            "  inflating: data/Readme.md          \n",
            "  inflating: __MACOSX/data/._Readme.md  \n",
            "   creating: data/C/\n",
            "   creating: data/B/\n",
            "  inflating: data/sample_submission.csv  \n",
            "  inflating: __MACOSX/data/._sample_submission.csv  \n",
            "  inflating: data/read_files.ipynb   \n",
            "  inflating: data/A/X_train_observed.parquet  \n",
            "  inflating: data/A/train_targets.parquet  \n",
            "  inflating: data/A/X_train_estimated.parquet  \n",
            "  inflating: data/A/X_test_estimated.parquet  \n",
            "  inflating: data/C/X_train_observed.parquet  \n",
            "  inflating: data/C/train_targets.parquet  \n",
            "  inflating: data/C/X_train_estimated.parquet  \n",
            "  inflating: data/C/X_test_estimated.parquet  \n",
            "  inflating: data/B/X_train_observed.parquet  \n",
            "  inflating: data/B/train_targets.parquet  \n",
            "  inflating: data/B/X_train_estimated.parquet  \n",
            "  inflating: data/B/X_test_estimated.parquet  \n"
          ]
        }
      ],
      "source": [
        "!pip install catboost\n",
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWsThJMmmNRy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "# Sopprime tutti i FutureWarning\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# ADD TIME FEATURES\n",
        "def add_time_features(df, time_column):\n",
        "    '''\n",
        "        This function will add some time feature based on the param 'time_columns'\n",
        "\n",
        "        Params:\n",
        "            df-> Dataframe with the column contained in 'time_column'\n",
        "            time_column -> the column that is a datetime object\n",
        "\n",
        "        Returns:\n",
        "            A dataframe with time features\n",
        "    '''\n",
        "\n",
        "    df[time_column] = pd.to_datetime(df[time_column])  # Make sure the time column is in datetime format\n",
        "    df['hour'] = df[time_column].dt.hour\n",
        "    df['day_of_week'] = df[time_column].dt.dayofweek\n",
        "    df['month'] = df[time_column].dt.month\n",
        "    df['day_of_year'] = df[time_column].dt.dayofyear\n",
        "    df['week_of_year'] = df[time_column].dt.isocalendar().week\n",
        "    df['year'] = df[time_column].dt.year\n",
        "    df['sin_hour'] = np.sin(np.pi * df[time_column].dt.hour/24.)\n",
        "    df['sin_month'] = np.sin(np.pi * df[time_column].dt.month/12.)\n",
        "    #why these feature? Who knows\n",
        "    return df\n",
        "\n",
        "def plot_targets(targets):\n",
        "    '''\n",
        "        Plot the target, by a giving date\n",
        "\n",
        "        Params:\n",
        "            Targets-> A dataframe with the target value\n",
        "            Start_date -> the start date\n",
        "            End_date -> the end date\n",
        "\n",
        "        Returns:\n",
        "            Sto cazzo\n",
        "    '''\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(targets, label='PV Measurement', color='blue')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('PV Measurement')\n",
        "    plt.title('PV Measurement Over Time')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def to_datetime(df,column):\n",
        "    '''\n",
        "        Make the column in datetime format\n",
        "    '''\n",
        "    return pd.to_datetime(df[column])\n",
        "\n",
        "def resampling(df,column):\n",
        "    '''\n",
        "        Resample df to 1 hour using mean() as aggregator and drop rows where all columns are NaN\n",
        "\n",
        "        Params :\n",
        "            df -> the dataframe to be resampled\n",
        "            column -> the time column\n",
        "    '''\n",
        "    return df.set_index(keys = column).resample('1H').mean().dropna(how='all').reset_index()\n",
        "\n",
        "def filter_df(df,columnlist):\n",
        "    return df.drop(columns = columnlist)\n",
        "\n",
        "def extract_data_calc(df):\n",
        "    '''\n",
        "    This function create a dataframe with 'date_forecast' as index and the column 'date_calc' resampled by '1H'.\n",
        "    If there's no data in a specific bin, the resulting value for that bin would be NaN (not a number).\n",
        "    Params:\n",
        "        df -> dataframe with 'date_forecast' and 'date_calc' columns.\n",
        "            'date_calc' is expected to contain data that the user wants to resample or analyze.\n",
        "    Returns:\n",
        "        A dataframe with 'date_calc' resampled.\n",
        "    '''\n",
        "    return df.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
        "\n",
        "\n",
        "\n",
        "def is_estimated_feature(df):\n",
        "    '''\n",
        "        This function will create some time feature and estimated information. It's need to let the model understand is\n",
        "        estimated value.\n",
        "        Params:\n",
        "            df -> It MUST be an estimated dataframe, that contains 'data_forecast' as datetime type\n",
        "        Returns:\n",
        "            A dataframe with 'time_dummy', 'time_delta' and 'is_estimated'\n",
        "    '''\n",
        "    df['time_delta'] = (df['date_calc'] - df['date_forecast']).dt.total_seconds() / 3600\n",
        "    df['is_estimated'] = 1\n",
        "    return df\n",
        "\n",
        "def delete_stationarity(df):\n",
        "    '''\n",
        "    Removes constant stretches of data within a DataFrame where the 'pv_measurement' column does not change.\n",
        "    The function identifies blocks of data where the 'pv_measurement' stays constant for more than two consecutive\n",
        "    points and removes these blocks to address data stationarity.\n",
        "\n",
        "    params:\n",
        "        df -> DataFrame\n",
        "              A pandas DataFrame with a 'pv_measurement' column which contains the data from which to remove stationarity.\n",
        "\n",
        "    return:\n",
        "        The DataFrame with constant stretches of data removed from the 'pv_measurement' column.\n",
        "    '''\n",
        "\n",
        "    #Calculate the difference, this need for check the constant\n",
        "    df['diff'] = df['pv_measurement'].diff().fillna(0)\n",
        "\n",
        "    # Create an indicator for constant stretches\n",
        "    df['constant'] = (df['diff'] == 0).astype(int)\n",
        "\n",
        "    # Use the indicator to mark stretches. The diff() function here identifies change-points.\n",
        "    df['block'] = (df['constant'].diff() != 0).astype(int).cumsum()\n",
        "\n",
        "    # Get the size of each constant block\n",
        "    block_sizes = df.groupby('block')['constant'].sum()\n",
        "\n",
        "    # Identify blocks that are constant for more than N consecutive time points (in this case 2)\n",
        "    constant_blocks = block_sizes[block_sizes > 2].index\n",
        "\n",
        "    # Remove the constant\n",
        "    filtered_df = df[~df['block'].isin(constant_blocks)]\n",
        "\n",
        "    return filtered_df.drop(columns=['diff', 'constant', 'block'])\n",
        "\n",
        "def preprocessing(targets, observed, estimated, test):\n",
        "    '''\n",
        "        This function makes all the preprocessing needed for the correct run of the model, it will perform:\n",
        "            - Resampling\n",
        "            - Filtering\n",
        "            - Imputation\n",
        "            - Outliers removal\n",
        "            - Categorical Encoding\n",
        "\n",
        "        Params:\n",
        "            targets -> dataframe of the target parquet\n",
        "            observed -> dataframe of observed train data\n",
        "            estimated -> dataframe of estimated train data\n",
        "            test -> dataframe of test data\n",
        "        Returns:\n",
        "            train_data -> dataframe of all data ready to train\n",
        "            test_data -> dataframe of all data ready to test\n",
        "            is_day -> dataframe of is_day categorical feature for post processing\n",
        "\n",
        "    '''\n",
        "    targets['time'] = to_datetime(targets,'time')\n",
        "    estimated['date_forecast'] = to_datetime(estimated,'date_forecast')\n",
        "    observed['date_forecast'] = to_datetime(observed,'date_forecast')\n",
        "    test['date_forecast'] = to_datetime(test,'date_forecast')\n",
        "\n",
        "    observed_resampled = resampling(observed,'date_forecast')\n",
        "    estimated_resampled = resampling(estimated,'date_forecast')\n",
        "    test_resampled = resampling(test,'date_forecast')\n",
        "\n",
        "    date_calc_resampled_observed = extract_data_calc(estimated)\n",
        "    date_calc_resampled_test = extract_data_calc(test)\n",
        "\n",
        "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_observed, left_on='date_forecast', right_index=True)\n",
        "    test_resampled = test_resampled.merge(date_calc_resampled_test, left_on='date_forecast', right_index=True)\n",
        "\n",
        "    is_day = test_resampled[['date_forecast', 'is_day:idx']]\n",
        "    test_resampled = filter_df(test_resampled,['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
        "    observed_resampled = filter_df(observed_resampled,['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
        "    estimated_resampled = filter_df(estimated_resampled,[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
        "\n",
        "    #This MUST be zero because is not estimated.\n",
        "    observed_resampled['is_estimated'] = 0\n",
        "    observed_resampled['time_delta'] = 0\n",
        "\n",
        "    estimated_resampled = is_estimated_feature(estimated_resampled)\n",
        "    test_resampled = is_estimated_feature(test_resampled)\n",
        "\n",
        "    X = pd.concat([observed_resampled,estimated_resampled],axis = 0)\n",
        "    train_data = pd.merge(targets, X, how='inner', left_on='time', right_on='date_forecast')\n",
        "\n",
        "    train_data = add_time_features(train_data, 'time')\n",
        "    test_data = add_time_features(test_resampled, 'date_forecast')\n",
        "\n",
        "    train_data = delete_stationarity(train_data)\n",
        "\n",
        "    train_data = filter_df(train_data, ['time','date_calc'])\n",
        "    test_data = filter_df(test_resampled, ['date_calc'])\n",
        "\n",
        "    return train_data, test_data, is_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWE6WtCJmODf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "def train_data_split(n_splits = 5):\n",
        "    '''\n",
        "        This function return the cross_validator split\n",
        "    '''\n",
        "    return TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "import re\n",
        "def regexdf(df):\n",
        "    '''\n",
        "        This function let lgbm work, this because it cannot accept ':'\n",
        "    '''\n",
        "    return df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_-]+', '', x))\n",
        "\n",
        "\n",
        "\n",
        "def final_model(df,model,param,X_test):\n",
        "    '''\n",
        "        This function will retrain on the bagged model so the model is trained on ALL TRAIN DATA.\n",
        "\n",
        "        Params:\n",
        "            df -> Train data\n",
        "            model -> your model\n",
        "            param -> your hyperparameter\n",
        "            X_test -> Test Data\n",
        "        Return:\n",
        "            the prediction\n",
        "    '''\n",
        "    X_train = df.drop(columns = 'pv_measurement')\n",
        "    y_train = df['pv_measurement']\n",
        "    model.fit(X_train,y_train)\n",
        "    return model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk22EnjLmP1E"
      },
      "outputs": [],
      "source": [
        "def post_process(prediction, is_day):\n",
        "    '''\n",
        "        This function will post_process the predicition, by multiplying the is_day on that prediction.\n",
        "        I.E.\n",
        "            This is for rescale the prediction that can be too high in a 0.5 day moment.\n",
        "        Params:\n",
        "            Prediction -> your prediction\n",
        "            is_day -> the is_day dataframe\n",
        "        Return:\n",
        "            the clipped version of the adjusted prediction, it's clipped for any eventual negative prediction\n",
        "    '''\n",
        "    adjusted_predictions = prediction * is_day['is_day:idx']\n",
        "    return np.clip(adjusted_predictions, 0, None)\n",
        "\n",
        "def submission(predictions):\n",
        "    '''\n",
        "        This function will create a ready to deliver file\n",
        "\n",
        "        Params:\n",
        "            predictions -> the entire prediction for all the location\n",
        "\n",
        "        Returns:\n",
        "            stocazzo\n",
        "        Save:\n",
        "            Submission file\n",
        "\n",
        "    '''\n",
        "    all_predictions = np.array(predictions).flatten()\n",
        "    # Save the final_predictions to CSV\n",
        "    df = pd.DataFrame(all_predictions, columns=['prediction'])\n",
        "    df['id'] = df.index\n",
        "    df = df[['id', 'prediction']]\n",
        "    df.to_csv('TestFECla3.csv', index=False)\n",
        "\n",
        "def feature_engineering(df):\n",
        "    '''\n",
        "        This function will create new feature from the interaction of different feature of df dataframe.\n",
        "\n",
        "        Params:\n",
        "            df: dataframe\n",
        "        Return:\n",
        "            FE dataframe\n",
        "    '''\n",
        "    '''\n",
        "    df['direct_diffuse_rad_interaction'] = df['direct_rad:W'] * df['diffuse_rad:W']\n",
        "    df['raddir'] = (df['direct_rad:W'] ) * (df['absolute_humidity_2m:gm3'])\n",
        "    df['effectivehum'] = (df['absolute_humidity_2m:gm3']) * (df['direct_rad:W'])\n",
        "    df['Radiazione_solare_effettiva'] = (df['direct_rad:W'] + df['diffuse_rad:W'] ) * (df['effective_cloud_cover:p'])\n",
        "    df['direct_radW_squared'] = df['direct_rad:W'] ** 2\n",
        "    df['clearcloud'] =   df['clear_sky_rad:W']*df['total_cloud_cover:p']\n",
        "    df['radiation_squared'] = df['clear_sky_rad:W'] ** 2\n",
        "    df['effectivehum'] = (df['absolute_humidity_2m:gm3']) * (df['direct_rad:W'])\n",
        "    '''\n",
        "    df['refined_global_rad'] = df['direct_rad:W'] + df['diffuse_rad:W']\n",
        "\n",
        "   # df = df[df.index.month.isin([4, 5, 6, 7, 8, 9])]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l5kjhGBmSOJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "\n",
        "def tune_model(X, y, model_params, n_iter=200, cv=2, verbose=1, random_state=42, n_jobs=-1):\n",
        "    # Store the best estimators here\n",
        "    best_estimators = {}\n",
        "\n",
        "    # Define a dictionary with model shorthand and actual regressor objects\n",
        "    models = {\n",
        "        'catboost': CatBoostRegressor(verbose=0, thread_count=n_jobs),\n",
        "    }\n",
        "    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "    # Loop through the models and perform Randomized Search\n",
        "    for name, model in models.items():\n",
        "        if name in model_params:  # Only if model parameters were provided\n",
        "            print(f\"Tuning hyperparameters for {name}...\")\n",
        "            search = RandomizedSearchCV(\n",
        "                estimator=model,\n",
        "                param_distributions=model_params[name],\n",
        "                n_iter=n_iter,\n",
        "                cv=cv,\n",
        "                verbose=verbose,\n",
        "                random_state=random_state,\n",
        "                scoring=mae_scorer,\n",
        "                n_jobs=n_jobs\n",
        "            )\n",
        "            search.fit(X, y)\n",
        "            best_estimators[name] = search.best_estimator_\n",
        "            print(f\"Best parameters for {name}: {search.best_params_}\")\n",
        "        else:\n",
        "            print(f\"No parameters provided for {name}, skipping...\")\n",
        "\n",
        "    return best_estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6--0hRK1mbeK"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# Parameters for Random Forest\n",
        "rf_params = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': randint(3, 20),\n",
        "    'min_samples_split': uniform(0.01, 0.1),\n",
        "    'min_samples_leaf': uniform(0.01, 0.1),\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Parameters for Extra Trees\n",
        "et_params = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': randint(3, 20),\n",
        "    'min_samples_split': uniform(0.01, 0.1),\n",
        "    'min_samples_leaf': uniform(0.01, 0.1),\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Parameters for XGBoost\n",
        "xgb_params = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'subsample': uniform(0.5, 0.5),\n",
        "    'colsample_bytree': uniform(0.5, 0.5),\n",
        "    'min_child_weight': randint(1, 10)\n",
        "}\n",
        "\n",
        "# Parameters for LightGBM\n",
        "lgbm_params = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': randint(-1, 20),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'num_leaves': randint(20, 300),\n",
        "    'min_child_samples': randint(10, 100),\n",
        "    'subsample': uniform(0.5, 0.5),\n",
        "    'colsample_bytree': uniform(0.5, 0.5),\n",
        "    'verbose' : [-1]\n",
        "}\n",
        "\n",
        "# Parameters for CatBoost\n",
        "cat_params = {\n",
        "    'iterations': randint(100, 500),\n",
        "    'depth': randint(3, 10),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'l2_leaf_reg': randint(1, 10),\n",
        "    'border_count': randint(1, 255),\n",
        "    'subsample': uniform(0.5, 0.5)\n",
        "}\n",
        "\n",
        "# Combine all parameter dictionaries into one\n",
        "model_params = {\n",
        "    'catboost': cat_params\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3iA3nsnMIeUl"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def pipeline(df):\n",
        "  numerical_df = df.drop(columns = ['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'])\n",
        "  categorical_df = df[['dew_or_rime:idx', 'is_in_shadow:idx']]\n",
        "  numerical_imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "  # Adattare l'imputer ai dati\n",
        "  numerical_imputer.fit(numerical_df)\n",
        "\n",
        "  # Trasformare i dati con i valori mancanti imputati\n",
        "  transformed_data = pd.DataFrame(numerical_imputer.transform(numerical_df),columns = numerical_df.columns)\n",
        "\n",
        "  categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "  # Adattare l'imputer ai dati\n",
        "  categorical_imputer.fit(categorical_df)\n",
        "\n",
        "  # Trasformare i dati con i valori mancanti imputati\n",
        "  transformed_data_categorical = pd.DataFrame(categorical_imputer.transform(categorical_df),columns = categorical_df.columns)\n",
        "\n",
        "  # Creiamo un OneHotEncoder\n",
        "  encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "  # Adattiamo e trasformiamo i dati usando l'encoder\n",
        "  encoded_data = encoder.fit_transform(transformed_data_categorical)\n",
        "\n",
        "  # Il risultato è un array NumPy, quindi se vogliamo un DataFrame dobbiamo convertirlo\n",
        "  encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(transformed_data_categorical.columns))\n",
        "\n",
        "  return pd.concat([transformed_data,encoded_df],axis = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "SIrpoqgKmiUY",
        "outputId": "9a72c544-d357-4fbe-8477-bd6100dfd3f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(19622, 60) (720, 59)\n",
            "(19622, 59)\n",
            "__________\n",
            "Tuning hyperparameters for xgboost...\n",
            "Fitting 2 folds for each of 200 candidates, totalling 400 fits\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-699f9c4e96e5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__________'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__________'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'catboost'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-ad976e594438>\u001b[0m in \u001b[0;36mtune_model\u001b[0;34m(X, y, model_params, n_iter, cv, verbose, random_state, n_jobs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             )\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mbest_estimators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best parameters for {name}: {search.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "locations = ['A','B','C']\n",
        "all_predictions = []\n",
        "\n",
        "for loc in locations:\n",
        "    train = pd.read_parquet(f'data/{loc}/train_targets.parquet').fillna(0)\n",
        "    X_train_estimated = pd.read_parquet(f'data/{loc}/X_train_estimated.parquet')\n",
        "    X_train_observed = pd.read_parquet(f'data/{loc}/X_train_observed.parquet')\n",
        "    X_test_estimated = pd.read_parquet(f'data/{loc}/X_test_estimated.parquet')\n",
        "\n",
        "    train_data, test_data, is_day= preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
        "    train_data = train_data.drop(columns = ['date_forecast'])\n",
        "    test_data = test_data.drop(columns = 'date_forecast')\n",
        "\n",
        "    train_data = feature_engineering(train_data)\n",
        "    test_data = feature_engineering(test_data)\n",
        "    train_data = regexdf(train_data)\n",
        "    test_data = regexdf(test_data)\n",
        "    print(train_data.shape, test_data.shape)\n",
        "    X = train_data.drop(columns = ['pv_measurement']).reset_index().drop(columns = 'index')\n",
        "    y = train_data['pv_measurement'].reset_index().drop(columns = 'index')\n",
        "\n",
        "    print(X.shape)\n",
        "    print('__________')\n",
        "    model = tune_model(X,y,model_params)\n",
        "    print('__________')\n",
        "    model['catboost'].fit(X,y)\n",
        "    predictions =  model['catboost'].predict(test_data)\n",
        "    predictions_fix = post_process(predictions.flatten(),is_day)\n",
        "    all_predictions.append(predictions_fix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phPgI7DHm0K2"
      },
      "outputs": [],
      "source": [
        "test_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haGKV29emi6_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def save(best_models, loc):\n",
        "  best_params = {}\n",
        "\n",
        "  # Leggere il file esistente se esiste e caricare i parametri\n",
        "  filename = f'best_params{loc}.json'\n",
        "  try:\n",
        "      with open(filename, 'r') as file:\n",
        "          best_params = json.load(file)\n",
        "  except FileNotFoundError:\n",
        "      print(f\"Il file '{filename}' non esiste. Sarà creato uno nuovo.\")\n",
        "\n",
        "  # Iterazione attraverso ciascun modello nel dizionario best_models\n",
        "  for model_name, model in best_models.items():\n",
        "      if model_name in ['rf', 'et']:  # Scikit-learn models\n",
        "          best_params[model_name] = model.get_params()\n",
        "      elif model_name == 'xgboost':\n",
        "          best_params[model_name] = model.get_xgb_params()\n",
        "      elif model_name == 'lightgbm':\n",
        "          best_params[model_name] = model.get_params()\n",
        "      elif model_name == 'catboost':\n",
        "          best_params[model_name] = model.get_params()\n",
        "\n",
        "  # Salvare il dizionario aggiornato nel file\n",
        "  with open(filename, 'w') as file:\n",
        "      json.dump(best_params, file, indent=4)\n",
        "\n",
        "  print(f\"I parametri dei modelli sono stati salvati o aggiornati in '{filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf6PCPOammUt"
      },
      "outputs": [],
      "source": [
        "'''sample_submission = pd.read_csv('sample_submission.csv')\n",
        "sample_submission\n",
        "sample_submission = sample_submission[['id']].merge(final_df[['id', 'prediction']], on='id', how='left')\n",
        "sample_submission.to_csv('my_first_submission.csv', index=False)'''\n",
        "\n",
        "final_predictions = np.concatenate(all_predictions)\n",
        "\n",
        "# Save the final_predictions to CSV\n",
        "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
        "df['id'] = df.index\n",
        "df = df[['id', 'prediction']]\n",
        "df.to_csv('TestFE+Ens.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
