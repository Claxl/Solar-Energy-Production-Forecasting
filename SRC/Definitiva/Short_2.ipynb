{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from pycaret.regression import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the maximum number of rows when you printing out a DataFrame\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features nr 1\n",
    "def add_time_features(df, time_column):\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  \n",
    "\n",
    "    # Extract various time features\n",
    "    df['hour'] = df[time_column].dt.hour\n",
    "    df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "    df['month'] = df[time_column].dt.month\n",
    "    df['day_of_year'] = df[time_column].dt.dayofyear\n",
    "    df['week_of_year'] = df[time_column].dt.isocalendar().week \n",
    "    df['year'] = df[time_column].dt.year\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features nr 2\n",
    "def add_time_features_cat(df, time_column):\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  \n",
    "    \n",
    "    df['sin_hour'] = np.sin(np.pi * df[time_column].dt.hour/23.)\n",
    "    df['sin_month'] = np.sin(np.pi * df[time_column].dt.month/12.)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_est(observed, estimated, test):\n",
    "    \n",
    "    # 1. Create time-delta for estimated data\n",
    "      estimated['time_dummy'] = (estimated['date_forecast'] - estimated['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
    "      observed['time_dummy'] = 0 \n",
    "      test['time_dummy'] = (test['date_forecast'] - test['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
    "      \n",
    "      estimated['time_delta'] = (estimated['date_calc'] - estimated['date_forecast']).dt.total_seconds() / 3600\n",
    "      observed['time_delta'] = 0  # since observed data is not forecasting ahead\n",
    "      test['time_delta'] = (test['date_calc'] - test['date_forecast']).dt.total_seconds() / 3600\n",
    "      \n",
    "      # 2. Add indicator variable for estimated data\n",
    "      estimated['is_estimated'] = 1\n",
    "      observed['is_estimated'] = 0\n",
    "      test['is_estimated'] = 1\n",
    "      # Merge or concatenate data\n",
    "      df = pd.concat([observed, estimated], axis=0).sort_values(by='date_forecast')\n",
    "      \n",
    "      return df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stationary(df):\n",
    "    \n",
    "    # Removing data where the power output is saturated\n",
    "    # Step 1: Calculate the difference\n",
    "    df['diff'] = df['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    # Step 2: Create an indicator where diff is zero\n",
    "    df['constant'] = (df['diff'] == 0).astype(int)\n",
    "\n",
    "    # Step 3: Use the indicator where diff is zero. The diff() function here identifies change-points.\n",
    "    df['block'] = (df['constant'].diff() != 0).astype(int).cumsum()\n",
    "    block_sizes = df.groupby('block')['constant'].sum()\n",
    "\n",
    "    # Identify blocks that are constant for more than 2 consecutive time points (you can adjust this threshold)\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "\n",
    "    # Step 4: Remove the constant where diff is zero\n",
    "    filtered_data = df[~df['block'].isin(constant_blocks)]\n",
    "\n",
    "    # Drop time and temporary features\n",
    "    targets_ny = filtered_data[ ['time', 'pv_measurement']]\n",
    "    filtered_data = filtered_data.drop(columns=['diff', 'constant', 'block'])\n",
    "    return filtered_data, targets_ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing LigtGBM + catboost\n",
    "def preprocessing(targets, observed, estimated, test):\n",
    "    \n",
    "    # Ensure the datetime columns are in datetime format\n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "\n",
    "    date_calc_resampled_ob = estimated.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    date_calc_resampled_te = test.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    \n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_ob, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_te, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    #Save the is_day feature as this says a lot about when the power output is zero or not\n",
    "    is_day_feature = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    \n",
    "    #Drop some features that is noise\n",
    "    test_resampled = test_resampled.drop(columns =['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = observed_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    estimated_resampled = estimated_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "\n",
    "    \n",
    "    # Add extra features\n",
    "    weather_data, test_resampled = is_est(observed_resampled, estimated_resampled, test_resampled)\n",
    "    \n",
    "    # Merge with target values\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "\n",
    "    # Add the time-based features\n",
    "    merged_data = add_time_features(merged_data, 'time')  \n",
    "    test_resampled = add_time_features(test_resampled, 'date_forecast') \n",
    "    \n",
    "    filtered_data, targets_ny = delete_stationary(merged_data)\n",
    "    \n",
    "    # Drop time features\n",
    "    filtered_data = filtered_data.drop(columns=['time', 'pv_measurement','date_calc'])\n",
    "    test_resampled = test_resampled.drop(columns=[ 'date_forecast', 'date_calc'])\n",
    "    \n",
    "    return filtered_data, test_resampled, is_day_feature, targets_ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_est_cat(observed, estimated, test):\n",
    "      # Add indicator variable for estimated data\n",
    "      estimated['is_estimated'] = 1\n",
    "      observed['is_estimated'] = 0\n",
    "      test['is_estimated'] = 1\n",
    "\n",
    "      # Merge or concatenate data\n",
    "      df = pd.concat([observed, estimated], axis=0).sort_values(by='date_forecast')\n",
    "      \n",
    "      return df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing nr 2 for catboost models\n",
    "def preprocessing_cat(targets, observed, estimated, test):\n",
    "    \n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "\n",
    "    # Start the resampling from 15min to 1 hour\n",
    "    date_calc_resampled_ob = estimated.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    date_calc_resampled_te = test.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    \n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_ob, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_te, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    #Save the is_day feature as this says a lot about when the power output is zero or not\n",
    "    is_day_feature = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    \n",
    "    #Drop some features that is noise\n",
    "    test_resampled = test_resampled.drop(columns =['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = observed_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    estimated_resampled = estimated_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "\n",
    "    # Filter observed and estimated data for April to August\n",
    "    observed_resampled = observed_resampled[observed_resampled['date_forecast'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    estimated_resampled = estimated_resampled[estimated_resampled['date_forecast'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    targets = targets[targets['time'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "\n",
    "    # Merge the observed and estimated data\n",
    "    weather_data, test_resampled = is_est_cat(observed_resampled, estimated_resampled, test_resampled)\n",
    "\n",
    "    # Merge with target values filtering for the same months\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "    merged_data = add_time_features_cat(merged_data, 'time')  \n",
    "    test_resampled = add_time_features_cat(test_resampled, 'date_forecast')\n",
    "\n",
    "    filtered_data, targets_ny = delete_stationary(merged_data)\n",
    "    \n",
    "    # Drop some time features\n",
    "    filtered_data = filtered_data.drop(columns=['time', 'date_forecast', 'pv_measurement','date_calc'])\n",
    "    test_resampled = test_resampled.drop(columns=[ 'date_forecast','date_calc'])\n",
    "    \n",
    "    return filtered_data, test_resampled, is_day_feature, targets_ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_location_ex(X, y, location_name,seeds):\n",
    "    \n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "\n",
    "    # Added some extra features to this one model, did it here so we could reuse the same preprocesssing function on diffrent models\n",
    "    # Feature Combination 1: Solar Radiation and Cloud Cover Combination\n",
    "    data['weighted_rad'] = ((data['direct_rad:W'] * (1 - data['total_cloud_cover:p']/100)) +\n",
    "                        (data['diffuse_rad:W'] * (data['total_cloud_cover:p']/100)))\n",
    "\n",
    "    # Feature Combination 2: Atmospheric Conditions Combination\n",
    "    data['adjusted_clear_sky_rad'] = (data['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * data['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (data['air_density_2m:kgm3'] - 1.225)))  # Adjusted based on humidity and air density\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    #remove_outliers=True,  #Ble dårligere med denne\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm)#, early_stopping=True, fold=15)\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_location_cat_2(X, y, location_name,seeds):\n",
    "    \n",
    "    # Dropping some features for this one model\n",
    "    features_to_drop = ['dew_or_rime:idx', #'snow_density:kgm3',\n",
    "                        'fresh_snow_3h:cm', 'fresh_snow_1h:cm', 'snow_drift:idx', \n",
    "                        'snow_depth:cm', 'wind_speed_w_1000hPa:ms', 'prob_rime:p', \n",
    "                        'fresh_snow_6h:cm', 'snow_melt_10min:mm', \n",
    "                        'fresh_snow_12h:cm', 'rain_water:kgm2', \n",
    "                        'super_cooled_liquid_water:kgm2']\n",
    "    \n",
    "    X = X.drop(columns=features_to_drop)\n",
    "    \n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "\n",
    "                    #categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    #remove_outliers=True,  #Ble dårligere med denne\n",
    "                    html=False,\n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a Catboost model\n",
    "    cat = create_model('catboost')\n",
    "\n",
    "    # Tune the model\n",
    "    tuned_cat = tune_model(cat)\n",
    "    \n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_cat = ensemble_model(tuned_cat, method='Bagging')\n",
    "\n",
    "    # Train on whole dataset\n",
    "    final_model = finalize_model(bagged_cat)\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['A']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and predictions\n",
    "\n",
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM training and predictions\n",
    "all_predictions_lGBM_e = []\n",
    "for loc in locations:\n",
    "\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Calling preprocessing\n",
    "    X_train_1, X_test_1, is_day_feature_1, targets_1 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    \n",
    "    # Adding the extra features to the test set as well\n",
    "    X_train_1 = X_train_1.drop(columns=['date_forecast'])\n",
    "    X_test_1['weighted_rad'] = ((X_test_1['direct_rad:W'] * (1 - X_test_1['total_cloud_cover:p']/100)) +\n",
    "                        (X_test_1['diffuse_rad:W'] * (X_test_1['total_cloud_cover:p']/100)))\n",
    "\n",
    "    X_test_1['adjusted_clear_sky_rad'] = (X_test_1['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * X_test_1['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (X_test_1['air_density_2m:kgm3'] - 1.225)))\n",
    "    \n",
    "    # Training and prediction for diffrent seeds\n",
    "    total_predictions_light = None\n",
    "    seeds = [42]\n",
    "    for seed in seeds: \n",
    "        final_model_lGBM_e = process_location_ex(X_train_1, targets_1, loc, seed)\n",
    "        predictions_lGBM_e = predict_model(final_model_lGBM_e, data=X_test_1)\n",
    "        final_predictions_lGBM_e = predictions_lGBM_e['prediction_label']\n",
    "        if total_predictions_light is None:\n",
    "            total_predictions_light = np.zeros_like(final_predictions_lGBM_e)\n",
    "        total_predictions_light += final_predictions_lGBM_e\n",
    "\n",
    "    mean_pred_light = total_predictions_light/len(seeds)\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_lGBM_e = mean_pred_light * is_day_feature_1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_lGBM_e = np.clip(adjusted_final_predictions_lGBM_e, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_lGBM_e.append([adjusted_final_predictions_lGBM_e])\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat = []\n",
    "# Catboost model nr 1\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train_cat, X_test_cat, is_day_feature1, targets_cat = preprocessing_cat(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Making categorical features\n",
    "    cat_features = ['dew_or_rime:idx' ,'is_in_shadow:idx']\n",
    "    X_train_cat['dew_or_rime:idx'] = X_train_cat['dew_or_rime:idx'].astype(int)\n",
    "    X_train_cat['is_in_shadow:idx'] = X_train_cat['is_in_shadow:idx'].astype(int)\n",
    "    X_test_cat['dew_or_rime:idx'] = X_test_cat['dew_or_rime:idx'].astype(int)\n",
    "    X_test_cat['is_in_shadow:idx'] = X_test_cat['is_in_shadow:idx'].astype(int)\n",
    "\n",
    "    # Catboooooooozt fun\n",
    "    model_cat = CatBoostRegressor(\n",
    "        loss_function='MAE', \n",
    "        learning_rate=0.1, \n",
    "        verbose=200,\n",
    "        cat_features=cat_features,\n",
    "        random_state=42) \n",
    "        #n_estimators=20000,\n",
    "        #early_stopping_rounds=50,)\n",
    "\n",
    "    #X_train_cat1, X_val_cat1, y_train_cat1, y_val_cat1 = train_test_split(X_train_cat, targets_cat, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Training\n",
    "    model_cat.fit(X_train_cat,targets_cat['pv_measurement']) #X_train_cat1, y_train_cat1['pv_measurement'],eval_set=(X_val_cat1, y_val_cat1['pv_measurement']),)\n",
    "\n",
    "    # Prediction\n",
    "    predictions_cat = model_cat.predict(X_test_cat)\n",
    "    feature_importances = model_cat.get_feature_importance()\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat = predictions_cat * is_day_feature1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat = np.clip(adjusted_final_predictions_cat, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat.append(adjusted_final_predictions_cat)\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_cat = np.array(all_predictions_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat_2 = []\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train, X_test, is_day_feature, targets = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Dropping some winter months\n",
    "    X_train_cat_2 = X_train#[X_train['date_forecast'].dt.month.isin([3,4, 5, 6, 7, 8, 9,10])]\n",
    "\n",
    "    # Dropping date feature\n",
    "    X_train_cat_2 = X_train_cat_2.drop(columns=['date_forecast'])\n",
    "\n",
    "    # Training and prediction for diffrent seeds\n",
    "    seeds = [42]\n",
    "    total_predictions_cat_2 = None\n",
    "    for seed in seeds: \n",
    "        final_model_cat_2 = process_location_cat_2(X_train_cat_2, targets, loc,seed)#its aactually a catboost wohoo\n",
    "        predictions_cat_2 = predict_model(final_model_cat_2, X_test)\n",
    "        final_predictions_cat_2 = predictions_cat_2['prediction_label']\n",
    "        if total_predictions_cat_2 is None:\n",
    "            total_predictions_cat_2 = np.zeros_like(final_predictions_cat_2)\n",
    "            total_predictions_cat_2+=final_predictions_cat_2\n",
    "\n",
    "    mean_pred_cat_2 = total_predictions_cat_2/len(seeds)\n",
    "\n",
    "    adjusted_final_predictions_cat_2 = mean_pred_cat_2 * is_day_feature['is_day:idx']\n",
    "    adjusted_final_predictions_cat_2 = np.clip(adjusted_final_predictions_cat_2, 0, None)\n",
    "    all_predictions_cat_2.append([adjusted_final_predictions_cat_2])\n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat_3 = []\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train_3, X_test_3, is_day_feature_3, targets_3 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Dropping date feature\n",
    "    X_train_3 = X_train_3.drop(columns=['date_forecast'])\n",
    "    \n",
    "    # Catboooooooozt fun round 3 wohoo\n",
    "    model_cat_3 = CatBoostRegressor(\n",
    "    verbose=200, \n",
    "    learning_rate=0.03,\n",
    "    depth=10,\n",
    "    l2_leaf_reg=5,\n",
    "    random_state=42, \n",
    "    n_estimators=20000, \n",
    "    loss_function='MAE', \n",
    "    early_stopping_rounds=100,)\n",
    "\n",
    "    # Create 'sin_sun_azimuth' and 'cos_sun_azimuth' from 'sun_azimuth' in radians\n",
    "    X_train_3['sin_sun_azimuth'] = np.sin(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_train_3['cos_sun_azimuth'] = np.cos(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_test_3['sin_sun_azimuth'] = np.sin(np.radians(X_test_3['sun_azimuth:d']))\n",
    "    X_test_3['cos_sun_azimuth'] = np.cos(np.radians(X_test_3['sun_azimuth:d']))\n",
    "\n",
    "    # Now drop the original 'sun_azimuth' feature\n",
    "    X_train_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    X_test_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train_cat_3, X_test_cat_3, y_train_cat_3, y_test_cat_3 = train_test_split(X_train_3, targets_3, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model_cat_3.fit(X_train_cat_3, y_train_cat_3['pv_measurement'],eval_set=(X_test_cat_3, y_test_cat_3['pv_measurement']),)  \n",
    "    \n",
    "    # Pred\n",
    "    pred_cat_3 = model_cat_3.predict(X_test_3)\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat_3 = pred_cat_3 * is_day_feature_3['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat_3 = np.clip(adjusted_final_predictions_cat_3, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat_3.append(adjusted_final_predictions_cat_3) \n",
    "\n",
    "# Changing final list to array   \n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with some extra features\n",
    "def train_xgb(X, y, location_name,seeds):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Added some extra features to this one model, did it here so we could reuse the same preprocesssing function on diffrent models\n",
    "    # Feature Combination 1: Solar Radiation and Cloud Cover Combination\n",
    "    data['weighted_rad'] = ((data['direct_rad:W'] * (1 - data['total_cloud_cover:p']/100)) +\n",
    "                        (data['diffuse_rad:W'] * (data['total_cloud_cover:p']/100)))\n",
    "\n",
    "    # Feature Combination 2: Atmospheric Conditions Combination\n",
    "    data['adjusted_clear_sky_rad'] = (data['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * data['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (data['air_density_2m:kgm3'] - 1.225)))  # Adjusted based on humidity and air density\n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    xgb = create_model('xgboost')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_xgb = tune_model(xgb, optimize='MAE')\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_xgb = ensemble_model(tuned_xgb, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_xgb)\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = []  # Initialize a list to store results for each location.\n",
    "\n",
    "for loc in locations:  # Iterate through each location.\n",
    "    # Load data specific to the current location.\n",
    "    train, X_train_estimated, X_train_observed, X_test_estimated = load(loc)\n",
    "    \n",
    "    # Preprocess the data.\n",
    "    X_train, X_test, is_day_feature, targets = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    X_train = X_train.drop(columns=['date_forecast'])\n",
    "    X_test['weighted_rad'] = ((X_test['direct_rad:W'] * (1 - X_test['total_cloud_cover:p']/100)) +\n",
    "                        (X_test['diffuse_rad:W'] * (X_test['total_cloud_cover:p']/100)))\n",
    "\n",
    "    X_test['adjusted_clear_sky_rad'] = (X_test['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * X_test['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (X_test['air_density_2m:kgm3'] - 1.225)))\n",
    "    \n",
    "\n",
    "    # Train the XGBoost model using a custom function (train_xgb).\n",
    "    final_model_xgb = train_xgb(X_train, targets, loc, 42)\n",
    "\n",
    "    # Make predictions on the test dataset using the trained model.\n",
    "    predictions_xgb = predict_model(final_model_xgb, data=X_test)\n",
    "    final_predictions_xgb = predictions_xgb['prediction_label']\n",
    "        \n",
    "    # Adjust the predictions by multiplying with the 'is_day:idx' feature.\n",
    "    final_predictions_xgb_filter = final_predictions_xgb * is_day_feature['is_day:idx']\n",
    "\n",
    "    # Clip the predictions to ensure they are non-negative.\n",
    "    final_predictions_xgb_filter = np.clip(final_predictions_xgb_filter, 0, None)\n",
    "\n",
    "    # Append the adjusted predictions for each location to the 'all_predictions_xb' list.\n",
    "    xb.append([final_predictions_xgb_filter])\n",
    "\n",
    "# Convert the list of predictions for all locations into a numpy array.\n",
    "xb = np.array(xb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e).flatten()\n",
    "all_predictions_xb = np.array(xb).flatten()\n",
    "all_predictions_cat = np.array(all_predictions_cat).flatten()\n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2).flatten()\n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3).flatten()\n",
    "all_pred = 0.2*all_predictions_cat+0.2 * all_predictions_lGBM_e+ 0.2* all_predictions_cat_2+0.2*all_predictions_cat_3 + 0.2*all_predictions_xb\n",
    "all_pred[all_pred < 6] = 0\n",
    "print(all_pred.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the final predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = all_pred\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df.to_csv('final_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('final_predictions_ok.csv')\n",
    "df_diff = df1[0:720] - df\n",
    "df_diff.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-name",
   "language": "python",
   "name": "pycaret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
