{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pycaret.regression import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD TIME FEATURES\n",
    "def add_time_features(df, time_column, mode = 'lgbm'):\n",
    "    '''\n",
    "        This function will add some time feature based on the param 'time_columns'\n",
    "        \n",
    "        Params:\n",
    "            df-> Dataframe with the column contained in 'time_column'\n",
    "            time_column -> the column that is a datetime object\n",
    "        \n",
    "        Returns:\n",
    "            A dataframe with time features\n",
    "    '''\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  # Make sure the time column is in datetime format\n",
    "    if mode == 'lgbm':\n",
    "        df['hour'] = df[time_column].dt.hour\n",
    "        df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "        df['month'] = df[time_column].dt.month\n",
    "        df['day_of_year'] = df[time_column].dt.dayofyear\n",
    "        df['week_of_year'] = df[time_column].dt.isocalendar().week\n",
    "        df['year'] = df[time_column].dt.year\n",
    "    elif mode == 'cat_boost':\n",
    "        df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "        df['sin_hour'] = np.sin(2*np.pi * df[time_column].dt.hour/23.)\n",
    "        df['sin_month'] = np.sin(2*np.pi * df[time_column].dt.month/12.)\n",
    "        df['cos_hour'] = np.cos(2*np.pi * df[time_column].dt.hour/23.)\n",
    "        df['cos_month'] = np.cos(2*np.pi * df[time_column].dt.month/12.)\n",
    "    elif mode == 'cat':\n",
    "        df['sin_hour'] = np.sin(2*np.pi * df[time_column].dt.hour/23.)\n",
    "        df['sin_month'] = np.sin(2*np.pi * df[time_column].dt.month/12.)   \n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(df,column):\n",
    "    '''\n",
    "        Make the column in datetime format\n",
    "    '''\n",
    "    return pd.to_datetime(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(df,column):\n",
    "    '''\n",
    "        Resample df to 1 hour using mean() as aggregator and drop rows where all columns are NaN\n",
    "        \n",
    "        Params :\n",
    "            df -> the dataframe to be resampled\n",
    "            column -> the time column\n",
    "    '''\n",
    "    return df.set_index(keys = column).resample('1H').mean().dropna(how='all').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df,columnlist):\n",
    "    return df.drop(columns = columnlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_calc(df):\n",
    "    '''\n",
    "    This function create a dataframe with 'date_forecast' as index and the column 'date_calc' resampled by '1H'.\n",
    "    If there's no data in a specific bin, the resulting value for that bin would be NaN (not a number).\n",
    "    Params:\n",
    "        df -> dataframe with 'date_forecast' and 'date_calc' columns.\n",
    "            'date_calc' is expected to contain data that the user wants to resample or analyze.\n",
    "    Returns:\n",
    "        A dataframe with 'date_calc' resampled.\n",
    "    '''\n",
    "    return df.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_estimated_feature(df):\n",
    "    '''\n",
    "        This function will create some time feature and estimated information. It's need to let the model understand is\n",
    "        estimated value.\n",
    "        Params:\n",
    "            df -> It MUST be an estimated dataframe, that contains 'data_forecast' as datetime type\n",
    "        Returns:\n",
    "            A dataframe with 'time_dummy', 'time_delta' and 'is_estimated'     \n",
    "    '''\n",
    "    df['time_delta'] = (df['date_calc'] - df['date_forecast']).dt.total_seconds() / 3600\n",
    "    df['is_estimated'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stationarity(df):\n",
    "    '''\n",
    "    Removes constant stretches of data within a DataFrame where the 'pv_measurement' column does not change.    \n",
    "    The function identifies blocks of data where the 'pv_measurement' stays constant for more than two consecutive\n",
    "    points and removes these blocks to address data stationarity.\n",
    "\n",
    "    params:\n",
    "        df -> DataFrame\n",
    "              A pandas DataFrame with a 'pv_measurement' column which contains the data from which to remove stationarity.\n",
    "        \n",
    "    return:\n",
    "        The DataFrame with constant stretches of data removed from the 'pv_measurement' column.\n",
    "    '''\n",
    "    \n",
    "    #Calculate the difference, this need for check the constant\n",
    "    df['diff'] = df['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    # Create an indicator for constant stretches\n",
    "    df['constant'] = (df['diff'] == 0).astype(int)\n",
    "\n",
    "    # Use the indicator to mark stretches. The diff() function here identifies change-points.\n",
    "    df['block'] = (df['constant'].diff() != 0).astype(int).cumsum()\n",
    "\n",
    "    # Get the size of each constant block\n",
    "    block_sizes = df.groupby('block')['constant'].sum()\n",
    "\n",
    "    # Identify blocks that are constant for more than N consecutive time points (in this case 2)\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "    \n",
    "    # Remove the constant\n",
    "    filtered_df = df[~df['block'].isin(constant_blocks)]\n",
    "        \n",
    "    return filtered_df.drop(columns=['diff', 'constant', 'block'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(targets, observed, estimated, test, mode: str = 'lgbm'):\n",
    "    '''\n",
    "        This function makes all the preprocessing needed for the correct run of the model, it will perform:\n",
    "            - Resampling\n",
    "            - Filtering\n",
    "            - Imputation\n",
    "            - Outliers removal\n",
    "            - Categorical Encoding\n",
    "        \n",
    "        Params:\n",
    "            targets -> dataframe of the target parquet\n",
    "            observed -> dataframe of observed train data\n",
    "            estimated -> dataframe of estimated train data\n",
    "            test -> dataframe of test data\n",
    "        Returns:\n",
    "            train_data -> dataframe of all data ready to train\n",
    "            test_data -> dataframe of all data ready to test\n",
    "            is_day -> dataframe of is_day categorical feature for post processing\n",
    "    \n",
    "    '''    \n",
    "    targets['time'] = to_datetime(targets,'time')\n",
    "    estimated['date_forecast'] = to_datetime(estimated,'date_forecast')\n",
    "    observed['date_forecast'] = to_datetime(observed,'date_forecast')\n",
    "    test['date_forecast'] = to_datetime(test,'date_forecast')\n",
    "\n",
    "    observed_resampled = resampling(observed,'date_forecast')\n",
    "    estimated_resampled = resampling(estimated,'date_forecast')\n",
    "    test_resampled = resampling(test,'date_forecast')\n",
    "    \n",
    "    date_calc_resampled_observed = extract_data_calc(estimated)\n",
    "    date_calc_resampled_test = extract_data_calc(test)\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_observed, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_test, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    is_day = test_resampled[['date_forecast', 'is_day:idx']]   \n",
    "    test_resampled = filter_df(test_resampled,['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = filter_df(observed_resampled,['is_day:idx', 'snow_density:kgm3','elevation:m']) \n",
    "    estimated_resampled = filter_df(estimated_resampled,[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    \n",
    "    #This MUST be zero because is not estimated.\n",
    "    observed_resampled['is_estimated'] = 0\n",
    "    observed_resampled['time_delta'] = 0\n",
    "    \n",
    "    estimated_resampled = is_estimated_feature(estimated_resampled)\n",
    "    test_resampled = is_estimated_feature(test_resampled)\n",
    "    \n",
    "    X = pd.concat([observed_resampled,estimated_resampled],axis = 0)\n",
    "    train_data = pd.merge(targets, X, how='inner', left_on='time', right_on='date_forecast')\n",
    "    if mode == 'lgbm':\n",
    "        train_data = add_time_features(train_data, 'time')\n",
    "        test_data = add_time_features(test_resampled, 'date_forecast')\n",
    "    elif mode == 'cat':\n",
    "        train_data = add_time_features(train_data, 'time', mode = 'cat')\n",
    "        test_data = add_time_features(test_resampled, 'date_forecast', mode = 'cat')\n",
    "        train_data = train_data[train_data['date_forecast'].dt.month.isin([4,5,6,7,8])]\n",
    "        test_data = test_data[test_data['date_forecast'].dt.month.isin([4,5,6,7,8])]\n",
    "    elif mode == 'cat_boost':\n",
    "        train_data = add_time_features(train_data, 'time', mode = 'cat_boost')\n",
    "        test_data = add_time_features(test_resampled, 'date_forecast', mode = 'cat_boost')\n",
    "\n",
    "    \n",
    "    train_data = delete_stationarity(train_data)\n",
    "    \n",
    "    train_data = filter_df(train_data, ['time','date_calc'])\n",
    "    test_data = filter_df(test_resampled, ['date_calc'])\n",
    "\n",
    "    return train_data, test_data, is_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with some extra features\n",
    "def process_location(X, y, location_name,seeds):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm, optimize='MAE')\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    # Added some extra features to this one model, did it here so we could reuse the same preprocesssing function on diffrent models\n",
    "    # Feature Combination 1: Solar Radiation and Cloud Cover Combination\n",
    "    data['weighted_rad'] = ((data['direct_rad:W'] * (1 - data['total_cloud_cover:p']/100)) +\n",
    "                        (data['diffuse_rad:W'] * (data['total_cloud_cover:p']/100)))\n",
    "\n",
    "    # Feature Combination 2: Atmospheric Conditions Combination\n",
    "    data['adjusted_clear_sky_rad'] = (data['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * data['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (data['air_density_2m:kgm3'] - 1.225)))  # Adjusted based on humidity and air density\n",
    "    data['solar_incidence_factor'] = np.cos(np.radians(90 - data['sun_elevation:d'])) * np.cos(np.radians(data['sun_azimuth:d']))\n",
    "    data['seasonal_conversion_efficiency'] = data['weighted_rad'] * (1 - data['relative_humidity_1000hPa:p']/100) * (data['msl_pressure:hPa'] / 1013.25)\n",
    "    \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global lists to save predictions in\n",
    "locations = ['A', 'B', 'C']\n",
    "all_predictions_lGBM = []\n",
    "all_predictions_lGBM_e = []\n",
    "all_predictions_rf = []\n",
    "all_predictions_lasso = []\n",
    "all_predictions_cat = []\n",
    "all_predictions_cat_2 = []\n",
    "final_df_list = [] \n",
    "all_pred_stacked =[]\n",
    "all_predictions_cat_3=[]\n",
    "\n",
    "all_X_train_cat = pd.DataFrame()\n",
    "all_X_test_cat = pd.DataFrame()\n",
    "all_is_day_feature1 = pd.Series(dtype='float64')\n",
    "all_targets_cat = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM training and predictions\n",
    "for loc in locations:\n",
    "\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Calling preprocessing\n",
    "    train, test, is_day_feature = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "   \n",
    "    targets = pd.DataFrame( {'pv_measurement': train['pv_measurement']})\n",
    "    X_train = train.drop(columns=['date_forecast','pv_measurement'])\n",
    "    X_train = feature_engineering(X_train)\n",
    "    X_test = test.drop(columns=['date_forecast'])\n",
    "    X_test = feature_engineering(X_test)\n",
    "    \n",
    "    # Training and prediction for diffrent seeds\n",
    "    total_predictions_light = None\n",
    "    seeds = [42,123]\n",
    "    for seed in seeds: \n",
    "        final_model_lGBM_e = process_location(X_train, targets, loc, seed)\n",
    "        predictions_lGBM_e = predict_model(final_model_lGBM_e, data=X_test)\n",
    "        final_predictions_lGBM_e = predictions_lGBM_e['prediction_label']\n",
    "        if total_predictions_light is None:\n",
    "            total_predictions_light = np.zeros_like(final_predictions_lGBM_e)\n",
    "        total_predictions_light += final_predictions_lGBM_e\n",
    "\n",
    "    mean_pred_light = total_predictions_light/len(seeds)\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_lGBM_e = mean_pred_light * is_day_feature['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_lGBM_e = np.clip(adjusted_final_predictions_lGBM_e, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_lGBM_e.append([adjusted_final_predictions_lGBM_e])\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['dew_or_rime:idx' ,'is_in_shadow:idx']\n",
    "cat_params = {\n",
    "    \"iterations\": 5000,\n",
    "    \"learning_rate\": 0.034867396508006264,\n",
    "    \"depth\": 8,\n",
    "    \"l2_leaf_reg\": 1,\n",
    "    \"loss_function\": \"MAE\",\n",
    "    \"border_count\": 92,\n",
    "    \"verbose\": 500,\n",
    "    \"subsample\": 0.7641850606486046,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'cat_features': cat_features,\n",
    "    'random_state': 42, \n",
    "}\n",
    "cat_params_no_feature = {\n",
    "    \"iterations\": 5000,\n",
    "    \"learning_rate\": 0.034867396508006264,\n",
    "    \"depth\": 8,\n",
    "    \"l2_leaf_reg\": 1,\n",
    "    \"loss_function\": \"MAE\",\n",
    "    \"border_count\": 92,\n",
    "    \"verbose\": 500,\n",
    "    \"subsample\": 0.7641850606486046,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'random_state': 42, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CATegorical(df):\n",
    "    df['dew_or_rime:idx'] = df['dew_or_rime:idx'].astype(int)\n",
    "    df['is_in_shadow:idx'] = df['is_in_shadow:idx'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import IterativeImputer\n",
    "# Cat_1 training and predictions\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train_cat, X_test_cat, is_day_feature1 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated,mode = 'cat')\n",
    "    X_train_cat.drop(columns=['date_forecast'], inplace=True)\n",
    "    print(f'Doing... {loc}')\n",
    "\n",
    "    imputer = IterativeImputer(max_iter=5, random_state=42)\n",
    "    for col in X_train_cat.columns:\n",
    "        X_train_cat[col] = imputer.fit_transform(np.array(X_train_cat[col]).reshape(-1,1))\n",
    "    for col in X_test_cat.columns:\n",
    "        X_test_cat[col] = imputer.fit_transform(np.array(X_test_cat[col]).reshape(-1,1))\n",
    "    \n",
    "    targets_cat = pd.DataFrame( {'pv_measurement': X_train_cat['pv_measurement']})\n",
    "    X_train_cat = X_train_cat.drop(columns=['pv_measurement'])\n",
    "    X_train_cat = CATegorical(X_train_cat)\n",
    "    X_test_cat = CATegorical(X_test_cat)\n",
    "\n",
    "    # Catboooooooozt fun\n",
    "    model_cat = CatBoostRegressor(**cat_params)\n",
    "\n",
    "    X_train_cat1, X_val_cat1, y_train_cat1, y_val_cat1 = train_test_split(X_train_cat, targets_cat, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Training\n",
    "    model_cat.fit(X_train_cat1, y_train_cat1['pv_measurement'],eval_set=(X_val_cat1, y_val_cat1['pv_measurement']),)\n",
    "\n",
    "    # Prediction\n",
    "    predictions_cat = model_cat.predict(X_test_cat[model_cat.feature_names_])\n",
    "    \n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat = predictions_cat * is_day_feature1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat = np.clip(adjusted_final_predictions_cat, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat.append(adjusted_final_predictions_cat)\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_cat = np.array(all_predictions_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost nr 3\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    X_train_3, X_test_3, is_day_feature_3 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    X_train_3.drop(columns=['date_forecast'], inplace=True)\n",
    "    imputer = IterativeImputer(max_iter=5, random_state=42)\n",
    "    for col in X_train_3.columns:\n",
    "        X_train_3[col] = imputer.fit_transform(np.array(X_train_3[col]).reshape(-1,1))\n",
    "    for col in X_test_3.columns:\n",
    "        X_test_3[col] = imputer.fit_transform(np.array(X_test_3[col]).reshape(-1,1))\n",
    "    targets_3 = pd.DataFrame( {'pv_measurement': X_train_3['pv_measurement']})\n",
    "    X_train_3 = X_train_3.drop(columns=['pv_measurement'])\n",
    "    \n",
    "    model_cat_3 = CatBoostRegressor(**cat_params_no_feature)\n",
    "    X_train_3 = feature_engineering(X_train_3)\n",
    "    X_test_3 = feature_engineering(X_test_3)\n",
    "\n",
    "    # Create 'sin_sun_azimuth' and 'cos_sun_azimuth' from 'sun_azimuth' in radians\n",
    "    X_train_3['sin_sun_azimuth'] = np.sin(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_train_3['cos_sun_azimuth'] = np.cos(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_test_3['sin_sun_azimuth'] = np.sin(np.radians(X_test_3['sun_azimuth:d']))\n",
    "    X_test_3['cos_sun_azimuth'] = np.cos(np.radians(X_test_3['sun_azimuth:d']))\n",
    "\n",
    "    # Now drop the original 'sun_azimuth' feature\n",
    "    X_train_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    X_test_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train_cat_3, X_test_cat_3, y_train_cat_3, y_test_cat_3 = train_test_split(X_train_3, targets_3, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model_cat_3.fit(X_train_cat_3, y_train_cat_3['pv_measurement'],eval_set=(X_test_cat_3, y_test_cat_3['pv_measurement']),)  \n",
    "    \n",
    "    # Pred\n",
    "    pred_cat_3 = model_cat_3.predict(X_test_3[model_cat_3.feature_names_])\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat_3 = pred_cat_3 * is_day_feature_3['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat_3 = np.clip(adjusted_final_predictions_cat_3, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat_3.append(adjusted_final_predictions_cat_3) \n",
    "\n",
    "# Changing final list to array   \n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "def process_location_rf(X, y, location_name):\n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "\n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=123,\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Random forest\n",
    "    rf= create_model('rf')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_rf = tune_model(rf)#, early_stopping=True, fold=15)\n",
    "    print(tuned_rf)\n",
    "    \n",
    "    # Create a boosted version of the tuned model\n",
    "    boosting_rf = ensemble_model(tuned_rf, method='Boosting')\n",
    "\n",
    "    # Finalize the model - this will train it on the complete dataset\n",
    "    final_model = finalize_model(boosting_rf)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_rf = []\n",
    "# Random forest\n",
    "for loc in locations:\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    X_train_rf, X_test_rf, is_day_feature_rf = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    \n",
    "    targets_rf = pd.DataFrame( {'pv_measurement': X_train_rf['pv_measurement']})\n",
    "    X_train_rf = X_train_rf.drop(columns=['pv_measurement'])\n",
    "    \n",
    "    \n",
    "    y_train = targets\n",
    "   \n",
    "    # Filter observed and estimated data for April to August\n",
    "    X_train_rf = X_train_rf[X_train_rf['date_forecast'].dt.month.isin([3, 4, 5, 6, 7, 8, 9])]\n",
    " \n",
    "    X_train_rf = X_train_rf.drop(columns=['date_forecast'])\n",
    "    \n",
    "    X_train_rf['sin_sun_azimuth'] = np.sin(np.radians(X_train_rf['sun_azimuth:d']))\n",
    "    X_train_rf['cos_sun_azimuth'] = np.cos(np.radians(X_train_rf['sun_azimuth:d']))\n",
    "    \n",
    "    X_test_rf['sin_sun_azimuth'] = np.sin(np.radians(X_test_rf['sun_azimuth:d']))\n",
    "    X_test_rf['cos_sun_azimuth'] = np.cos(np.radians(X_test_rf['sun_azimuth:d']))\n",
    "\n",
    "    # Now drop the original 'sun_azimuth' feature\n",
    "    X_train_rf.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    X_test_rf.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    \n",
    "    X_train_rf = feature_engineering(X_train_rf)\n",
    "    X_test_rf = feature_engineering(X_test_rf)\n",
    "    \n",
    "    final_model_rf = process_location_rf(X_train_rf, targets_rf, loc)\n",
    "    predictions_rf = predict_model(final_model_rf, data=X_test_rf)\n",
    "    \n",
    "    final_predictions_rf = predictions_rf['prediction_label']   \n",
    "    \n",
    "    # Multiply final predictions with the 'is_day:idx' values\n",
    "    adjusted_final_predictions_rf = final_predictions_rf * is_day_feature['is_day:idx']\n",
    "   \n",
    "    \n",
    "    # Cliping values under zero to zero   \n",
    "    adjusted_final_predictions_rf = np.clip(adjusted_final_predictions_rf, 0, None)    \n",
    "    # Store predictions   \n",
    "    all_predictions_rf.append([adjusted_final_predictions_rf]) \n",
    "     \n",
    "     \n",
    "    \n",
    " \n",
    "all_predictions_rf = np.array(all_predictions_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_cat_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e).flatten()\n",
    "all_predictions_rf = np.array(all_predictions_rf).flatten()\n",
    "all_predictions_cat = np.array(all_predictions_cat).flatten()\n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2).flatten()\n",
    "all_pred = 0.15*all_predictions_cat+0.30 * all_predictions_lGBM_e+0.40*all_predictions_cat_2+ 0.15*all_predictions_rf\n",
    "print(all_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_predictions = all_pred\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df.to_csv('submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-name",
   "language": "python",
   "name": "pycaret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
