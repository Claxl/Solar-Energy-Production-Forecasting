{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pycaret.regression import *\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the maximum number of rows when you printing out a DataFrame\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time features\n",
    "\n",
    "First, we made 3 sets of time features to get models that capture diffrent important aspects from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features nr 1\n",
    "def add_time_features(df, time_column):\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  \n",
    "\n",
    "    # Extract various time features\n",
    "    df['hour'] = df[time_column].dt.hour\n",
    "    df['day_of_week'] = df[time_column].dt.dayofweek\n",
    "    df['month'] = df[time_column].dt.month\n",
    "    df['day_of_year'] = df[time_column].dt.dayofyear\n",
    "    df['week_of_year'] = df[time_column].dt.isocalendar().week \n",
    "    df['year'] = df[time_column].dt.year\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time features nr 2\n",
    "def add_time_features_cat(df, time_column):\n",
    "    \n",
    "    df[time_column] = pd.to_datetime(df[time_column])  \n",
    "    \n",
    "    df['sin_hour'] = np.sin(np.pi * df[time_column].dt.hour/23.)\n",
    "    df['sin_month'] = np.sin(np.pi * df[time_column].dt.month/12.)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_est(observed, estimated, test):\n",
    "    \n",
    "    # 1. Create time-delta for estimated data\n",
    "      estimated['time_dummy'] = (estimated['date_forecast'] - estimated['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
    "      observed['time_dummy'] = 0 \n",
    "      test['time_dummy'] = (test['date_forecast'] - test['date_forecast'].dt.normalize()).dt.total_seconds() / 3600\n",
    "      \n",
    "      estimated['time_delta'] = (estimated['date_calc'] - estimated['date_forecast']).dt.total_seconds() / 3600\n",
    "      observed['time_delta'] = 0  # since observed data is not forecasting ahead\n",
    "      test['time_delta'] = (test['date_calc'] - test['date_forecast']).dt.total_seconds() / 3600\n",
    "      \n",
    "      # 2. Add indicator variable for estimated data\n",
    "      estimated['is_estimated'] = 1\n",
    "      observed['is_estimated'] = 0\n",
    "      test['is_estimated'] = 1\n",
    "      # Merge or concatenate data\n",
    "      df = pd.concat([observed, estimated], axis=0).sort_values(by='date_forecast')\n",
    "      \n",
    "      return df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stationary(df):\n",
    "    \n",
    "    # Removing data where the power output is saturated\n",
    "    # Step 1: Calculate the difference\n",
    "    df['diff'] = df['pv_measurement'].diff().fillna(0)\n",
    "\n",
    "    # Step 2: Create an indicator where diff is zero\n",
    "    df['constant'] = (df['diff'] == 0).astype(int)\n",
    "\n",
    "    # Step 3: Use the indicator where diff is zero. The diff() function here identifies change-points.\n",
    "    df['block'] = (df['constant'].diff() != 0).astype(int).cumsum()\n",
    "    block_sizes = df.groupby('block')['constant'].sum()\n",
    "\n",
    "    # Identify blocks that are constant for more than 2 consecutive time points (you can adjust this threshold)\n",
    "    constant_blocks = block_sizes[block_sizes > 2].index\n",
    "\n",
    "    # Step 4: Remove the constant where diff is zero\n",
    "    filtered_data = df[~df['block'].isin(constant_blocks)]\n",
    "\n",
    "    # Drop time and temporary features\n",
    "    targets_ny = filtered_data[ ['time', 'pv_measurement']]\n",
    "    filtered_data = filtered_data.drop(columns=['diff', 'constant', 'block'])\n",
    "    return filtered_data, targets_ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing LigtGBM + catboost\n",
    "def preprocessing(targets, observed, estimated, test):\n",
    "    \n",
    "    # Ensure the datetime columns are in datetime format\n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "\n",
    "    date_calc_resampled_ob = estimated.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    date_calc_resampled_te = test.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    \n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_ob, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_te, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    #Save the is_day feature as this says a lot about when the power output is zero or not\n",
    "    is_day_feature = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    \n",
    "    #Drop some features that is noise\n",
    "    test_resampled = test_resampled.drop(columns =['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = observed_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    estimated_resampled = estimated_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "\n",
    "    \n",
    "    # Add extra features\n",
    "    weather_data, test_resampled = is_est(observed_resampled, estimated_resampled, test_resampled)\n",
    "    \n",
    "    # Merge with target values\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "\n",
    "    # Add the time-based features\n",
    "    merged_data = add_time_features(merged_data, 'time')  \n",
    "    test_resampled = add_time_features(test_resampled, 'date_forecast') \n",
    "    \n",
    "    filtered_data, targets_ny = delete_stationary(merged_data)\n",
    "    \n",
    "    # Drop time features\n",
    "    filtered_data = filtered_data.drop(columns=['time', 'pv_measurement','date_calc'])\n",
    "    test_resampled = test_resampled.drop(columns=[ 'date_forecast', 'date_calc'])\n",
    "    \n",
    "    return filtered_data, test_resampled, is_day_feature, targets_ny"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for catboost models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_est_cat(observed, estimated, test):\n",
    "      # Add indicator variable for estimated data\n",
    "      estimated['is_estimated'] = 1\n",
    "      observed['is_estimated'] = 0\n",
    "      test['is_estimated'] = 1\n",
    "\n",
    "      # Merge or concatenate data\n",
    "      df = pd.concat([observed, estimated], axis=0).sort_values(by='date_forecast')\n",
    "      \n",
    "      return df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing nr 2 for catboost models\n",
    "def preprocessing_cat(targets, observed, estimated, test):\n",
    "    \n",
    "    targets['time'] = pd.to_datetime(targets['time'])\n",
    "    observed['date_forecast'] = pd.to_datetime(observed['date_forecast'])\n",
    "    estimated['date_forecast'] = pd.to_datetime(estimated['date_forecast'])\n",
    "    test['date_forecast'] = pd.to_datetime(test['date_forecast'])\n",
    "\n",
    "    # Start the resampling from 15min to 1 hour\n",
    "    date_calc_resampled_ob = estimated.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    date_calc_resampled_te = test.set_index('date_forecast')['date_calc'].resample('1H').first().to_frame()\n",
    "    \n",
    "    observed_resampled = observed.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    estimated_resampled = estimated.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    test_resampled = test.set_index('date_forecast').resample('1H').mean().dropna(how='all').reset_index()\n",
    "    \n",
    "    estimated_resampled = estimated_resampled.merge(date_calc_resampled_ob, left_on='date_forecast', right_index=True)\n",
    "    test_resampled = test_resampled.merge(date_calc_resampled_te, left_on='date_forecast', right_index=True)\n",
    "    \n",
    "    #Save the is_day feature as this says a lot about when the power output is zero or not\n",
    "    is_day_feature = test_resampled[['date_forecast', 'is_day:idx']]\n",
    "    \n",
    "    #Drop some features that is noise\n",
    "    test_resampled = test_resampled.drop(columns =['is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    observed_resampled = observed_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "    estimated_resampled = estimated_resampled.drop(columns =[ 'is_day:idx', 'snow_density:kgm3','elevation:m'])\n",
    "\n",
    "    # Filter observed and estimated data for April to August\n",
    "    observed_resampled = observed_resampled[observed_resampled['date_forecast'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    estimated_resampled = estimated_resampled[estimated_resampled['date_forecast'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "    targets = targets[targets['time'].dt.month.isin([4, 5, 6, 7, 8])]\n",
    "\n",
    "    # Merge the observed and estimated data\n",
    "    weather_data, test_resampled = is_est_cat(observed_resampled, estimated_resampled, test_resampled)\n",
    "\n",
    "    # Merge with target values filtering for the same months\n",
    "    merged_data = pd.merge(targets, weather_data, how='inner', left_on='time', right_on='date_forecast')\n",
    "    merged_data = add_time_features_cat(merged_data, 'time')  \n",
    "    test_resampled = add_time_features_cat(test_resampled, 'date_forecast')\n",
    "\n",
    "    filtered_data, targets_ny = delete_stationary(merged_data)\n",
    "    \n",
    "    # Drop some time features\n",
    "    filtered_data = filtered_data.drop(columns=['time', 'date_forecast', 'pv_measurement','date_calc'])\n",
    "    test_resampled = test_resampled.drop(columns=[ 'date_forecast','date_calc'])\n",
    "    \n",
    "    return filtered_data, test_resampled, is_day_feature, targets_ny"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM with some extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with some extra features\n",
    "def process_location_ex(X, y, location_name,seeds):\n",
    "    \n",
    "    # Combine feature data and target into a single DataFrame\n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "\n",
    "    # Added some extra features to this one model, did it here so we could reuse the same preprocesssing function on diffrent models\n",
    "    # Feature Combination 1: Solar Radiation and Cloud Cover Combination\n",
    "    data['weighted_rad'] = ((data['direct_rad:W'] * (1 - data['total_cloud_cover:p']/100)) +\n",
    "                        (data['diffuse_rad:W'] * (data['total_cloud_cover:p']/100)))\n",
    "\n",
    "    # Feature Combination 2: Atmospheric Conditions Combination\n",
    "    data['adjusted_clear_sky_rad'] = (data['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * data['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (data['air_density_2m:kgm3'] - 1.225)))  # Adjusted based on humidity and air density\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "\n",
    "                    categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    #remove_outliers=True,  #Ble dårligere med denne\n",
    "                    html=False, \n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a LightGBM model\n",
    "    lightgbm = create_model('lightgbm')\n",
    "    \n",
    "    # Tune the model\n",
    "    tuned_lightgbm = tune_model(lightgbm)#, early_stopping=True, fold=15)\n",
    "\n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_lightgbm = ensemble_model(tuned_lightgbm, method='Bagging')\n",
    "\n",
    "    # Finalize the model by training on whole dataset\n",
    "    final_model = finalize_model(bagged_lightgbm)\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost model nr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost model nr 2\n",
    "def process_location_cat_2(X, y, location_name,seeds):\n",
    "    \n",
    "    # Dropping some features for this one model\n",
    "    features_to_drop = ['dew_or_rime:idx', #'snow_density:kgm3',\n",
    "                        'fresh_snow_3h:cm', 'fresh_snow_1h:cm', 'snow_drift:idx', \n",
    "                        'snow_depth:cm', 'wind_speed_w_1000hPa:ms', 'prob_rime:p', \n",
    "                        'fresh_snow_6h:cm', 'snow_melt_10min:mm', \n",
    "                        'fresh_snow_12h:cm', 'rain_water:kgm2', \n",
    "                        'super_cooled_liquid_water:kgm2']\n",
    "    \n",
    "    X = X.drop(columns=features_to_drop)\n",
    "    \n",
    "    data = X.copy()\n",
    "    data['target'] = y['pv_measurement']\n",
    "    \n",
    "    # Setup the environment in PyCaret\n",
    "    exp_reg = setup(data=data, target='target', session_id=seeds,\n",
    "                    imputation_type=\"iterative\", categorical_iterative_imputer=\"lightgbm\", numeric_iterative_imputer=\"lightgbm\", iterative_imputation_iters = 5,\n",
    "\n",
    "                    #categorical_features=['dew_or_rime:idx', 'is_in_shadow:idx','is_estimated'],\n",
    "                    #remove_outliers=True,  #Ble dårligere med denne\n",
    "                    html=False,\n",
    "                    experiment_name=f'exp_{location_name}')\n",
    "\n",
    "    # Create a Catboost model\n",
    "    cat = create_model('catboost')\n",
    "\n",
    "    # Tune the model\n",
    "    tuned_cat = tune_model(cat)\n",
    "    \n",
    "    # Create a bagged version of the tuned model\n",
    "    bagged_cat = ensemble_model(tuned_cat, method='Bagging')\n",
    "\n",
    "    # Train on whole dataset\n",
    "    final_model = finalize_model(bagged_cat)\n",
    "\n",
    "    # Save the model for future use\n",
    "    save_model(final_model, f'final_model_for_location_{location_name}')\n",
    "        \n",
    "    return final_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing lists and dataframes for storing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global lists to save predictions in\n",
    "locations = ['A']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and predictions\n",
    "\n",
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8668\n",
      "[LightGBM] [Info] Number of data points in the train set: 12714, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 1799.064605\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8631\n",
      "[LightGBM] [Info] Number of data points in the train set: 10988, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 3135.630038\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8668\n",
      "[LightGBM] [Info] Number of data points in the train set: 12714, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 1799.064605\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8631\n",
      "[LightGBM] [Info] Number of data points in the train set: 10988, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 3135.630038\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8668\n",
      "[LightGBM] [Info] Number of data points in the train set: 12714, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 1799.064605\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8631\n",
      "[LightGBM] [Info] Number of data points in the train set: 10988, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 3135.630038\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8668\n",
      "[LightGBM] [Info] Number of data points in the train set: 12714, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 1799.064605\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8631\n",
      "[LightGBM] [Info] Number of data points in the train set: 10988, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 3135.630038\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8668\n",
      "[LightGBM] [Info] Number of data points in the train set: 12714, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 1799.064605\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003001 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8631\n",
      "[LightGBM] [Info] Number of data points in the train set: 10988, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 3135.630038\n",
      "                        Description        Value\n",
      "0                        Session id           42\n",
      "1                            Target       target\n",
      "2                       Target type   Regression\n",
      "3               Original data shape  (19622, 54)\n",
      "4            Transformed data shape  (19622, 66)\n",
      "5       Transformed train set shape  (13735, 66)\n",
      "6        Transformed test set shape   (5887, 66)\n",
      "7                  Ordinal features            1\n",
      "8                  Numeric features           50\n",
      "9              Categorical features            3\n",
      "10         Rows with missing values        20.2%\n",
      "11                       Preprocess         True\n",
      "12                  Imputation type    iterative\n",
      "13  Iterative imputation iterations            5\n",
      "14        Numeric iterative imputer     lightgbm\n",
      "15    Categorical iterative imputer     lightgbm\n",
      "16         Maximum one-hot encoding           25\n",
      "17                  Encoding method         None\n",
      "18                   Fold Generator        KFold\n",
      "19                      Fold Number           10\n",
      "20                         CPU Jobs           -1\n",
      "21                          Use GPU        False\n",
      "22                   Log Experiment        False\n",
      "23                  Experiment Name        exp_A\n",
      "24                              USI         b9c0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# LightGBM training and predictions\n",
    "all_predictions_lGBM_e = []\n",
    "for loc in locations:\n",
    "\n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "    \n",
    "    # Calling preprocessing\n",
    "    X_train_1, X_test_1, is_day_feature_1, targets_1 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "    \n",
    "    # Adding the extra features to the test set as well\n",
    "    X_train_1 = X_train_1.drop(columns=['date_forecast'])\n",
    "    X_test_1['weighted_rad'] = ((X_test_1['direct_rad:W'] * (1 - X_test_1['total_cloud_cover:p']/100)) +\n",
    "                        (X_test_1['diffuse_rad:W'] * (X_test_1['total_cloud_cover:p']/100)))\n",
    "\n",
    "    X_test_1['adjusted_clear_sky_rad'] = (X_test_1['clear_sky_rad:W'] *\n",
    "                                  np.exp(-0.0001 * X_test_1['absolute_humidity_2m:gm3']) *\n",
    "                                  (1 - 0.1 * (X_test_1['air_density_2m:kgm3'] - 1.225)))\n",
    "    \n",
    "    # Training and prediction for diffrent seeds\n",
    "    total_predictions_light = None\n",
    "    seeds = [42]\n",
    "    for seed in seeds: \n",
    "        final_model_lGBM_e = process_location_ex(X_train_1, targets_1, loc, seed)\n",
    "        predictions_lGBM_e = predict_model(final_model_lGBM_e, data=X_test_1)\n",
    "        final_predictions_lGBM_e = predictions_lGBM_e['prediction_label']\n",
    "        if total_predictions_light is None:\n",
    "            total_predictions_light = np.zeros_like(final_predictions_lGBM_e)\n",
    "        total_predictions_light += final_predictions_lGBM_e\n",
    "\n",
    "    mean_pred_light = total_predictions_light/len(seeds)\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_lGBM_e = mean_pred_light * is_day_feature_1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_lGBM_e = np.clip(adjusted_final_predictions_lGBM_e, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_lGBM_e.append([adjusted_final_predictions_lGBM_e])\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost model nr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset starts from 2019-06-02 22:00:00 and ends at 2023-04-30 23:00:00\n",
      "0:\tlearn: 1098.0450846\ttotal: 170ms\tremaining: 2m 50s\n",
      "200:\tlearn: 364.8966719\ttotal: 10s\tremaining: 39.9s\n",
      "400:\tlearn: 328.6032161\ttotal: 18.8s\tremaining: 28.1s\n",
      "600:\tlearn: 306.8253447\ttotal: 27.7s\tremaining: 18.4s\n",
      "800:\tlearn: 291.9455358\ttotal: 36.4s\tremaining: 9.04s\n",
      "999:\tlearn: 280.1029739\ttotal: 45.7s\tremaining: 0us\n",
      "The dataset starts from 2018-12-31 23:00:00 and ends at 2023-04-30 23:00:00\n",
      "0:\tlearn: 218.7008398\ttotal: 38.8ms\tremaining: 38.8s\n",
      "200:\tlearn: 62.6422414\ttotal: 8.56s\tremaining: 34s\n",
      "400:\tlearn: 55.2251326\ttotal: 17s\tremaining: 25.4s\n",
      "600:\tlearn: 50.7912516\ttotal: 25.3s\tremaining: 16.8s\n",
      "800:\tlearn: 47.4006980\ttotal: 33.4s\tremaining: 8.31s\n",
      "999:\tlearn: 44.7965259\ttotal: 41.6s\tremaining: 0us\n",
      "The dataset starts from 2018-12-31 23:00:00 and ends at 2023-04-30 23:00:00\n",
      "0:\tlearn: 180.1584679\ttotal: 32.3ms\tremaining: 32.2s\n",
      "200:\tlearn: 52.7492691\ttotal: 8.3s\tremaining: 33s\n",
      "400:\tlearn: 45.5206549\ttotal: 16.6s\tremaining: 24.8s\n",
      "600:\tlearn: 40.9247729\ttotal: 24.8s\tremaining: 16.5s\n",
      "800:\tlearn: 38.0035210\ttotal: 32.8s\tremaining: 8.15s\n",
      "999:\tlearn: 35.6636235\ttotal: 40.7s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "all_predictions_cat = []\n",
    "# Catboost model nr 1\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train_cat, X_test_cat, is_day_feature1, targets_cat = preprocessing_cat(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Making categorical features\n",
    "    cat_features = ['dew_or_rime:idx' ,'is_in_shadow:idx']\n",
    "    X_train_cat['dew_or_rime:idx'] = X_train_cat['dew_or_rime:idx'].astype(int)\n",
    "    X_train_cat['is_in_shadow:idx'] = X_train_cat['is_in_shadow:idx'].astype(int)\n",
    "    X_test_cat['dew_or_rime:idx'] = X_test_cat['dew_or_rime:idx'].astype(int)\n",
    "    X_test_cat['is_in_shadow:idx'] = X_test_cat['is_in_shadow:idx'].astype(int)\n",
    "\n",
    "    # Catboooooooozt fun\n",
    "    model_cat = CatBoostRegressor(\n",
    "        loss_function='MAE', \n",
    "        learning_rate=0.1, \n",
    "        verbose=200,\n",
    "        cat_features=cat_features,\n",
    "        random_state=42) \n",
    "        #n_estimators=20000,\n",
    "        #early_stopping_rounds=50,)\n",
    "\n",
    "    #X_train_cat1, X_val_cat1, y_train_cat1, y_val_cat1 = train_test_split(X_train_cat, targets_cat, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Training\n",
    "    model_cat.fit(X_train_cat,targets_cat['pv_measurement']) #X_train_cat1, y_train_cat1['pv_measurement'],eval_set=(X_val_cat1, y_val_cat1['pv_measurement']),)\n",
    "\n",
    "    # Prediction\n",
    "    predictions_cat = model_cat.predict(X_test_cat)\n",
    "    feature_importances = model_cat.get_feature_importance()\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat = predictions_cat * is_day_feature1['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat = np.clip(adjusted_final_predictions_cat, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat.append(adjusted_final_predictions_cat)\n",
    "\n",
    "# Changing final list to array\n",
    "all_predictions_cat = np.array(all_predictions_cat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost model nr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset starts from 2019-06-02 22:00:00 and ends at 2023-04-30 23:00:00\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002576 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7013\n",
      "[LightGBM] [Info] Number of data points in the train set: 11699, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1772.496152\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6993\n",
      "[LightGBM] [Info] Number of data points in the train set: 10159, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3107.896390\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7013\n",
      "[LightGBM] [Info] Number of data points in the train set: 11699, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1772.496152\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6993\n",
      "[LightGBM] [Info] Number of data points in the train set: 10159, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3107.896390\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002729 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7013\n",
      "[LightGBM] [Info] Number of data points in the train set: 11699, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1772.496152\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6993\n",
      "[LightGBM] [Info] Number of data points in the train set: 10159, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3107.896390\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7013\n",
      "[LightGBM] [Info] Number of data points in the train set: 11699, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1772.496152\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6993\n",
      "[LightGBM] [Info] Number of data points in the train set: 10159, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3107.896390\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7013\n",
      "[LightGBM] [Info] Number of data points in the train set: 11699, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1772.496152\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6993\n",
      "[LightGBM] [Info] Number of data points in the train set: 10159, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3107.896390\n",
      "                        Description        Value\n",
      "0                        Session id           42\n",
      "1                            Target       target\n",
      "2                       Target type   Regression\n",
      "3               Original data shape  (18038, 40)\n",
      "4            Transformed data shape  (18038, 40)\n",
      "5       Transformed train set shape  (12626, 40)\n",
      "6        Transformed test set shape   (5412, 40)\n",
      "7                  Numeric features           39\n",
      "8          Rows with missing values        19.9%\n",
      "9                        Preprocess         True\n",
      "10                  Imputation type    iterative\n",
      "11  Iterative imputation iterations            5\n",
      "12        Numeric iterative imputer     lightgbm\n",
      "13    Categorical iterative imputer     lightgbm\n",
      "14                   Fold Generator        KFold\n",
      "15                      Fold Number           10\n",
      "16                         CPU Jobs           -1\n",
      "17                          Use GPU        False\n",
      "18                   Log Experiment        False\n",
      "19                  Experiment Name        exp_A\n",
      "20                              USI         db33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           MAE          MSE      RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                         \n",
      "0     331.5347  280088.3061  529.2337  0.8515  0.9573  4.4094\n",
      "1     329.8633  282572.0687  531.5751  0.8523  0.9035  2.8594\n",
      "2     330.1709  281579.4527  530.6406  0.8581  0.9094  3.5294\n",
      "3     340.2220  302901.1310  550.3645  0.8468  0.9635  4.2181\n",
      "4     311.7154  258253.4291  508.1864  0.8589  0.9887  4.1276\n",
      "5     332.0868  275836.8078  525.2017  0.8565  0.9528  3.9372\n",
      "6     327.8728  288634.4182  537.2471  0.8338  0.9640  3.7622\n",
      "7     343.9738  296294.5591  544.3295  0.8475  0.9642  4.1479\n",
      "8     297.9808  225688.0367  475.0663  0.8745  0.9624  4.6959\n",
      "9     317.6170  273433.8230  522.9090  0.8555  0.9261  3.9950\n",
      "Mean  326.3038  276528.2032  525.4754  0.8535  0.9492  3.9682\n",
      "Std    12.9897   20582.1803   20.0952  0.0099  0.0258  0.4807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n",
      "           MAE          MSE      RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                         \n",
      "0     373.7750  347800.1610  589.7458  0.8155  0.9998  5.4426\n",
      "1     376.3020  357862.4528  598.2161  0.8129  0.9240  3.4955\n",
      "2     372.8907  353602.6585  594.6450  0.8219  0.9237  2.8008\n",
      "3     369.0966  350865.7319  592.3392  0.8226  0.9733  4.1112\n",
      "4     350.0916  314306.3659  560.6303  0.8283  1.0124  4.6743\n",
      "5     383.1707  355459.8671  596.2046  0.8151  0.9840  4.2884\n",
      "6     374.6368  349193.2898  590.9258  0.7989  1.0071  5.1602\n",
      "7     394.0195  374185.4020  611.7070  0.8074  0.9794  3.5532\n",
      "8     327.8279  282676.6929  531.6735  0.8428  0.9712  4.4658\n",
      "9     362.1430  330657.0625  575.0279  0.8252  0.9519  4.1905\n",
      "Mean  368.3954  341660.9684  584.1115  0.8191  0.9727  4.2183\n",
      "Std    17.4524   24861.6143   21.7879  0.0115  0.0298  0.7514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           MAE          MSE      RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                         \n",
      "0     330.8393  280379.3867  529.5086  0.8513  0.9278  3.9378\n",
      "1     331.4055  288341.1455  536.9741  0.8492  0.8880  2.8825\n",
      "2     329.2066  290338.0350  538.8302  0.8537  0.8742  2.9036\n",
      "3     335.5599  298457.0409  546.3122  0.8491  0.8987  3.7164\n",
      "4     309.5971  261297.1835  511.1724  0.8572  0.9686  3.5521\n",
      "5     334.3443  281169.8822  530.2545  0.8537  0.8949  3.3587\n",
      "6     330.4999  288589.0153  537.2048  0.8338  0.9092  3.3626\n",
      "7     344.4032  303321.0872  550.7459  0.8439  0.9245  3.4580\n",
      "8     295.8456  228783.1977  478.3129  0.8728  0.9317  3.9928\n",
      "9     314.2570  266088.7083  515.8379  0.8594  0.9046  3.1702\n",
      "Mean  325.5958  278676.4682  527.5154  0.8524  0.9122  3.4335\n",
      "Std    13.7749   20700.8013   20.1002  0.0096  0.0256  0.3642\n",
      "Transformation Pipeline and Model Successfully Saved\n",
      "The dataset starts from 2018-12-31 23:00:00 and ends at 2023-04-30 23:00:00\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002218 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6942\n",
      "[LightGBM] [Info] Number of data points in the train set: 7996, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1949.476387\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6930\n",
      "[LightGBM] [Info] Number of data points in the train set: 7166, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2993.639074\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6942\n",
      "[LightGBM] [Info] Number of data points in the train set: 7996, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1949.476387\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6930\n",
      "[LightGBM] [Info] Number of data points in the train set: 7166, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2993.639074\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6942\n",
      "[LightGBM] [Info] Number of data points in the train set: 7996, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1949.476387\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6930\n",
      "[LightGBM] [Info] Number of data points in the train set: 7166, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2993.639074\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6942\n",
      "[LightGBM] [Info] Number of data points in the train set: 7996, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1949.476387\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6930\n",
      "[LightGBM] [Info] Number of data points in the train set: 7166, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 2993.639074\n",
      "                        Description        Value\n",
      "0                        Session id           42\n",
      "1                            Target       target\n",
      "2                       Target type   Regression\n",
      "3               Original data shape  (12320, 40)\n",
      "4            Transformed data shape  (12320, 40)\n",
      "5       Transformed train set shape   (8624, 40)\n",
      "6        Transformed test set shape   (3696, 40)\n",
      "7                  Numeric features           39\n",
      "8          Rows with missing values        17.1%\n",
      "9                        Preprocess         True\n",
      "10                  Imputation type    iterative\n",
      "11  Iterative imputation iterations            5\n",
      "12        Numeric iterative imputer     lightgbm\n",
      "13    Categorical iterative imputer     lightgbm\n",
      "14                   Fold Generator        KFold\n",
      "15                      Fold Number           10\n",
      "16                         CPU Jobs           -1\n",
      "17                          Use GPU        False\n",
      "18                   Log Experiment        False\n",
      "19                  Experiment Name        exp_B\n",
      "20                              USI         93f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MAE        MSE     RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                     \n",
      "0     55.2925  8195.9723  90.5316  0.8887  0.7858  2.6975\n",
      "1     53.4155  7351.7509  85.7424  0.8916  0.7345  1.9512\n",
      "2     53.7811  7897.9747  88.8706  0.8881  0.7448  2.6786\n",
      "3     52.5334  7244.4916  85.1146  0.8917  0.7109  1.2701\n",
      "4     52.3604  7499.6344  86.6004  0.8886  0.7334  1.6543\n",
      "5     57.6694  9346.7952  96.6788  0.8704  0.7365  1.5813\n",
      "6     52.9883  7892.9986  88.8425  0.8766  0.7724  1.7505\n",
      "7     47.2678  6279.9268  79.2460  0.9005  0.7849  2.0325\n",
      "8     50.8885  7776.8520  88.1865  0.8783  0.6928  1.7984\n",
      "9     52.4412  8269.1442  90.9348  0.8814  0.7160  1.6146\n",
      "Mean  52.8638  7775.5541  88.0748  0.8856  0.7412  1.9029\n",
      "Std    2.5673   754.4953   4.2872  0.0084  0.0299  0.4399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n",
      "          MAE         MSE      RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                       \n",
      "0     64.6567  10419.5884  102.0764  0.8585  0.8909  2.5365\n",
      "1     64.4230   9838.8150   99.1908  0.8549  0.8234  2.2928\n",
      "2     62.6430   9648.3255   98.2259  0.8633  0.8418  3.1937\n",
      "3     61.1695   8997.7021   94.8562  0.8655  0.7800  1.6399\n",
      "4     60.0173   8831.9214   93.9783  0.8688  0.7910  2.0048\n",
      "5     65.3801  10933.2821  104.5623  0.8484  0.8238  1.8896\n",
      "6     62.7953  10104.8497  100.5229  0.8420  0.8259  2.2908\n",
      "7     55.8646   7852.5380   88.6145  0.8755  0.8770  2.6198\n",
      "8     59.0839   9425.5016   97.0850  0.8525  0.7849  1.5691\n",
      "9     61.0257  10251.3966  101.2492  0.8530  0.7976  1.8593\n",
      "Mean  61.7059   9630.3921   98.0362  0.8582  0.8236  2.1896\n",
      "Std    2.7614    849.4392    4.3936  0.0096  0.0358  0.4749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MAE        MSE     RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                     \n",
      "0     55.3481  8157.0904  90.3166  0.8892  0.7818  2.3528\n",
      "1     53.0148  7226.8425  85.0108  0.8934  0.7221  2.2099\n",
      "2     54.5020  8022.0094  89.5657  0.8863  0.7349  2.3513\n",
      "3     52.6041  7394.3988  85.9907  0.8895  0.6614  1.1496\n",
      "4     52.5246  7497.5845  86.5886  0.8886  0.7051  1.4672\n",
      "5     57.5614  9372.7464  96.8129  0.8700  0.7073  1.4039\n",
      "6     53.4591  8020.5427  89.5575  0.8746  0.7271  1.5350\n",
      "7     47.2117  6083.6467  77.9977  0.9036  0.7497  1.8689\n",
      "8     50.2438  7662.7244  87.5370  0.8801  0.6814  1.5604\n",
      "9     52.8751  8459.5152  91.9756  0.8787  0.6919  1.4282\n",
      "Mean  52.9345  7789.7101  88.1353  0.8854  0.7163  1.7327\n",
      "Std    2.6504   817.4389   4.6772  0.0093  0.0331  0.4115\n",
      "Transformation Pipeline and Model Successfully Saved\n",
      "The dataset starts from 2018-12-31 23:00:00 and ends at 2023-04-30 23:00:00\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6881\n",
      "[LightGBM] [Info] Number of data points in the train set: 6310, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1857.903174\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6845\n",
      "[LightGBM] [Info] Number of data points in the train set: 5371, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3360.778939\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6881\n",
      "[LightGBM] [Info] Number of data points in the train set: 6310, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1857.903174\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6845\n",
      "[LightGBM] [Info] Number of data points in the train set: 5371, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3360.778939\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6881\n",
      "[LightGBM] [Info] Number of data points in the train set: 6310, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 1857.903174\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6845\n",
      "[LightGBM] [Info] Number of data points in the train set: 5371, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 3360.778939\n",
      "                        Description       Value\n",
      "0                        Session id          42\n",
      "1                            Target      target\n",
      "2                       Target type  Regression\n",
      "3               Original data shape  (9957, 40)\n",
      "4            Transformed data shape  (9957, 40)\n",
      "5       Transformed train set shape  (6969, 40)\n",
      "6        Transformed test set shape  (2988, 40)\n",
      "7                  Numeric features          39\n",
      "8          Rows with missing values       23.0%\n",
      "9                        Preprocess        True\n",
      "10                  Imputation type   iterative\n",
      "11  Iterative imputation iterations           5\n",
      "12        Numeric iterative imputer    lightgbm\n",
      "13    Categorical iterative imputer    lightgbm\n",
      "14                   Fold Generator       KFold\n",
      "15                      Fold Number          10\n",
      "16                         CPU Jobs          -1\n",
      "17                          Use GPU       False\n",
      "18                   Log Experiment       False\n",
      "19                  Experiment Name       exp_C\n",
      "20                              USI        3777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MAE        MSE     RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                     \n",
      "0     44.8659  5268.6844  72.5857  0.8859  0.5239  0.5684\n",
      "1     46.1461  5759.6166  75.8921  0.8595  0.4926  0.5369\n",
      "2     45.3551  5537.7865  74.4163  0.8883  0.5426  0.5980\n",
      "3     44.9488  5252.4417  72.4737  0.8953  0.5338  0.5999\n",
      "4     49.0496  6176.6203  78.5915  0.8697  0.5570  0.7175\n",
      "5     47.1803  5853.1389  76.5058  0.8781  0.5294  0.5987\n",
      "6     45.4758  5118.4743  71.5435  0.8872  0.5676  0.7396\n",
      "7     43.7447  5454.5422  73.8549  0.8800  0.5243  0.5232\n",
      "8     48.5103  6544.9285  80.9007  0.8691  0.5110  0.5280\n",
      "9     43.9500  5093.3190  71.3675  0.8828  0.5181  0.5822\n",
      "Mean  45.9227  5605.9552  74.8132  0.8796  0.5300  0.5992\n",
      "Std    1.7125   453.7368   2.9906  0.0102  0.0207  0.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n",
      "          MAE        MSE     RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                     \n",
      "0     53.3827  7200.2031  84.8540  0.8440  0.6493  0.9292\n",
      "1     54.3933  7370.2562  85.8502  0.8202  0.6951  0.9240\n",
      "2     54.2086  7450.0224  86.3135  0.8498  0.6261  0.7692\n",
      "3     52.3703  6536.3136  80.8475  0.8697  0.6301  0.7945\n",
      "4     55.3366  7307.3833  85.4832  0.8459  0.6681  0.9720\n",
      "5     54.6105  7152.4735  84.5723  0.8511  0.6218  0.6945\n",
      "6     51.7842  6267.6417  79.1684  0.8619  0.6677  1.0852\n",
      "7     53.2772  7268.7824  85.2572  0.8401  0.6341  0.7491\n",
      "8     55.3948  8029.9917  89.6102  0.8395  0.6178  0.7265\n",
      "9     52.6034  6550.2556  80.9336  0.8493  0.6561  0.8468\n",
      "Mean  53.7362  7113.3324  84.2890  0.8471  0.6466  0.8491\n",
      "Std    1.1843   495.5534   2.9485  0.0126  0.0238  0.1188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MAE        MSE     RMSE      R2   RMSLE    MAPE\n",
      "Fold                                                     \n",
      "0     45.2991  5414.5559  73.5837  0.8827  0.5139  0.5651\n",
      "1     46.4801  6160.3509  78.4879  0.8497  0.4966  0.5301\n",
      "2     45.0682  5489.9397  74.0941  0.8893  0.5237  0.5914\n",
      "3     44.9686  5418.9988  73.6138  0.8919  0.5259  0.5868\n",
      "4     49.0807  6354.1695  79.7130  0.8660  0.5666  0.7223\n",
      "5     46.2135  5647.0769  75.1470  0.8824  0.5231  0.5988\n",
      "6     44.8437  5064.7652  71.1672  0.8884  0.5606  0.7628\n",
      "7     44.7537  5785.2334  76.0607  0.8727  0.4802  0.5115\n",
      "8     47.4073  6294.5041  79.3379  0.8742  0.4822  0.5201\n",
      "9     43.9148  5090.3176  71.3465  0.8829  0.5235  0.5990\n",
      "Mean  45.8030  5671.9912  75.2552  0.8780  0.5196  0.5988\n",
      "Std    1.4498   444.4455   2.9407  0.0122  0.0274  0.0788\n",
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    }
   ],
   "source": [
    "all_predictions_cat_2 = []\n",
    "# Catboost model nr 2\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train, X_test, is_day_feature, targets = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Dropping some winter months\n",
    "    X_train_cat_2 = X_train#[X_train['date_forecast'].dt.month.isin([3,4, 5, 6, 7, 8, 9,10])]\n",
    "\n",
    "    # Dropping date feature\n",
    "    X_train_cat_2 = X_train_cat_2.drop(columns=['date_forecast'])\n",
    "\n",
    "    # Training and prediction for diffrent seeds\n",
    "    seeds = [42]\n",
    "    total_predictions_cat_2 = None\n",
    "    for seed in seeds: \n",
    "        final_model_cat_2 = process_location_cat_2(X_train_cat_2, targets, loc,seed)#its aactually a catboost wohoo\n",
    "        predictions_cat_2 = predict_model(final_model_cat_2, X_test)\n",
    "        final_predictions_cat_2 = predictions_cat_2['prediction_label']\n",
    "        if total_predictions_cat_2 is None:\n",
    "            total_predictions_cat_2 = np.zeros_like(final_predictions_cat_2)\n",
    "            total_predictions_cat_2+=final_predictions_cat_2\n",
    "\n",
    "    mean_pred_cat_2 = total_predictions_cat_2/len(seeds)\n",
    "\n",
    "    adjusted_final_predictions_cat_2 = mean_pred_cat_2 * is_day_feature['is_day:idx']\n",
    "    adjusted_final_predictions_cat_2 = np.clip(adjusted_final_predictions_cat_2, 0, None)\n",
    "    all_predictions_cat_2.append([adjusted_final_predictions_cat_2])\n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost model nr 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset starts from 2019-06-02 22:00:00 and ends at 2023-04-30 23:00:00\n",
      "0:\tlearn: 981.0541229\ttest: 986.5219433\tbest: 986.5219433 (0)\ttotal: 59.3ms\tremaining: 19m 46s\n",
      "200:\tlearn: 320.9450192\ttest: 341.6960508\tbest: 341.6960508 (200)\ttotal: 16.3s\tremaining: 26m 47s\n",
      "400:\tlearn: 281.8109323\ttest: 323.5911619\tbest: 323.5911619 (400)\ttotal: 31.2s\tremaining: 25m 23s\n",
      "600:\tlearn: 246.4965531\ttest: 313.0847849\tbest: 313.0847849 (600)\ttotal: 45.9s\tremaining: 24m 42s\n",
      "800:\tlearn: 217.5035448\ttest: 306.7147854\tbest: 306.7147854 (800)\ttotal: 1m\tremaining: 24m 12s\n",
      "1000:\tlearn: 190.6737885\ttest: 302.6330363\tbest: 302.6323358 (999)\ttotal: 1m 15s\tremaining: 23m 50s\n",
      "1200:\tlearn: 174.3532699\ttest: 299.9268452\tbest: 299.9268452 (1200)\ttotal: 1m 30s\tremaining: 23m 29s\n",
      "1400:\tlearn: 160.3087673\ttest: 297.9194447\tbest: 297.9095416 (1398)\ttotal: 1m 44s\tremaining: 23m 13s\n",
      "1600:\tlearn: 150.9765890\ttest: 296.3387070\tbest: 296.3278847 (1598)\ttotal: 1m 59s\tremaining: 22m 57s\n",
      "1800:\tlearn: 142.7118904\ttest: 295.0725273\tbest: 295.0579019 (1794)\ttotal: 2m 15s\tremaining: 22m 47s\n",
      "2000:\tlearn: 134.5763388\ttest: 294.0450473\tbest: 294.0436494 (1999)\ttotal: 2m 30s\tremaining: 22m 29s\n",
      "2200:\tlearn: 128.7990715\ttest: 293.1826644\tbest: 293.1826644 (2200)\ttotal: 2m 44s\tremaining: 22m 13s\n",
      "2400:\tlearn: 123.1460490\ttest: 292.4162683\tbest: 292.4103265 (2398)\ttotal: 2m 59s\tremaining: 21m 58s\n",
      "2600:\tlearn: 118.7545134\ttest: 292.0727209\tbest: 292.0024948 (2573)\ttotal: 3m 14s\tremaining: 21m 42s\n",
      "2800:\tlearn: 114.1409084\ttest: 291.6365512\tbest: 291.6349569 (2797)\ttotal: 3m 29s\tremaining: 21m 27s\n",
      "3000:\tlearn: 110.6750030\ttest: 291.2830737\tbest: 291.2466541 (2971)\ttotal: 3m 44s\tremaining: 21m 11s\n",
      "3200:\tlearn: 107.0859893\ttest: 291.0159520\tbest: 291.0159520 (3200)\ttotal: 3m 59s\tremaining: 20m 55s\n",
      "3400:\tlearn: 103.9199608\ttest: 290.7670771\tbest: 290.7090653 (3351)\ttotal: 4m 14s\tremaining: 20m 40s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 290.7090653\n",
      "bestIteration = 3351\n",
      "\n",
      "Shrink model to first 3352 iterations.\n",
      "The dataset starts from 2018-12-31 23:00:00 and ends at 2023-04-30 23:00:00\n",
      "0:\tlearn: 185.0321347\ttest: 184.4813583\tbest: 184.4813583 (0)\ttotal: 63.8ms\tremaining: 21m 15s\n",
      "200:\tlearn: 49.2267440\ttest: 54.5721107\tbest: 54.5721107 (200)\ttotal: 14.4s\tremaining: 23m 43s\n",
      "400:\tlearn: 40.0148888\ttest: 51.0957403\tbest: 51.0957403 (400)\ttotal: 29.6s\tremaining: 24m 8s\n",
      "600:\tlearn: 33.8173131\ttest: 49.5168721\tbest: 49.5160368 (598)\ttotal: 44s\tremaining: 23m 40s\n",
      "800:\tlearn: 28.5978575\ttest: 48.4759509\tbest: 48.4759509 (800)\ttotal: 58.4s\tremaining: 23m 18s\n",
      "1000:\tlearn: 25.1718747\ttest: 47.8269326\tbest: 47.8269204 (999)\ttotal: 1m 11s\tremaining: 22m 42s\n",
      "1200:\tlearn: 22.8856085\ttest: 47.4411204\tbest: 47.4411204 (1200)\ttotal: 1m 25s\tremaining: 22m 11s\n",
      "1400:\tlearn: 20.8544290\ttest: 47.1394490\tbest: 47.1394490 (1400)\ttotal: 1m 38s\tremaining: 21m 45s\n",
      "1600:\tlearn: 18.9670975\ttest: 46.9528789\tbest: 46.9483283 (1597)\ttotal: 1m 51s\tremaining: 21m 21s\n",
      "1800:\tlearn: 17.4139992\ttest: 46.7346479\tbest: 46.7266873 (1780)\ttotal: 2m 4s\tremaining: 20m 59s\n",
      "2000:\tlearn: 16.1359137\ttest: 46.6053114\tbest: 46.6025763 (1992)\ttotal: 2m 17s\tremaining: 20m 40s\n",
      "2200:\tlearn: 14.8533938\ttest: 46.5096603\tbest: 46.5096603 (2200)\ttotal: 2m 30s\tremaining: 20m 20s\n",
      "2400:\tlearn: 13.9983143\ttest: 46.3884081\tbest: 46.3841472 (2391)\ttotal: 2m 44s\tremaining: 20m 2s\n",
      "2600:\tlearn: 13.2345248\ttest: 46.2636172\tbest: 46.2625729 (2592)\ttotal: 2m 57s\tremaining: 19m 45s\n",
      "2800:\tlearn: 12.6960315\ttest: 46.2064455\tbest: 46.2064455 (2800)\ttotal: 3m 10s\tremaining: 19m 28s\n",
      "3000:\tlearn: 12.2899178\ttest: 46.1769685\tbest: 46.1734043 (2953)\ttotal: 3m 23s\tremaining: 19m 11s\n",
      "3200:\tlearn: 11.8396668\ttest: 46.1407667\tbest: 46.1354596 (3182)\ttotal: 3m 36s\tremaining: 18m 54s\n",
      "3400:\tlearn: 11.3704953\ttest: 46.0931663\tbest: 46.0929808 (3398)\ttotal: 3m 49s\tremaining: 18m 40s\n",
      "3600:\tlearn: 10.8952056\ttest: 46.0740828\tbest: 46.0546386 (3556)\ttotal: 4m 3s\tremaining: 18m 28s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 46.05463862\n",
      "bestIteration = 3556\n",
      "\n",
      "Shrink model to first 3557 iterations.\n",
      "The dataset starts from 2018-12-31 23:00:00 and ends at 2023-04-30 23:00:00\n",
      "0:\tlearn: 152.8243626\ttest: 157.7285790\tbest: 157.7285790 (0)\ttotal: 66.6ms\tremaining: 22m 11s\n",
      "200:\tlearn: 43.5613166\ttest: 49.1860538\tbest: 49.1860538 (200)\ttotal: 13.4s\tremaining: 22m\n",
      "400:\tlearn: 35.4510657\ttest: 45.7833274\tbest: 45.7833274 (400)\ttotal: 27.3s\tremaining: 22m 14s\n",
      "600:\tlearn: 29.2330753\ttest: 43.7454151\tbest: 43.7454151 (600)\ttotal: 40.8s\tremaining: 21m 56s\n",
      "800:\tlearn: 24.7716296\ttest: 42.6670553\tbest: 42.6640382 (797)\ttotal: 54.6s\tremaining: 21m 48s\n",
      "1000:\tlearn: 21.7026201\ttest: 42.0395841\tbest: 42.0391080 (999)\ttotal: 1m 8s\tremaining: 21m 35s\n",
      "1200:\tlearn: 19.3212459\ttest: 41.5825111\tbest: 41.5825111 (1200)\ttotal: 1m 21s\tremaining: 21m 21s\n",
      "1400:\tlearn: 17.5308989\ttest: 41.1592840\tbest: 41.1592840 (1400)\ttotal: 1m 35s\tremaining: 21m 8s\n",
      "1600:\tlearn: 15.8723877\ttest: 40.8992881\tbest: 40.8992881 (1600)\ttotal: 1m 49s\tremaining: 20m 55s\n",
      "1800:\tlearn: 14.5497538\ttest: 40.6476987\tbest: 40.6476987 (1800)\ttotal: 2m 2s\tremaining: 20m 42s\n",
      "2000:\tlearn: 13.6785206\ttest: 40.4752831\tbest: 40.4694531 (1998)\ttotal: 2m 16s\tremaining: 20m 29s\n",
      "2200:\tlearn: 12.7236257\ttest: 40.3413434\tbest: 40.3392776 (2190)\ttotal: 2m 30s\tremaining: 20m 16s\n",
      "2400:\tlearn: 11.8186929\ttest: 40.2401450\tbest: 40.2323256 (2390)\ttotal: 2m 44s\tremaining: 20m 3s\n",
      "2600:\tlearn: 11.0691416\ttest: 40.1801094\tbest: 40.1752952 (2597)\ttotal: 2m 58s\tremaining: 19m 51s\n",
      "2800:\tlearn: 10.4613597\ttest: 40.0653987\tbest: 40.0653987 (2800)\ttotal: 3m 11s\tremaining: 19m 37s\n",
      "3000:\tlearn: 9.7379289\ttest: 39.9575403\tbest: 39.9575403 (3000)\ttotal: 3m 25s\tremaining: 19m 24s\n",
      "3200:\tlearn: 9.2707747\ttest: 39.8999504\tbest: 39.8999504 (3200)\ttotal: 3m 39s\tremaining: 19m 10s\n",
      "3400:\tlearn: 8.7985888\ttest: 39.8408590\tbest: 39.8355657 (3389)\ttotal: 3m 52s\tremaining: 18m 57s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 39.83556573\n",
      "bestIteration = 3389\n",
      "\n",
      "Shrink model to first 3390 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Catboost model nr 3\n",
    "for loc in locations:\n",
    "    \n",
    "    # Load your data\n",
    "    train = pd.read_parquet(f'{loc}/train_targets.parquet').fillna(0)\n",
    "    X_train_estimated = pd.read_parquet(f'{loc}/X_train_estimated.parquet')\n",
    "    X_train_observed = pd.read_parquet(f'{loc}/X_train_observed.parquet')\n",
    "    X_test_estimated = pd.read_parquet(f'{loc}/X_test_estimated.parquet')\n",
    "\n",
    "    # Calling preprocessing\n",
    "    X_train_3, X_test_3, is_day_feature_3, targets_3 = preprocessing(train, X_train_observed, X_train_estimated, X_test_estimated)\n",
    "\n",
    "    # Dropping date feature\n",
    "    X_train_3 = X_train_3.drop(columns=['date_forecast'])\n",
    "    \n",
    "    # Catboooooooozt fun round 3 wohoo\n",
    "    model_cat_3 = CatBoostRegressor(\n",
    "    verbose=200, \n",
    "    learning_rate=0.03,\n",
    "    depth=10,\n",
    "    l2_leaf_reg=5,\n",
    "    random_state=42, \n",
    "    n_estimators=20000, \n",
    "    loss_function='MAE', \n",
    "    early_stopping_rounds=100,)\n",
    "\n",
    "    # Create 'sin_sun_azimuth' and 'cos_sun_azimuth' from 'sun_azimuth' in radians\n",
    "    X_train_3['sin_sun_azimuth'] = np.sin(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_train_3['cos_sun_azimuth'] = np.cos(np.radians(X_train_3['sun_azimuth:d']))\n",
    "    X_test_3['sin_sun_azimuth'] = np.sin(np.radians(X_test_3['sun_azimuth:d']))\n",
    "    X_test_3['cos_sun_azimuth'] = np.cos(np.radians(X_test_3['sun_azimuth:d']))\n",
    "\n",
    "    # Now drop the original 'sun_azimuth' feature\n",
    "    X_train_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "    X_test_3.drop('sun_azimuth:d', axis=1, inplace=True)\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train_cat_3, X_test_cat_3, y_train_cat_3, y_test_cat_3 = train_test_split(X_train_3, targets_3, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model_cat_3.fit(X_train_cat_3, y_train_cat_3['pv_measurement'],eval_set=(X_test_cat_3, y_test_cat_3['pv_measurement']),)  \n",
    "    \n",
    "    # Pred\n",
    "    pred_cat_3 = model_cat_3.predict(X_test_3)\n",
    "\n",
    "    # Multiplying the predictions with is_day, so setting predictions at night to zero\n",
    "    adjusted_final_predictions_cat_3 = pred_cat_3 * is_day_feature_3['is_day:idx']\n",
    "\n",
    "    # Setting negative predictions to zero\n",
    "    adjusted_final_predictions_cat_3 = np.clip(adjusted_final_predictions_cat_3, 0, None)\n",
    "\n",
    "    # Appening predictions for each location to final list\n",
    "    all_predictions_cat_3.append(adjusted_final_predictions_cat_3) \n",
    "\n",
    "# Changing final list to array   \n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2160,)\n"
     ]
    }
   ],
   "source": [
    "all_predictions_lGBM_e = np.array(all_predictions_lGBM_e).flatten()\n",
    "#all_predictions_rf = np.array(all_predictions_rf).flatten()\n",
    "all_predictions_cat = np.array(all_predictions_cat).flatten()\n",
    "all_predictions_cat_2 = np.array(all_predictions_cat_2).flatten()\n",
    "all_predictions_cat_3 = np.array(all_predictions_cat_3).flatten()\n",
    "#all_pred_stacked = np.array(all_pred_stacked).flatten()\n",
    "all_pred = 0.25*all_predictions_cat+0.25 * all_predictions_lGBM_e+ 0.25* all_predictions_cat_2+0.25*all_predictions_cat_3#+ 0.1*all_predictions_rf#+ 0.45*all_predictions_lGBM  +  0.1*all_predictions_cat + 0.25*all_predictions_lasso + 0.1*all_predictions_rf\n",
    "all_pred[all_pred < 6] = 0\n",
    "print(all_pred.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the final predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission\n",
    "sample_submission = sample_submission[['id']].merge(final_df[['id', 'prediction']], on='id', how='left')\n",
    "sample_submission.to_csv('my_first_submission.csv', index=False)'''\n",
    "\n",
    "final_predictions = all_pred\n",
    "\n",
    "# Save the final_predictions to CSV\n",
    "df = pd.DataFrame(final_predictions, columns=['prediction'])\n",
    "df['id'] = df.index\n",
    "df = df[['id', 'prediction']]\n",
    "df.to_csv('final_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('final_predictions_ok.csv')\n",
    "df_diff = df1[0:720] - df\n",
    "df_diff.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-name",
   "language": "python",
   "name": "pycaret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
